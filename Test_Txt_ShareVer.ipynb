{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterHJY628/MyOwnExample/blob/main/Test_Txt_ShareVer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"torch==2.4.0\" tensorboard pillow torchvision accelerate huggingface_hub\n",
        "!pip -q install  --upgrade \\\n",
        "  \"transformers==4.45.1\" \\\n",
        "  \"datasets==3.0.1\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.44.0\" \\\n",
        "  \"trl==0.11.1\" \\\n",
        "  \"peft==0.13.0\" \\\n",
        "  \"qwen_vl_utils\""
      ],
      "metadata": {
        "id": "31hYynMQalOW",
        "outputId": "a46a1db6-48a3-4d68-9bce-f65809dd1fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "31hYynMQalOW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6XTxQsAahAV",
        "outputId": "000f5296-74cb-449d-c7cf-4a6c1b16b1e8"
      },
      "id": "j6XTxQsAahAV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
      "metadata": {
        "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
        "outputId": "03dbf0b5-1cf5-4a12-81b7-7a20b3648f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/sa5u24/VQA\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['HF_HOME'] = '/home/sa5u24/VQA'\n",
        "hf_home = os.path.expanduser(\n",
        "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
        ")\n",
        "print(hf_home)\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your-hf-token-here' with your actual Hugging Face token\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6",
      "metadata": {
        "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\" You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
        "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
        "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
        "The agent may call more than one model sequentially or in parallel based on query.\n",
        "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
        "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
        "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
        "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "\"\"\"\n",
        "\n",
        "def format_data(sample):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"Agent\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                # {\n",
        "                #     \"type\": \"image\",\n",
        "                #     \"image\": None,\n",
        "                # },\n",
        "                # {\n",
        "                #     \"type\": \"image\",\n",
        "                #     \"image\": sample[\"image\"],\n",
        "                # },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[0]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[1]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"HuggingFaceM4/ChartQA\"\n",
        "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=['train[:10%]', 'val[:10%]', 'test[:10%]'])\n",
        "\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "train_dataset[200], len(train_dataset), len(eval_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "e776ZyBAnZ5G",
        "outputId": "afdc7b27-8460-4804-ed0c-31de6149b63c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e776ZyBAnZ5G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'role': 'system',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'You are a Vision Language Model specialized in interpreting visual data from chart images.\\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
              "  {'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Is the rightmost value of light brown graph 58?'}]},\n",
              "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'No'}]}],\n",
              " 2830,\n",
              " 192,\n",
              " 250)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access the file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TextQuestionLabelDataset(Dataset):\n",
        "    def __init__(self, input_file):\n",
        "        # Load data from a CSV file\n",
        "        self.data = pd.read_csv(input_file)\n",
        "        print(self.data.head())  # 显示前 5 行数据\n",
        "\n",
        "\n",
        "        # Extract questions and labels from the Input column\n",
        "        #self.questions = self.data['Input'].apply(lambda x: re.search(r'Question: (.+?)\\n', x).group(1)).tolist()\n",
        "        self.questions = self.data['Input'].tolist()\n",
        "        self.labels = self.data['Label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (question, label) given an index\n",
        "        question = self.questions[idx]\n",
        "        label = self.labels[idx]\n",
        "        return question, label\n",
        "\n",
        "input_file = '/content/drive/My Drive/agent_data.csv'\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = TextQuestionLabelDataset(input_file)\n",
        "\n",
        "# Split the dataset for train, eval, and test (e.g., 10% for each)\n",
        "dataset_length = len(dataset)\n",
        "#train_size = int(0.1 * dataset_length)\n",
        "train_size = 0\n",
        "#eval_size = int(0.1 * dataset_length)\n",
        "eval_size = 0\n",
        "test_size = dataset_length - train_size - eval_size\n",
        "#seed everything\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, eval_size, test_size]\n",
        ")\n",
        "#print(train_dataset[0])\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "# eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "test_dataset[0]"
      ],
      "metadata": {
        "id": "0idS6tUnWhuz",
        "outputId": "c2bfa416-9f6f-4da0-d110-6161b6b4a85a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0idS6tUnWhuz",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Index                    Type  \\\n",
            "0      1             Segment-MRI   \n",
            "1      2  Segment-MRI|Overlaying   \n",
            "2      3            Surgical-VQA   \n",
            "3      4           Segment-Video   \n",
            "4      5              Overlaying   \n",
            "\n",
            "                                               Input  \\\n",
            "0  Question: Identify and segment the right carot...   \n",
            "1  Question: Can you segment the Left Optic Protu...   \n",
            "2              Question: Can you identify the steps?   \n",
            "3  Question: Precisely segment Right Optic Protub...   \n",
            "4  Question: Project tumor segmentation imaging o...   \n",
            "\n",
            "                                               Label  \n",
            "0  Model: Segment-MRI\\nPrompt: Segment right caro...  \n",
            "1  Model: Segment-MRI|Overlaying\\nPrompt: Segment...  \n",
            "2  Model: Surgical-VQA\\nPrompt: can you identify ...  \n",
            "3  Model: Segment-Video\\nPrompt: Precise segmenta...  \n",
            "4  Model: Overlaying\\nPrompt: Overlay tumor segme...  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'Agent',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': ' You are an LLM agent which can call functions and corresponding prompt based on surgeons\\' query.\\nThe agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video,\\xa0Overlaying, Surgical-VQA.\\nThere are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\\nThe agent may call more than one model sequentially or in parallel based on query.\\nYour task is to analyze the user\\'s query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\\nIf you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\\nPrompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video,\\xa0Overlaying, Surgical-VQA].\\nIf you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\\nPrompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video,\\xa0Overlaying, Surgical-VQA].\\n'}]},\n",
              " {'role': 'user',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': 'Question: Analyze the video and identify all instruments present?'}]},\n",
              " {'role': 'assistant',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': 'Model: Detect-Video\\nPrompt: analyze the video and identify all instruments present?'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b309522e-b40b-4289-87e9-d28c27b81678",
      "metadata": {
        "id": "b309522e-b40b-4289-87e9-d28c27b81678"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
        "    # Prepare the text input by applying the chat template\n",
        "    text_input = processor.apply_chat_template(\n",
        "        sample[:2],  # Use the sample without the system message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    #print(\"Text Input:\", text_input)\n",
        "\n",
        "\n",
        "    # Process the visual input from the sample\n",
        "    # image_inputs, _ = process_vision_info(sample)\n",
        "    #image_inputs = sample[1]['content'][0]['image'].convert(\"RGB\")\n",
        "    image_data = sample[1]['content'][0].get('image')\n",
        "\n",
        "    if image_data is not None:\n",
        "        image_inputs = image_data.convert(\"RGB\")\n",
        "    else:\n",
        "        # 提供一个默认值或适当的处理逻辑\n",
        "        image_inputs = None\n",
        "        #print(\"Warning: The 'image' field is None.\")\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "    # Generate text with the model\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode the output text\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_text[0]  # Return the first decoded output text\n",
        "\n",
        "# def generate_text_from_sample(model, processor, question, max_new_tokens=1024, device=\"cuda\"):\n",
        "#     \"\"\"\n",
        "#     Generate text from a sample using the model and processor.\n",
        "\n",
        "#     Args:\n",
        "#         model: The model used for generation.\n",
        "#         processor: The processor for preparing inputs.\n",
        "#         question: The input question as a string.\n",
        "#         max_new_tokens: The maximum number of tokens to generate.\n",
        "#         device: The device to use for computation (\"cuda\" or \"cpu\").\n",
        "\n",
        "#     Returns:\n",
        "#         A string containing the generated output text.\n",
        "#     \"\"\"\n",
        "#     # Prepare the text input directly from the question\n",
        "#     text_input = processor.apply_chat_template(\n",
        "#         [question],  # Directly use the question\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#     )\n",
        "\n",
        "#     # Handle the case where no image is provided\n",
        "#     image_inputs = None  # No visual input in the current dataset\n",
        "\n",
        "#     # Prepare the inputs for the model\n",
        "#     model_inputs = processor(\n",
        "#         text=[text_input],\n",
        "#         images=image_inputs,\n",
        "#         return_tensors=\"pt\",\n",
        "#     ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "#     # Generate text with the model\n",
        "#     generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "#     # Trim the generated ids to remove the input ids\n",
        "#     trimmed_generated_ids = [\n",
        "#         out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "#     ]\n",
        "\n",
        "#     # Decode the output text\n",
        "#     output_text = processor.batch_decode(\n",
        "#         trimmed_generated_ids,\n",
        "#         skip_special_tokens=True,\n",
        "#         clean_up_tokenization_spaces=False\n",
        "#     )\n",
        "\n",
        "#     return output_text[0]  # Return the first decoded output text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
      "metadata": {
        "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
        "outputId": "076b8caf-53aa-4c6a-ab23-af30c1eb6739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU allocated memory: 0.01 GB\n",
            "GPU reserved memory: 0.02 GB\n"
          ]
        }
      ],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
      "metadata": {
        "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
        "outputId": "f1b014a4-c080-400e-ae8b-3ceb016b5a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "ebef6548f9d240e5b2032575e19256a6",
            "021af8b451b54d8cb42bf8f1f393ca26",
            "9d7dceef4a724914bd2717cd2ad362f4",
            "27c3aded8b674482a45cb3dc48cb25e9",
            "e248f5dee5524ceea599547cd52b4cd5",
            "0daeac6c04bd475694d00632b55567a2",
            "e3f8d91666424fbc8a9ffe8a25c44c61",
            "37cf4abd6f7448ddb112ebf02d4227ea",
            "4d198a60caf44fd790c8aa2fdbe997c2",
            "5a26c5f60758486089656aff1690be56",
            "d5d5f37a501e4e68ac738539ed7f12c8"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebef6548f9d240e5b2032575e19256a6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from nltk.translate.meteor_score import meteor_score, single_meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import requests\n",
        "from torch import nn\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor, MllamaConfig, AutoModelForCausalLM\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # low_cpu_mem_usage=True,\n",
        "    # bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = MllamaForConditionalGeneration.from_pretrained(\n",
        "            model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True,\n",
        "            quantization_config=quantization_config,\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "all_ans = []"
      ],
      "metadata": {
        "id": "q7PWaz-aZU-B"
      },
      "id": "q7PWaz-aZU-B",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sample in test_dataset:\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[2]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n",
        "# all_pred = []\n",
        "# all_ans = []\n",
        "\n",
        "# model.eval()  # 切换到评估模式\n",
        "# with torch.no_grad():  # 禁用梯度计算\n",
        "#     for question, label in test_loader:  # 遍历测试数据加载器\n",
        "#         # 将问题输入模型，生成预测结果\n",
        "#         # 假设 generate_text_from_sample 是用于推理的函数\n",
        "#         output = [generate_text_from_sample(model, processor, q) for q in question]\n",
        "\n",
        "#         # 将预测结果和真实标签保存\n",
        "#         all_pred.extend(output)\n",
        "#         all_ans.extend(label)\n",
        "\n",
        "# # 打印结果，或者使用 all_pred 和 all_ans 计算指标\n",
        "# print(\"Predictions:\", all_pred)\n",
        "# print(\"Answers:\", all_ans)"
      ],
      "metadata": {
        "id": "MGlopdzQf0t1",
        "outputId": "66763255-b5e0-49c9-be98-f8ac944c8fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "id": "MGlopdzQf0t1",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-72b56306580e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text_from_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mall_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b7ddf7081622>\u001b[0m in \u001b[0;36mgenerate_text_from_sample\u001b[0;34m(model, processor, sample, max_new_tokens, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Generate text with the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Trim the generated ids to remove the input ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2049\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3042\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 设置保存路径\n",
        "save_path = '/content/drive/My Drive/Sharing/test_progress.pkl'\n",
        "\n",
        "# 尝试加载之前的进度\n",
        "if os.path.exists(save_path):\n",
        "    with open(save_path, 'rb') as f:\n",
        "        saved_data = pickle.load(f)\n",
        "        start_index = saved_data['start_index']\n",
        "        all_pred = saved_data['all_pred']\n",
        "        all_ans = saved_data['all_ans']\n",
        "        print(f\"Resuming from index {start_index}\")\n",
        "else:\n",
        "    start_index = 0\n",
        "    all_pred = []\n",
        "    all_ans = []\n",
        "    print(\"Starting fresh\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2zOqW3f96W",
        "outputId": "a50f143f-6934-445f-cbf9-1ef43ab48c32"
      },
      "id": "JR2zOqW3f96W",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting fresh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, sample in enumerate(test_dataset):\n",
        "        # 跳过已处理的样本\n",
        "        if i < start_index:\n",
        "            continue\n",
        "\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[2]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n",
        "        # 更新进度\n",
        "        start_index = i + 1\n",
        "        saved_data = {\n",
        "            'start_index': start_index,\n",
        "            'all_pred': all_pred,\n",
        "            'all_ans': all_ans\n",
        "        }\n",
        "\n",
        "        # 保存到Google Drive\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(saved_data, f)\n",
        "\n",
        "        print(f\"Processed sample {i+1}/{len(test_dataset)}\")\n",
        "\n",
        "print(\"All samples processed!\")\n"
      ],
      "metadata": {
        "id": "TUO0A8QbFjF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcf9140-2db2-423d-9d30-4d8f993f6d3f"
      },
      "id": "TUO0A8QbFjF3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample 1/6000\n",
            "Processed sample 2/6000\n",
            "Processed sample 3/6000\n",
            "Processed sample 4/6000\n",
            "Processed sample 5/6000\n",
            "Processed sample 6/6000\n",
            "Processed sample 7/6000\n",
            "Processed sample 8/6000\n",
            "Processed sample 9/6000\n",
            "Processed sample 10/6000\n",
            "Processed sample 11/6000\n",
            "Processed sample 12/6000\n",
            "Processed sample 13/6000\n",
            "Processed sample 14/6000\n",
            "Processed sample 15/6000\n",
            "Processed sample 16/6000\n",
            "Processed sample 17/6000\n",
            "Processed sample 18/6000\n",
            "Processed sample 19/6000\n",
            "Processed sample 20/6000\n",
            "Processed sample 21/6000\n",
            "Processed sample 22/6000\n",
            "Processed sample 23/6000\n",
            "Processed sample 24/6000\n",
            "Processed sample 25/6000\n",
            "Processed sample 26/6000\n",
            "Processed sample 27/6000\n",
            "Processed sample 28/6000\n",
            "Processed sample 29/6000\n",
            "Processed sample 30/6000\n",
            "Processed sample 31/6000\n",
            "Processed sample 32/6000\n",
            "Processed sample 33/6000\n",
            "Processed sample 34/6000\n",
            "Processed sample 35/6000\n",
            "Processed sample 36/6000\n",
            "Processed sample 37/6000\n",
            "Processed sample 38/6000\n",
            "Processed sample 39/6000\n",
            "Processed sample 40/6000\n",
            "Processed sample 41/6000\n",
            "Processed sample 42/6000\n",
            "Processed sample 43/6000\n",
            "Processed sample 44/6000\n",
            "Processed sample 45/6000\n",
            "Processed sample 46/6000\n",
            "Processed sample 47/6000\n",
            "Processed sample 48/6000\n",
            "Processed sample 49/6000\n",
            "Processed sample 50/6000\n",
            "Processed sample 51/6000\n",
            "Processed sample 52/6000\n",
            "Processed sample 53/6000\n",
            "Processed sample 54/6000\n",
            "Processed sample 55/6000\n",
            "Processed sample 56/6000\n",
            "Processed sample 57/6000\n",
            "Processed sample 58/6000\n",
            "Processed sample 59/6000\n",
            "Processed sample 60/6000\n",
            "Processed sample 61/6000\n",
            "Processed sample 62/6000\n",
            "Processed sample 63/6000\n",
            "Processed sample 64/6000\n",
            "Processed sample 65/6000\n",
            "Processed sample 66/6000\n",
            "Processed sample 67/6000\n",
            "Processed sample 68/6000\n",
            "Processed sample 69/6000\n",
            "Processed sample 70/6000\n",
            "Processed sample 71/6000\n",
            "Processed sample 72/6000\n",
            "Processed sample 73/6000\n",
            "Processed sample 74/6000\n",
            "Processed sample 75/6000\n",
            "Processed sample 76/6000\n",
            "Processed sample 77/6000\n",
            "Processed sample 78/6000\n",
            "Processed sample 79/6000\n",
            "Processed sample 80/6000\n",
            "Processed sample 81/6000\n",
            "Processed sample 82/6000\n",
            "Processed sample 83/6000\n",
            "Processed sample 84/6000\n",
            "Processed sample 85/6000\n",
            "Processed sample 86/6000\n",
            "Processed sample 87/6000\n",
            "Processed sample 88/6000\n",
            "Processed sample 89/6000\n",
            "Processed sample 90/6000\n",
            "Processed sample 91/6000\n",
            "Processed sample 92/6000\n",
            "Processed sample 93/6000\n",
            "Processed sample 94/6000\n",
            "Processed sample 95/6000\n",
            "Processed sample 96/6000\n",
            "Processed sample 97/6000\n",
            "Processed sample 98/6000\n",
            "Processed sample 99/6000\n",
            "Processed sample 100/6000\n",
            "Processed sample 101/6000\n",
            "Processed sample 102/6000\n",
            "Processed sample 103/6000\n",
            "Processed sample 104/6000\n",
            "Processed sample 105/6000\n",
            "Processed sample 106/6000\n",
            "Processed sample 107/6000\n",
            "Processed sample 108/6000\n",
            "Processed sample 109/6000\n",
            "Processed sample 110/6000\n",
            "Processed sample 111/6000\n",
            "Processed sample 112/6000\n",
            "Processed sample 113/6000\n",
            "Processed sample 114/6000\n",
            "Processed sample 115/6000\n",
            "Processed sample 116/6000\n",
            "Processed sample 117/6000\n",
            "Processed sample 118/6000\n",
            "Processed sample 119/6000\n",
            "Processed sample 120/6000\n",
            "Processed sample 121/6000\n",
            "Processed sample 122/6000\n",
            "Processed sample 123/6000\n",
            "Processed sample 124/6000\n",
            "Processed sample 125/6000\n",
            "Processed sample 126/6000\n",
            "Processed sample 127/6000\n",
            "Processed sample 128/6000\n",
            "Processed sample 129/6000\n",
            "Processed sample 130/6000\n",
            "Processed sample 131/6000\n",
            "Processed sample 132/6000\n",
            "Processed sample 133/6000\n",
            "Processed sample 134/6000\n",
            "Processed sample 135/6000\n",
            "Processed sample 136/6000\n",
            "Processed sample 137/6000\n",
            "Processed sample 138/6000\n",
            "Processed sample 139/6000\n",
            "Processed sample 140/6000\n",
            "Processed sample 141/6000\n",
            "Processed sample 142/6000\n",
            "Processed sample 143/6000\n",
            "Processed sample 144/6000\n",
            "Processed sample 145/6000\n",
            "Processed sample 146/6000\n",
            "Processed sample 147/6000\n",
            "Processed sample 148/6000\n",
            "Processed sample 149/6000\n",
            "Processed sample 150/6000\n",
            "Processed sample 151/6000\n",
            "Processed sample 152/6000\n",
            "Processed sample 153/6000\n",
            "Processed sample 154/6000\n",
            "Processed sample 155/6000\n",
            "Processed sample 156/6000\n",
            "Processed sample 157/6000\n",
            "Processed sample 158/6000\n",
            "Processed sample 159/6000\n",
            "Processed sample 160/6000\n",
            "Processed sample 161/6000\n",
            "Processed sample 162/6000\n",
            "Processed sample 163/6000\n",
            "Processed sample 164/6000\n",
            "Processed sample 165/6000\n",
            "Processed sample 166/6000\n",
            "Processed sample 167/6000\n",
            "Processed sample 168/6000\n",
            "Processed sample 169/6000\n",
            "Processed sample 170/6000\n",
            "Processed sample 171/6000\n",
            "Processed sample 172/6000\n",
            "Processed sample 173/6000\n",
            "Processed sample 174/6000\n",
            "Processed sample 175/6000\n",
            "Processed sample 176/6000\n",
            "Processed sample 177/6000\n",
            "Processed sample 178/6000\n",
            "Processed sample 179/6000\n",
            "Processed sample 180/6000\n",
            "Processed sample 181/6000\n",
            "Processed sample 182/6000\n",
            "Processed sample 183/6000\n",
            "Processed sample 184/6000\n",
            "Processed sample 185/6000\n",
            "Processed sample 186/6000\n",
            "Processed sample 187/6000\n",
            "Processed sample 188/6000\n",
            "Processed sample 189/6000\n",
            "Processed sample 190/6000\n",
            "Processed sample 191/6000\n",
            "Processed sample 192/6000\n",
            "Processed sample 193/6000\n",
            "Processed sample 194/6000\n",
            "Processed sample 195/6000\n",
            "Processed sample 196/6000\n",
            "Processed sample 197/6000\n",
            "Processed sample 198/6000\n",
            "Processed sample 199/6000\n",
            "Processed sample 200/6000\n",
            "Processed sample 201/6000\n",
            "Processed sample 202/6000\n",
            "Processed sample 203/6000\n",
            "Processed sample 204/6000\n",
            "Processed sample 205/6000\n",
            "Processed sample 206/6000\n",
            "Processed sample 207/6000\n",
            "Processed sample 208/6000\n",
            "Processed sample 209/6000\n",
            "Processed sample 210/6000\n",
            "Processed sample 211/6000\n",
            "Processed sample 212/6000\n",
            "Processed sample 213/6000\n",
            "Processed sample 214/6000\n",
            "Processed sample 215/6000\n",
            "Processed sample 216/6000\n",
            "Processed sample 217/6000\n",
            "Processed sample 218/6000\n",
            "Processed sample 219/6000\n",
            "Processed sample 220/6000\n",
            "Processed sample 221/6000\n",
            "Processed sample 222/6000\n",
            "Processed sample 223/6000\n",
            "Processed sample 224/6000\n",
            "Processed sample 225/6000\n",
            "Processed sample 226/6000\n",
            "Processed sample 227/6000\n",
            "Processed sample 228/6000\n",
            "Processed sample 229/6000\n",
            "Processed sample 230/6000\n",
            "Processed sample 231/6000\n",
            "Processed sample 232/6000\n",
            "Processed sample 233/6000\n",
            "Processed sample 234/6000\n",
            "Processed sample 235/6000\n",
            "Processed sample 236/6000\n",
            "Processed sample 237/6000\n",
            "Processed sample 238/6000\n",
            "Processed sample 239/6000\n",
            "Processed sample 240/6000\n",
            "Processed sample 241/6000\n",
            "Processed sample 242/6000\n",
            "Processed sample 243/6000\n",
            "Processed sample 244/6000\n",
            "Processed sample 245/6000\n",
            "Processed sample 246/6000\n",
            "Processed sample 247/6000\n",
            "Processed sample 248/6000\n",
            "Processed sample 249/6000\n",
            "Processed sample 250/6000\n",
            "Processed sample 251/6000\n",
            "Processed sample 252/6000\n",
            "Processed sample 253/6000\n",
            "Processed sample 254/6000\n",
            "Processed sample 255/6000\n",
            "Processed sample 256/6000\n",
            "Processed sample 257/6000\n",
            "Processed sample 258/6000\n",
            "Processed sample 259/6000\n",
            "Processed sample 260/6000\n",
            "Processed sample 261/6000\n",
            "Processed sample 262/6000\n",
            "Processed sample 263/6000\n",
            "Processed sample 264/6000\n",
            "Processed sample 265/6000\n",
            "Processed sample 266/6000\n",
            "Processed sample 267/6000\n",
            "Processed sample 268/6000\n",
            "Processed sample 269/6000\n",
            "Processed sample 270/6000\n",
            "Processed sample 271/6000\n",
            "Processed sample 272/6000\n",
            "Processed sample 273/6000\n",
            "Processed sample 274/6000\n",
            "Processed sample 275/6000\n",
            "Processed sample 276/6000\n",
            "Processed sample 277/6000\n",
            "Processed sample 278/6000\n",
            "Processed sample 279/6000\n",
            "Processed sample 280/6000\n",
            "Processed sample 281/6000\n",
            "Processed sample 282/6000\n",
            "Processed sample 283/6000\n",
            "Processed sample 284/6000\n",
            "Processed sample 285/6000\n",
            "Processed sample 286/6000\n",
            "Processed sample 287/6000\n",
            "Processed sample 288/6000\n",
            "Processed sample 289/6000\n",
            "Processed sample 290/6000\n",
            "Processed sample 291/6000\n",
            "Processed sample 292/6000\n",
            "Processed sample 293/6000\n",
            "Processed sample 294/6000\n",
            "Processed sample 295/6000\n",
            "Processed sample 296/6000\n",
            "Processed sample 297/6000\n",
            "Processed sample 298/6000\n",
            "Processed sample 299/6000\n",
            "Processed sample 300/6000\n",
            "Processed sample 301/6000\n",
            "Processed sample 302/6000\n",
            "Processed sample 303/6000\n",
            "Processed sample 304/6000\n",
            "Processed sample 305/6000\n",
            "Processed sample 306/6000\n",
            "Processed sample 307/6000\n",
            "Processed sample 308/6000\n",
            "Processed sample 309/6000\n",
            "Processed sample 310/6000\n",
            "Processed sample 311/6000\n",
            "Processed sample 312/6000\n",
            "Processed sample 313/6000\n",
            "Processed sample 314/6000\n",
            "Processed sample 315/6000\n",
            "Processed sample 316/6000\n",
            "Processed sample 317/6000\n",
            "Processed sample 318/6000\n",
            "Processed sample 319/6000\n",
            "Processed sample 320/6000\n",
            "Processed sample 321/6000\n",
            "Processed sample 322/6000\n",
            "Processed sample 323/6000\n",
            "Processed sample 324/6000\n",
            "Processed sample 325/6000\n",
            "Processed sample 326/6000\n",
            "Processed sample 327/6000\n",
            "Processed sample 328/6000\n",
            "Processed sample 329/6000\n",
            "Processed sample 330/6000\n",
            "Processed sample 331/6000\n",
            "Processed sample 332/6000\n",
            "Processed sample 333/6000\n",
            "Processed sample 334/6000\n",
            "Processed sample 335/6000\n",
            "Processed sample 336/6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
      "metadata": {
        "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
        "outputId": "90fb5d6d-cc4b-4624-bae5-394cc5e44fdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: Model: Overlaying\n",
            "Prompt: Overlay diagnosis MRI on the surgical field?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay diagnosis MRI with surgical scene.\n",
            "pred: Model: Detect-Instruments\n",
            "Prompt: What instrument quantities are present in the frame?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: what instrument quantities is present in the frame?\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Locate instruments in the surgical scene.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: locate the instruments in the surgical scene.\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Identify instruments present in the video?\n",
            "ans: Model: Detect-Video\n",
            "Prompt: can you identify the instruments present in the video?\n",
            "pred: Model: Detect-Video, Segment-MRI\n",
            "Prompt: Detect instruments near the Left Carotid and segment the anatomy?\"\n",
            "ans: Model: Detect-Video|Segment-Video\n",
            "Prompt: Detect instruments near Left Carotid?|Segment anatomy from video?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment the region of Left Optic Protuberance from video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Analyze left optic protuberance?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Identify and segment the right carotid in MRI?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment right carotid from MRI.\n",
            "pred: Model: Overlaying\n",
            "Prompt: Overlay the tumor segmentation data onto the current surgical scene?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay tumor segmentation with surgical scene.\n",
            "pred: Model: Step-Detection\n",
            "Prompt: What steps are present in the frame?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: what steps is present in the frame?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Segment sella?\n",
            "pred: Model: Surgical-VQA\n",
            "Prompt: Analyze the operation notes in this phase.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: analyze the operation notes in this phase.\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Clival Recess from video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Precise segmentation of clival recess?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Right Optic Protuberance from video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Precise segmentation of right optic protuberance?\n",
            "pred: Model: Phase\n",
            "Prompt: Identify the steps in the surgical procedure.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: can you identify the steps?\n",
            "pred: Model: Detect-Video, Segment-MRI\n",
            "Prompt: Detect instruments near the Right Carotid? | Segment anatomy?\"\n",
            "ans: Model: Detect-Video|Segment-Video\n",
            "Prompt: Detect instruments near Right Carotid?|Segment anatomy from video?\n",
            "pred: Model: Segment-Video, Overlaying\n",
            "Prompt: Segment Left Optic Protuberance from video| Overlay onto the surgical scene.\n",
            "ans: Model: Segment-MRI|Overlaying\n",
            "Prompt: Segment Left Optic Protuberance from MRI?|Overlay Left Optic Protuberance onto surgical scene?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Locate and segment Left Optic Protuberance from video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Locate and segment left optic protuberance?\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Identify the tools being used in the surgical video?\n",
            "ans: Model: Detect-Video\n",
            "Prompt: identify the tools being used in the surgical video?\n",
            "pred: Model: Overlaying\n",
            "Prompt: Combine anatomy segmentation with surgical visuals into an overlay?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Left Optic Protuberance from video? Overlay onto surgical scene.\n",
            "ans: Model: Segment-MRI|Overlaying\n",
            "Prompt: Segment Left Optic Protuberance from MRI?|Overlay Left Optic Protuberance onto surgical scene?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Identify and segment the pituitary region in MRI?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment pituitary region from MRI.\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Explain the role of the instrument quantities in this step.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: explain the role of the instrument quantities in this step.\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Analyze the tumor margins and segment it from MRI?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment tumor margins from MRI.\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Identify the surgical phase and|Overlay the Left Carotid location from MRI?\n",
            "ans: Model: Surgical-VQA|Overlaying\n",
            "Prompt: Identify surgical phase?|Overlay Left Carotid location from MRI?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Right Carotid accurately from video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Segment right carotid?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Segment the anatomical structure of anatomy and tumor using MRI?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment anatomy and tumor from MRI.\n",
            "pred: Model: Instrument-Position\n",
            "Prompt: Which instrument positions are being used?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: which instrument positions is being used?\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Locate and detect instruments in the surgical video?\n",
            "ans: Model: Detect-Video\n",
            "Prompt: locate and detect the instruments in the surgical video?\n",
            "pred: Model: Overlaying\n",
            "Prompt: Overlay the anatomy segmentation data onto the current surgical scene?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: Model: Detect-Instruments\n",
            "Prompt: What is the current instrument positions?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: what is the current instrument positions?\n",
            "pred: Model: Detect-Video, Segment-MRI\n",
            "Prompt: Detect instruments near the Left Carotid?| Segment anatomy?\"\n",
            "ans: Model: Detect-Video|Segment-Video\n",
            "Prompt: Detect instruments near Left Carotid?|Segment anatomy from video?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Identify and segment the right carotid in MRI?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment right carotid from MRI.\n",
            "pred: Model: Overlaying\n",
            "Prompt: Project tumor segmentation imaging onto the live surgical feed?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay tumor segmentation with surgical scene.\n",
            "pred: Model: Operation Notes\n",
            "Prompt: Which operation notes is being used?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: which operation notes is being used?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Identify the surgical phase?| Overlay Left Carotid location from MRI?\"\n",
            "ans: Model: Surgical-VQA|Overlaying\n",
            "Prompt: Identify surgical phase?|Overlay Left Carotid location from MRI?\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Analyze the video and identify all instruments present?\n",
            "ans: Model: Detect-Video\n",
            "Prompt: analyze the video and identify all instruments present?\n",
            "pred: Model: Detect-Video\n",
            "Prompt: Detect instruments near the Left Carotid and segment the anatomy?\"\n",
            "ans: Model: Detect-Video|Segment-Video\n",
            "Prompt: Detect instruments near Left Carotid?|Segment anatomy from video?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Sella in the current frame of video?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Locate sella?\n",
            "pred: Model: Detect-Video\n",
            "Prompt: What is the position of the instruments?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: what is the position of the instruments?\n",
            "pred: Model: Surgical-VQA\n",
            "Prompt: What is the current operation note?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: what is the current operation notes?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Identify the surgical phase and Overlay Right Carotid from MRI?\n",
            "ans: Model: Surgical-VQA|Overlaying\n",
            "Prompt: Identify surgical phase?|Overlay Right Carotid location from MRI?\n",
            "pred: Model: Overlaying\n",
            "Prompt: Combine anatomy segmentation and surgical visuals into an overlay?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation and highlight anatomical structures?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: Model: Segment-Video, Overlaying\n",
            "Prompt: Segment Sella from video and overlay onto surgical scene?\n",
            "ans: Model: Segment-MRI|Overlaying\n",
            "Prompt: Segment Sella from MRI?|Overlay Sella onto surgical scene?\n",
            "pred: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?|Overlay the segmented Sella onto the surgical scene?\"\n",
            "ans: Model: Segment-MRI|Overlaying\n",
            "Prompt: Segment Sella from MRI?|Overlay Sella onto surgical scene?\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Segment anatomy and tumor from MRI?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment sella from MRI.\n",
            "pred: Model: Segment-MRI\n",
            "Prompt: Precisely segment the anatomy and tumor from MRI data?\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment anatomy and tumor from MRI.\n",
            "pred: Model: Overlaying\n",
            "Prompt: Overlay MRI diagnosis on surgical scene?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay tumor segmentation with surgical scene.\n",
            "pred: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation and highlight anatomical structures?\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: Model: Step-Detection\n",
            "Prompt: Identify the current step being performed in the endonasal pituitary surgery?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: which steps is being used?\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(all_pred)):\n",
        "    print(\"pred:\", all_pred[i])\n",
        "    print(\"ans:\", all_ans[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File Path\n",
        "output_file_path = '/content/drive/My Drive/pre_ans.txt'\n",
        "\n",
        "# Write data\n",
        "with open(output_file_path, 'w') as file:\n",
        "    for i in range(len(all_pred)):\n",
        "        file.write(f\"pred: {all_pred[i]}\\n\")\n",
        "        file.write(f\"ans: {all_ans[i]}\\n\")\n",
        "        file.write(\"\\n\")  # 添加换行以便于阅读\n",
        "\n",
        "print(f\"file saved at: {output_file_path}\")"
      ],
      "metadata": {
        "id": "hlQxn84Jt15m",
        "outputId": "7b98a5fd-c813-40c0-a54a-d0b8bfefa3aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hlQxn84Jt15m",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "file saved at: /content/drive/My Drive/pre_ans.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
      "metadata": {
        "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
        "outputId": "b7164344-17a5-4dab-da8b-5e9c0d3b0e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393,
          "referenced_widgets": [
            "78264a16019943749ca003d2d9204844",
            "673a0197534e4ef8a31219355be4b86b",
            "960ea26fbb754734825a5158b1a0e8db",
            "a925d2fc2d244a5eb61505821d792594",
            "82b19860d110421b9f30aa6a76dfde1d",
            "4ffdbd9cffe444aea0a12bf2dba120d4",
            "457bf5a90c3e4af38019ba615f37cbfb",
            "6345f58df5564da98016ab17b41452a7",
            "b0d3ff39a5564a92a86626a4992b1720",
            "7b72f083ad0b40fa806b1f4f14aec3ba",
            "4f91880c2dee4708a10079cb51ec2820"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=99fff44fda1dd4aa57910458d3134ad6ee252ec03c31487b8571fdeacb17b7c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78264a16019943749ca003d2d9204844"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.7634339043333009, 'rouge2': 0.5756699120693858, 'rougeL': 0.7527997158112699, 'rougeLsum': 0.7579360331282896}\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions= all_pred, references=all_ans)\n",
        "print(rouge_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
      "metadata": {
        "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
        "outputId": "0b2b432c-5605-4f85-c434-90ac619a36c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3081185256564683\n"
          ]
        }
      ],
      "source": [
        "bleu_score = corpus_bleu(all_ans, all_pred, weights=(1.0, 0.0, 0.0, 0.0))\n",
        "print(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
      "metadata": {
        "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
        "outputId": "c5a4b8f9-08a3-43cc-85fd-8b3edf4458d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7941484814983952\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "m_score=0\n",
        "for line in zip(all_ans, all_pred):\n",
        "    ref = word_tokenize(line[0])\n",
        "    hypo = word_tokenize(line[1])\n",
        "    m_score += meteor_score([ref], hypo)\n",
        "meteors = m_score/len(all_ans)\n",
        "print(meteors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract the \"Model\" part from a given text\n",
        "def extract_model(text):\n",
        "    match = re.search(r\"Model:\\s*(.*?)\\n\", text)\n",
        "    return match.group(1) if match else \"\"\n",
        "\n",
        "# Function to calculate exact match accuracy\n",
        "def calculate_exact_match_accuracy(predictions, answers):\n",
        "    pred_models = [extract_model(pred) for pred in predictions]\n",
        "    true_models = [extract_model(ans) for ans in answers]\n",
        "    correct_matches = sum(p == t for p, t in zip(pred_models, true_models))\n",
        "    total = len(true_models)\n",
        "    return correct_matches / total if total > 0 else 0\n",
        "\n",
        "# Calculate exact match accuracy\n",
        "accuracy = calculate_exact_match_accuracy(all_pred, all_ans)\n",
        "print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "0RmPYe-zHvfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52154738-6b29-4177-d52b-fe2e6bf08342"
      },
      "id": "0RmPYe-zHvfP",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Accuracy: 58.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa",
      "metadata": {
        "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6017dd69-1978-417f-93fe-0393b4cc3f74",
      "metadata": {
        "id": "6017dd69-1978-417f-93fe-0393b4cc3f74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25",
      "metadata": {
        "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c",
      "metadata": {
        "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebef6548f9d240e5b2032575e19256a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_021af8b451b54d8cb42bf8f1f393ca26",
              "IPY_MODEL_9d7dceef4a724914bd2717cd2ad362f4",
              "IPY_MODEL_27c3aded8b674482a45cb3dc48cb25e9"
            ],
            "layout": "IPY_MODEL_e248f5dee5524ceea599547cd52b4cd5"
          }
        },
        "021af8b451b54d8cb42bf8f1f393ca26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0daeac6c04bd475694d00632b55567a2",
            "placeholder": "​",
            "style": "IPY_MODEL_e3f8d91666424fbc8a9ffe8a25c44c61",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9d7dceef4a724914bd2717cd2ad362f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37cf4abd6f7448ddb112ebf02d4227ea",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d198a60caf44fd790c8aa2fdbe997c2",
            "value": 5
          }
        },
        "27c3aded8b674482a45cb3dc48cb25e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a26c5f60758486089656aff1690be56",
            "placeholder": "​",
            "style": "IPY_MODEL_d5d5f37a501e4e68ac738539ed7f12c8",
            "value": " 5/5 [01:52&lt;00:00, 19.38s/it]"
          }
        },
        "e248f5dee5524ceea599547cd52b4cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0daeac6c04bd475694d00632b55567a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f8d91666424fbc8a9ffe8a25c44c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37cf4abd6f7448ddb112ebf02d4227ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d198a60caf44fd790c8aa2fdbe997c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a26c5f60758486089656aff1690be56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d5f37a501e4e68ac738539ed7f12c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78264a16019943749ca003d2d9204844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_673a0197534e4ef8a31219355be4b86b",
              "IPY_MODEL_960ea26fbb754734825a5158b1a0e8db",
              "IPY_MODEL_a925d2fc2d244a5eb61505821d792594"
            ],
            "layout": "IPY_MODEL_82b19860d110421b9f30aa6a76dfde1d"
          }
        },
        "673a0197534e4ef8a31219355be4b86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ffdbd9cffe444aea0a12bf2dba120d4",
            "placeholder": "​",
            "style": "IPY_MODEL_457bf5a90c3e4af38019ba615f37cbfb",
            "value": "Downloading builder script: 100%"
          }
        },
        "960ea26fbb754734825a5158b1a0e8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6345f58df5564da98016ab17b41452a7",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0d3ff39a5564a92a86626a4992b1720",
            "value": 6270
          }
        },
        "a925d2fc2d244a5eb61505821d792594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b72f083ad0b40fa806b1f4f14aec3ba",
            "placeholder": "​",
            "style": "IPY_MODEL_4f91880c2dee4708a10079cb51ec2820",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 256kB/s]"
          }
        },
        "82b19860d110421b9f30aa6a76dfde1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ffdbd9cffe444aea0a12bf2dba120d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457bf5a90c3e4af38019ba615f37cbfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6345f58df5564da98016ab17b41452a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d3ff39a5564a92a86626a4992b1720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b72f083ad0b40fa806b1f4f14aec3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f91880c2dee4708a10079cb51ec2820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}