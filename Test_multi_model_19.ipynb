{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterHJY628/MyOwnExample/blob/main/Test_multi_model_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"torch==2.4.0\" tensorboard pillow torchvision accelerate huggingface_hub\n",
        "!pip -q install  --upgrade \\\n",
        "  \"transformers==4.45.1\" \\\n",
        "  \"datasets==3.0.1\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.44.0\" \\\n",
        "  \"trl==0.11.1\" \\\n",
        "  \"peft==0.13.0\" \\\n",
        "  \"qwen_vl_utils\""
      ],
      "metadata": {
        "id": "31hYynMQalOW",
        "outputId": "9285728c-c8c8-45f4-9559-457ed8f0c761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "31hYynMQalOW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6XTxQsAahAV",
        "outputId": "29e6dde3-c9b9-41d0-887e-e4145bc1b615"
      },
      "id": "j6XTxQsAahAV",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
      "metadata": {
        "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
        "outputId": "1473ade7-0480-4fab-deaf-14b37e90f3df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/sa5u24/VQA\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['HF_HOME'] = '/home/sa5u24/VQA'\n",
        "hf_home = os.path.expanduser(\n",
        "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
        ")\n",
        "print(hf_home)\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your-hf-token-here' with your actual Hugging Face token\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6",
      "metadata": {
        "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6"
      },
      "outputs": [],
      "source": [
        "# def generate_SM(que):\n",
        "#     sm = (\n",
        "#         \"Select models to answer the following question:\\n\"\n",
        "#         f\"{que}\\n\"\n",
        "#         \"Select models only from the following model list: 'Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA' to answer the question.\\n\"\n",
        "#         \"Generate the shortest prompts to apply models \\n\"\n",
        "#         \"Your response should be like: 'Model: ?| ? \\n Prompt: ?|? and no other words.\"\n",
        "#     )\n",
        "#     return sm\n",
        "\n",
        "def generate_SM(que):\n",
        "    sm = (\n",
        "        # \"Identify the appropriate models to answer the following question:\\n\"\n",
        "        # f\"{que}\\n\"\n",
        "        # \"Select models exclusively from the list: 'Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA.'\\n\"\n",
        "        # \"Generate the shortest possible prompts to utilize the selected models.\\n\"\n",
        "        # \"Your response format should be:\\n\"\n",
        "        # \"Model: Model A|Model B\\n\"\n",
        "        # \"Prompt: Prompt A?|Prompt B?\\n\"\n",
        "        # \"Do not include any additional text.\"\n",
        "        # \"Select the appropriate 'Models' and the shortest possible 'Prompts' to address the following question:\"\n",
        "        # f\"{que}\"\n",
        "        # \"Model Options: [Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA]\"\n",
        "        # \"Each prompt must correspond to a single model and should be as concise as possible. The number of prompts should match the number of selected models.\"\n",
        "        # \"Format your response as follows:\"\n",
        "        # \"Model: Model A|Model B \\nPrompt: Prompt to utilize Model A?|Prompt to utilize Model B?\"\n",
        "        # \"Maintain the exact symbols in the template and do not include any additional text.\"\n",
        "        \"Select models to answer the following question:\\n\"\n",
        "        f\"{que}\\n\"\n",
        "        \"Select models only from the following model list: 'Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA' to answer the question.\\n\"\n",
        "        \"Generate corresponding shortest prompts for chosen models.\\n\"\n",
        "        \"Your response should only have two lines and be like: 'Model: ?| ?\\nPrompt: ?| ?' and no other words.\"\n",
        "    )\n",
        "    return sm\n",
        "\n",
        "def format_data(sample):\n",
        "    system_message = generate_SM(sample[0])\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        # {\n",
        "        #     \"role\": \"user\",\n",
        "        #     \"content\": [\n",
        "\n",
        "        #         {\n",
        "        #             \"type\": \"text\",\n",
        "        #             \"text\": sample[0],\n",
        "\n",
        "        #         }\n",
        "        #     ],\n",
        "        # },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[1]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "#system_message = generate_SM(test_dataset[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"HuggingFaceM4/ChartQA\"\n",
        "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=['train[:10%]', 'val[:10%]', 'test[:10%]'])\n",
        "\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "train_dataset[200], len(train_dataset), len(eval_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "e776ZyBAnZ5G",
        "outputId": "afdc7b27-8460-4804-ed0c-31de6149b63c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e776ZyBAnZ5G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'role': 'system',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'You are a Vision Language Model specialized in interpreting visual data from chart images.\\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
              "  {'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Is the rightmost value of light brown graph 58?'}]},\n",
              "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'No'}]}],\n",
              " 2830,\n",
              " 192,\n",
              " 250)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access the file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TextQuestionLabelDataset(Dataset):\n",
        "    def __init__(self, input_file):\n",
        "        # Load data from a CSV file\n",
        "        self.data = pd.read_csv(input_file)\n",
        "        #print(self.data.head())  # 显示前 5 行数据\n",
        "\n",
        "\n",
        "        # Extract questions and labels from the Input column\n",
        "        #self.questions = self.data['Input'].apply(lambda x: re.search(r'Question: (.+?)\\n', x).group(1)).tolist()\n",
        "        self.questions = self.data['Input'].tolist()\n",
        "        self.labels = self.data['Label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (question, label) given an index\n",
        "        question = self.questions[idx]\n",
        "        label = self.labels[idx]\n",
        "        return question, label\n",
        "\n",
        "input_file = '/content/drive/MyDrive/two-model.csv'\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = TextQuestionLabelDataset(input_file)\n",
        "\n",
        "# Split the dataset for train, eval, and test (e.g., 10% for each)\n",
        "dataset_length = len(dataset)\n",
        "#train_size = int(0.1 * dataset_length)\n",
        "train_size = 0\n",
        "#eval_size = int(0.1 * dataset_length)\n",
        "eval_size = 0\n",
        "test_size = dataset_length - train_size - eval_size\n",
        "#seed everything\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(\n",
        "#     dataset, [train_size, eval_size, test_size]\n",
        "# )\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, range(0, train_size))\n",
        "eval_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + eval_size))\n",
        "test_dataset = torch.utils.data.Subset(dataset, range(train_size + eval_size, len(dataset)))\n",
        "#print(train_dataset[0])\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "# eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "# test_dataset[0]\n",
        "# test_dataset[0]"
      ],
      "metadata": {
        "id": "0idS6tUnWhuz",
        "outputId": "447ac260-0b29-464a-c629-2f015ff1cfc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0idS6tUnWhuz",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "b309522e-b40b-4289-87e9-d28c27b81678",
      "metadata": {
        "id": "b309522e-b40b-4289-87e9-d28c27b81678"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
        "    # Prepare the text input by applying the chat template\n",
        "    text_input = processor.apply_chat_template(\n",
        "        sample[:1],  # Use the sample without the system message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    #print(\"Text Input:\", text_input)\n",
        "\n",
        "\n",
        "    # Process the visual input from the sample\n",
        "    # image_inputs, _ = process_vision_info(sample)\n",
        "    #image_inputs = sample[1]['content'][0]['image'].convert(\"RGB\")\n",
        "    image_data = sample[1]['content'][0].get('image')\n",
        "\n",
        "    if image_data is not None:\n",
        "        image_inputs = image_data.convert(\"RGB\")\n",
        "    else:\n",
        "        # 提供一个默认值或适当的处理逻辑\n",
        "        image_inputs = None\n",
        "        #print(\"Warning: The 'image' field is None.\")\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "    # Generate text with the model\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode the output text\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_text[0]  # Return the first decoded output text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
      "metadata": {
        "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
        "outputId": "207766b2-939a-444a-b4a4-0a9ce2cce3a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU allocated memory: 0.01 GB\n",
            "GPU reserved memory: 0.02 GB\n"
          ]
        }
      ],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
      "metadata": {
        "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
        "outputId": "5f8362ec-9fe3-478b-bc1d-2559a54991ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b756f3277e7e49eb826ca811c298ff91",
            "7ee5438c9e6b441d8d810dcde4ed5ecd",
            "04cc4550d49d4800830b1884cb447826",
            "9eb361a8b6884ea997a534f7eb3b4304",
            "63e3918c02dd4cf49efb53618b6352a0",
            "938de35885f04215803f819d6332056d",
            "0732d5e286b04e239dfad427d674d39a",
            "bd5a6082f03b4e7dbc6441664fc4f4d7",
            "c882c021f862450592bfa9be9fc8c9c9",
            "62e3bae476f34c32bddabd6fd4ce51ff",
            "c6d16d4025944216b4bb52eaec8038de"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b756f3277e7e49eb826ca811c298ff91"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from nltk.translate.meteor_score import meteor_score, single_meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import requests\n",
        "from torch import nn\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor, MllamaConfig, AutoModelForCausalLM\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # low_cpu_mem_usage=True,\n",
        "    # bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
        "model = MllamaForConditionalGeneration.from_pretrained(\n",
        "            model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True,\n",
        "            quantization_config=quantization_config,\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
        "    # Prepare the text input by applying the chat template\n",
        "    text_input = processor.apply_chat_template(\n",
        "        sample[:1],  # Use the sample without the system message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    #print(\"Text Input:\", text_input)\n",
        "\n",
        "\n",
        "    # Process the visual input from the sample\n",
        "    # image_inputs, _ = process_vision_info(sample)\n",
        "    #image_inputs = sample[1]['content'][0]['image'].convert(\"RGB\")\n",
        "    image_data = sample[1]['content'][0].get('image')\n",
        "\n",
        "    if image_data is not None:\n",
        "        image_inputs = image_data.convert(\"RGB\")\n",
        "    else:\n",
        "        # 提供一个默认值或适当的处理逻辑\n",
        "        image_inputs = None\n",
        "        #print(\"Warning: The 'image' field is None.\")\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "    # Generate text with the model\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode the output text\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_text[0]\n",
        "\n",
        "# Mount Google Drive to access the file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TextQuestionLabelDataset(Dataset):\n",
        "    def __init__(self, input_file):\n",
        "        # Load data from a CSV file\n",
        "        self.data = pd.read_csv(input_file)\n",
        "        # print(self.data.head())  # 显示前 5 行数据\n",
        "\n",
        "\n",
        "        # Extract questions and labels from the Input column\n",
        "        #self.questions = self.data['Input'].apply(lambda x: re.search(r'Question: (.+?)\\n', x).group(1)).tolist()\n",
        "        self.questions = self.data['Input'].tolist()\n",
        "        self.labels = self.data['Label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (question, label) given an index\n",
        "        question = self.questions[idx]\n",
        "        label = self.labels[idx]\n",
        "        return question, label\n",
        "\n",
        "input_file = '/content/drive/MyDrive/two-model.csv'\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = TextQuestionLabelDataset(input_file)\n",
        "\n",
        "# Split the dataset for train, eval, and test (e.g., 10% for each)\n",
        "dataset_length = len(dataset)\n",
        "#train_size = int(0.1 * dataset_length)\n",
        "train_size = 0\n",
        "#eval_size = int(0.1 * dataset_length)\n",
        "eval_size = 0\n",
        "test_size = dataset_length - train_size - eval_size\n",
        "\n",
        "#seed everything\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(\n",
        "#     dataset, [train_size, eval_size, test_size]\n",
        "# )\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, range(0, train_size))\n",
        "eval_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + eval_size))\n",
        "test_dataset = torch.utils.data.Subset(dataset, range(train_size + eval_size, len(dataset)))\n",
        "\n",
        "def generate_SM(que):\n",
        "    sm = (\n",
        "        # \"select 'Models' and shortest 'Prompts' as output based on the Question below?\\n\"\n",
        "        # f\"{que}\\n\"\n",
        "        # \"Model Options: [Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA]\\n\"\n",
        "        # \"Each prompt should only utilize one single model and should be as short as possible, the number of prompt should be equal to the number of model you select\"\n",
        "        # \"The format of your response should be like:'Model: Model A|Model B\\nPrompt: Prompt to utilize model A?|Prompt to utilize model B?\\n' Keep all symbols in the template and do not include any additional text.\"\n",
        "        # \"Select the appropriate 'Models' and the shortest possible 'Prompts' to address the following question:\"\n",
        "        # f\"{que}\"\n",
        "        # \"Model Options: [Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA]\"\n",
        "        # \"Each prompt must correspond to a single model and should be as concise as possible. The number of prompts should match the number of selected models.\"\n",
        "        # \"Format your response as follows:\"\n",
        "        # \"Model: Model A|Model B \\nPrompt: Prompt to utilize Model A?|Prompt to utilize Model B?\"\n",
        "        # \"Maintain the exact symbols in the template and do not include any additional text.\"\n",
        "        # \"Select the appropriate 'Models' and the shortest possible 'Prompts' to address the following question:\"\n",
        "        # f\"{que}\"\n",
        "        # \"Model Options: [Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA]\"\n",
        "        # \"if the question is 'Detect the sella in the video and overlay it on surgical scene?'\"\n",
        "        # \"Your output should be: 'Model: Segment-Video|Detect-Instrument\\n'Prompt: Segment sella?|Overlay segmentation result on surgical scene?\"\n",
        "        \"Select models to answer the following question:\\n\"\n",
        "        f\"{que}\\n\"\n",
        "        \"Select models only from the following model list: 'Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA' to answer the question.\\n\"\n",
        "        \"Generate corresponding shortest prompts for chosen models.\\n\"\n",
        "        \"Your response should only have two lines and be like: 'Model: ?| ?\\nPrompt: ?| ?' and no other words.\"\n",
        "        )\n",
        "    return sm\n",
        "\n",
        "\n",
        "# system_message = \"\"\" Select a model to answer the following question: What is the position of the instrument?\n",
        "# Select model only from the following model list: Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA to answer the question?\n",
        "# Generate a shortest prompt for the chosen model. Your response should be like Model: ? Prompt: ? and no other words\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# system_message = \"\"\" You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
        "# The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
        "# There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
        "# The agent may call more than one model sequentially or in parallel based on query.\n",
        "# Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
        "# If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
        "# Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "# If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
        "# Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "# \"\"\"\n",
        "# system_message = \"\"\"\n",
        "# Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
        "# If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
        "# Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "# If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
        "# Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "# \"\"\"\n",
        "\n",
        "# system_message=\"\"\"\n",
        "# Select models only from the following list:\n",
        "# ['Segment-Video', 'Segment-MRI', 'Detect-Video', 'Overlaying', 'Surgical-VQA'].\n",
        "# For each input question, determine the required models and generate corresponding prompts.\n",
        "\n",
        "# Output Format:\n",
        "# - For a single model:\n",
        "#   Model: <ModelName>\n",
        "#   Prompt: <CorrespondingPrompt>?\n",
        "\n",
        "# - For multiple models:\n",
        "#   Model: <Model1>|<Model2>\n",
        "#   Prompt: <Prompt1>?|<Prompt2>?\n",
        "# \"\"\"\n",
        "\n",
        "def format_data_test1(sample):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        # {\n",
        "        #     \"role\": \"user\",\n",
        "        #     \"content\": [\n",
        "\n",
        "        #         {\n",
        "        #             \"type\": \"text\",\n",
        "        #             \"text\": sample[0],\n",
        "\n",
        "        #         }\n",
        "        #     ],\n",
        "        # },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[1]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "system_message = generate_SM(test_dataset[2][0])\n",
        "print(test_dataset[1][0])\n",
        "input1 = format_data_test1(test_dataset[2])\n",
        "input2 = format_data_test1(test_dataset[2])\n",
        "print(input1)\n",
        "#print(input2)\n",
        "# print(\"input: \", input)\n",
        "# text_input = processor.apply_chat_template(\n",
        "#         input[:2],  # Use the sample without the system message\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#     )\n",
        "# print(\"text_input: \", text_input)\n",
        "# input[:2]\n",
        "output1 = generate_text_from_sample(model, processor, input1)\n",
        "ans1 = input1[1]['content'][0]['text']\n",
        "print(\"output1: \",output1)\n",
        "print(\"answer1: \",ans1)\n",
        "# print(\"--------------------------------------------\")\n",
        "# output2 = generate_text_from_sample(model, processor, input1)\n",
        "# ans2 = input2[2]['content'][0]['text']\n",
        "# print(\"output2: \",output2)\n",
        "# print(\"answer2: \",ans2)"
      ],
      "metadata": {
        "id": "qBxeOXNGbIhV",
        "outputId": "4bb39ee3-ef02-47b0-d928-51583d8ce852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qBxeOXNGbIhV",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Question: Use available tools to analyze and enhance the current surgical view.\n",
            "[{'role': 'system', 'content': [{'type': 'text', 'text': \"Select models to answer the following question:\\nQuestion: Overlay vascular landmarks and answer questions about their proximity to the tumor.\\nSelect models only from the following model list: 'Segment-Video, Segment-MRI, Detect-Instrument, Overlaying, Surgical-VQA' to answer the question.\\nGenerate corresponding shortest prompts for chosen models.\\nYour response should only have two lines and be like: 'Model: ?| ?\\nPrompt: ?| ?' and no other words.\"}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Model: Overlaying|Surgical-VQA\\nPrompt: Overlay vascular landmarks?|Answer questions about proximity to tumor?'}]}]\n",
            "output1:  Model: Segment-MRI| Overlaying\n",
            "Prompt: Show the MRI image and the vascular landmarks. Ask questions about the proximity of the landmarks to the tumor.\n",
            "answer1:  Model: Overlaying|Surgical-VQA\n",
            "Prompt: Overlay vascular landmarks?|Answer questions about proximity to tumor?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BUAHFJR8VWFr"
      },
      "id": "BUAHFJR8VWFr"
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "all_ans = []"
      ],
      "metadata": {
        "id": "q7PWaz-aZU-B"
      },
      "id": "q7PWaz-aZU-B",
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sample in test_dataset:\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[1]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n"
      ],
      "metadata": {
        "id": "MGlopdzQf0t1"
      },
      "id": "MGlopdzQf0t1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "start_index = 0\n",
        "# 设置保存路径\n",
        "save_path = '/content/drive/My Drive/Sharing/two-model.pkl'\n",
        "\n",
        "# 尝试加载之前的进度\n",
        "if os.path.exists(save_path):\n",
        "    with open(save_path, 'rb') as f:\n",
        "        saved_data = pickle.load(f)\n",
        "        start_index = saved_data['start_index']\n",
        "        all_pred = saved_data['all_pred']\n",
        "        all_ans = saved_data['all_ans']\n",
        "        print(f\"Resuming from index {start_index}\")\n",
        "else:\n",
        "    start_index = 0\n",
        "    all_pred = []\n",
        "    all_ans = []\n",
        "    print(\"Starting fresh\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2zOqW3f96W",
        "outputId": "aaf3220b-97d1-4161-ab2b-f5506999621b"
      },
      "id": "JR2zOqW3f96W",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting fresh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, sample in enumerate(test_dataset):\n",
        "        # 跳过已处理的样本\n",
        "        if i < start_index:\n",
        "            continue\n",
        "\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[1]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n",
        "        # 更新进度\n",
        "        start_index = i + 1\n",
        "        saved_data = {\n",
        "            'start_index': start_index,\n",
        "            'all_pred': all_pred,\n",
        "            'all_ans': all_ans\n",
        "        }\n",
        "\n",
        "        # 保存到Google Drive\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(saved_data, f)\n",
        "\n",
        "        print(f\"Processed sample {i+1}/{len(test_dataset)}\")\n",
        "\n",
        "print(\"All samples processed!\")\n"
      ],
      "metadata": {
        "id": "TUO0A8QbFjF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9472b207-10ee-4ff9-a492-2d099ba2bc78"
      },
      "id": "TUO0A8QbFjF3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample 3/2500\n",
            "Processed sample 4/2500\n",
            "Processed sample 5/2500\n",
            "Processed sample 6/2500\n",
            "Processed sample 7/2500\n",
            "Processed sample 8/2500\n",
            "Processed sample 9/2500\n",
            "Processed sample 10/2500\n",
            "Processed sample 11/2500\n",
            "Processed sample 12/2500\n",
            "Processed sample 13/2500\n",
            "Processed sample 14/2500\n",
            "Processed sample 15/2500\n",
            "Processed sample 16/2500\n",
            "Processed sample 17/2500\n",
            "Processed sample 18/2500\n",
            "Processed sample 19/2500\n",
            "Processed sample 20/2500\n",
            "Processed sample 21/2500\n",
            "Processed sample 22/2500\n",
            "Processed sample 23/2500\n",
            "Processed sample 24/2500\n",
            "Processed sample 25/2500\n",
            "Processed sample 26/2500\n",
            "Processed sample 27/2500\n",
            "Processed sample 28/2500\n",
            "Processed sample 29/2500\n",
            "Processed sample 30/2500\n",
            "Processed sample 31/2500\n",
            "Processed sample 32/2500\n",
            "Processed sample 33/2500\n",
            "Processed sample 34/2500\n",
            "Processed sample 35/2500\n",
            "Processed sample 36/2500\n",
            "Processed sample 37/2500\n",
            "Processed sample 38/2500\n",
            "Processed sample 39/2500\n",
            "Processed sample 40/2500\n",
            "Processed sample 41/2500\n",
            "Processed sample 42/2500\n",
            "Processed sample 43/2500\n",
            "Processed sample 44/2500\n",
            "Processed sample 45/2500\n",
            "Processed sample 46/2500\n",
            "Processed sample 47/2500\n",
            "Processed sample 48/2500\n",
            "Processed sample 49/2500\n",
            "Processed sample 50/2500\n",
            "Processed sample 51/2500\n",
            "Processed sample 52/2500\n",
            "Processed sample 53/2500\n",
            "Processed sample 54/2500\n",
            "Processed sample 55/2500\n",
            "Processed sample 56/2500\n",
            "Processed sample 57/2500\n",
            "Processed sample 58/2500\n",
            "Processed sample 59/2500\n",
            "Processed sample 60/2500\n",
            "Processed sample 61/2500\n",
            "Processed sample 62/2500\n",
            "Processed sample 63/2500\n",
            "Processed sample 64/2500\n",
            "Processed sample 65/2500\n",
            "Processed sample 66/2500\n",
            "Processed sample 67/2500\n",
            "Processed sample 68/2500\n",
            "Processed sample 69/2500\n",
            "Processed sample 70/2500\n",
            "Processed sample 71/2500\n",
            "Processed sample 72/2500\n",
            "Processed sample 73/2500\n",
            "Processed sample 74/2500\n",
            "Processed sample 75/2500\n",
            "Processed sample 76/2500\n",
            "Processed sample 77/2500\n",
            "Processed sample 78/2500\n",
            "Processed sample 79/2500\n",
            "Processed sample 80/2500\n",
            "Processed sample 81/2500\n",
            "Processed sample 82/2500\n",
            "Processed sample 83/2500\n",
            "Processed sample 84/2500\n",
            "Processed sample 85/2500\n",
            "Processed sample 86/2500\n",
            "Processed sample 87/2500\n",
            "Processed sample 88/2500\n",
            "Processed sample 89/2500\n",
            "Processed sample 90/2500\n",
            "Processed sample 91/2500\n",
            "Processed sample 92/2500\n",
            "Processed sample 93/2500\n",
            "Processed sample 94/2500\n",
            "Processed sample 95/2500\n",
            "Processed sample 96/2500\n",
            "Processed sample 97/2500\n",
            "Processed sample 98/2500\n",
            "Processed sample 99/2500\n",
            "Processed sample 100/2500\n",
            "Processed sample 101/2500\n",
            "Processed sample 102/2500\n",
            "Processed sample 103/2500\n",
            "Processed sample 104/2500\n",
            "Processed sample 105/2500\n",
            "Processed sample 106/2500\n",
            "Processed sample 107/2500\n",
            "Processed sample 108/2500\n",
            "Processed sample 109/2500\n",
            "Processed sample 110/2500\n",
            "Processed sample 111/2500\n",
            "Processed sample 112/2500\n",
            "Processed sample 113/2500\n",
            "Processed sample 114/2500\n",
            "Processed sample 115/2500\n",
            "Processed sample 116/2500\n",
            "Processed sample 117/2500\n",
            "Processed sample 118/2500\n",
            "Processed sample 119/2500\n",
            "Processed sample 120/2500\n",
            "Processed sample 121/2500\n",
            "Processed sample 122/2500\n",
            "Processed sample 123/2500\n",
            "Processed sample 124/2500\n",
            "Processed sample 125/2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
      "metadata": {
        "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
        "outputId": "636a8f9c-316a-4ea4-b3ff-127c3c52bfe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: Model: Segment-MRI| Segment-Video\n",
            "Prompt: Segment the nasal cavity structures and provide a detailed 3D reconstruction. Overlay the surgical pathway for better guidance and patient education.\n",
            "ans: Model: Segment-Video|Overlaying\n",
            "Prompt: Segment nasal cavity structures?|Overlay surgical pathway?\n",
            "pred: Model: Segment-MRI| Segment-Video\n",
            "Prompt: Analyze the current surgical view using MRI and video data. Enhance the view for better visualization.\n",
            "ans: Model: Segment-Video|Segment-MRI\n",
            "Prompt: Use Segment-Video functionality?|Use Segment-MRI functionality?\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(all_pred)):\n",
        "    print(\"pred:\", all_pred[i])\n",
        "    print(\"ans:\", all_ans[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File Path\n",
        "output_file_path = '/content/drive/My Drive/pre_ans_SINGLE.txt'\n",
        "\n",
        "# Write data\n",
        "with open(output_file_path, 'w') as file:\n",
        "    for i in range(len(all_pred)):\n",
        "        file.write(f\"pred: {all_pred[i]}\\n\")\n",
        "        file.write(f\"ans: {all_ans[i]}\\n\")\n",
        "        file.write(\"\\n\")  # 添加换行以便于阅读\n",
        "\n",
        "print(f\"file saved at: {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlQxn84Jt15m",
        "outputId": "2add3bb0-2156-4c5c-e3df-958098b39f42"
      },
      "id": "hlQxn84Jt15m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "file saved at: /content/drive/My Drive/pre_ans_SINGLE.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义新的变量来存储提取的 Prompt 部分\n",
        "all_pred_prompts = []\n",
        "all_ans_prompts = []\n",
        "\n",
        "# 提取 Prompt 部分的函数\n",
        "# def extract_prompt(entry):\n",
        "#     for line in entry.split('\\n'):\n",
        "#         if line.startswith(\"Prompt:\"):\n",
        "#             return line[len(\"Prompt: \"):]  # 去掉 \"Prompt: \" 保留内容\n",
        "\n",
        "def extract_prompt(entry):\n",
        "    # 用列表解析找到所有以 \"Prompt:\" 开头的行，并提取内容\n",
        "    prompts = [line[len(\"Prompt: \"):].strip() for line in entry.split('\\n') if line.startswith(\"Prompt:\")]\n",
        "    # 用 \"|\" 将所有提取的内容连接成一个字符串\n",
        "    return \"|\".join(prompts) if prompts else \"\"\n",
        "\n",
        "# 提取 all_pred 和 all_ans 中的 Prompt 部分\n",
        "all_pred_prompts = [extract_prompt(item) for item in all_pred]\n",
        "all_ans_prompts = [extract_prompt(item) for item in all_ans]\n",
        "\n",
        "# 打印结果验证\n",
        "print(\"Extracted Prompts from all_pred:\", all_pred_prompts)\n",
        "print(\"Extracted Prompts from all_ans:\", all_ans_prompts)\n",
        "print(len(all_pred_prompts))\n",
        "print(len(all_ans_prompts))\n"
      ],
      "metadata": {
        "id": "EYED9uSy5v6K",
        "outputId": "907ad4dc-1143-4ac6-ada3-c62385759eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EYED9uSy5v6K",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Prompts from all_pred: ['Segment the nasal cavity structures and provide a detailed 3D reconstruction. Overlay the surgical pathway for better guidance and patient education.', 'Analyze the current surgical view using MRI and video data. Enhance the view for better visualization.', 'Identify vascular landmarks on MRI images and calculate their proximity to a tumor.', 'Analyze the current surgical view to enhance its quality and visibility.', 'Identify the vascular landmarks in the MRI and video images and determine their proximity to the tumor.', 'Provide an MRI image of the tumor and ask about the proximity of vascular landmarks to the tumor. | Use the MRI image to answer questions about the proximity of vascular landmarks to the tumor.', 'Analyze and enhance the current surgical view using AI-powered video segmentation techniques to improve clarity and visibility.|Identify and highlight the surgical instruments in the current view to aid in precision and accuracy during the procedure.', 'Detect the tumor margin and provide a contour map. Detect the suction tool and track its movement relative to the tumor margin contour map.', 'Analyze the current surgical view using MRI and video segmentation tools to enhance the image quality and provide a clear view of the surgical site.', 'Highlight the brain tumor from the MRI images and identify the position of surgical instruments near it.']\n",
            "Extracted Prompts from all_ans: ['Segment nasal cavity structures?|Overlay surgical pathway?', 'Use Segment-Video functionality?|Use Segment-MRI functionality?', 'Overlay vascular landmarks?|Answer questions about proximity to tumor?', 'Use Segment-Video functionality?|Use Segment-MRI functionality?', 'Overlay vascular landmarks?|Answer questions about proximity to tumor?', 'Overlay vascular landmarks?|Answer questions about proximity to tumor?', 'Use Segment-MRI functionality?|Use Overlaying functionality?', 'Detect suction tool?|Overlay its path relative to tumor margin?', 'Use Segment-MRI functionality?|Use Surgical-VQA functionality?', 'Segment brain tumor from MRI?|Identify position of surgical instruments?']\n",
            "10\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
      "metadata": {
        "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
        "outputId": "eef25a29-8d58-45b0-e42d-e5716dc3593d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393,
          "referenced_widgets": [
            "b18f108956364693b8c2d58417387510",
            "f837eb2b34274dfe81d3e151e429cc03",
            "e26cf7b811c8408f8a953276ced891fa",
            "97553894905f46c0af5ae0989a3a41ed",
            "a392dce0f3d549faad1d39d26533ecfc",
            "9e6b0925bb184991829fa53eff7c3918",
            "27ab62b72b4c4af591591cc84aae1094",
            "1240a9f75e9b42dc9669c13981e78bf3",
            "08ff7798ef01438f93a4ac8e4f1b56ca",
            "f66ced64d47d4a208e6f06f1f636271d",
            "5d3f2408d79040d98670d508f0b2dfdb"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=258e708561a3fcabae4d32a0b264a8132e950e82ee1f1c4af45a89c31afe61bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b18f108956364693b8c2d58417387510"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.632200615738131, 'rouge2': 0.4359985900655008, 'rougeL': 0.6150176283554245, 'rougeLsum': 0.6148034171383235}\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions= all_pred_prompts, references=all_ans_prompts)\n",
        "print(rouge_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
      "metadata": {
        "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
        "outputId": "e4b91d4f-53e1-4be8-ee40-310bc3861b5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.290469509202878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "bleu_score = corpus_bleu(all_ans_prompts, all_pred_prompts, weights=(1.0, 0.0, 0.0, 0.0))\n",
        "print(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
      "metadata": {
        "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
        "outputId": "25f53c61-13de-45c1-dda6-03ffa12ba485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.647860450438184\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "m_score=0\n",
        "for line in zip(all_ans_prompts, all_pred_prompts):\n",
        "    ref = word_tokenize(line[0])\n",
        "    hypo = word_tokenize(line[1])\n",
        "    m_score += meteor_score([ref], hypo)\n",
        "meteors = m_score/len(all_ans)\n",
        "print(meteors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract the \"Model\" part from a given text\n",
        "# def extract_model(text):\n",
        "#     match = re.search(r\"Model:\\s*(.*?)\\n\", text)\n",
        "#     return match.group(1) if match else \"\"\n",
        "\n",
        "def extract_model(text):\n",
        "    # 使用正则表达式找到所有 \"Model:\" 后面紧跟的名称\n",
        "    matches = re.findall(r\"Model:\\s*(.*?)\\n\", text)\n",
        "    # 去除每个模型名称中的所有空格，并按顺序用 \"|\" 连接\n",
        "    cleaned_matches = [\"\".join(match.split()) for match in matches]\n",
        "    return \"|\".join(cleaned_matches) if cleaned_matches else \"\"\n",
        "\n",
        "# Function to calculate exact match accuracy\n",
        "def calculate_exact_match_accuracy(predictions, answers):\n",
        "    # Extract and strip spaces from the models\n",
        "    pred_models = [extract_model(pred).strip() for pred in predictions]\n",
        "    true_models = [extract_model(ans).strip() for ans in answers]\n",
        "\n",
        "    # Debug prints for checking values\n",
        "    print(pred_models)\n",
        "    print(true_models)\n",
        "\n",
        "    correct_matches = sum(p == t for p, t in zip(pred_models, true_models))\n",
        "    total = len(true_models)\n",
        "    return correct_matches / total if total > 0 else 0\n",
        "\n",
        "# Calculate exact match accuracy\n",
        "accuracy = calculate_exact_match_accuracy(all_pred, all_ans)\n",
        "print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0RmPYe-zHvfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c589caa8-dbde-4473-cb73-4e44e096c32a"
      },
      "id": "0RmPYe-zHvfP",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Segment-MRI|Segment-Video', 'Segment-MRI|Segment-Video', 'Segment-MRI|Overlaying', 'Segment-Video|Segment-MRI', 'Segment-MRI|Segment-Video', 'Segment-MRI|Overlaying', 'Segment-Video|Detect-Instrument', 'Segment-MRI|Detect-Instrument', 'Segment-MRI|Segment-Video', 'Segment-MRI|Segment-Video']\n",
            "['Segment-Video|Overlaying', 'Segment-Video|Segment-MRI', 'Overlaying|Surgical-VQA', 'Segment-Video|Segment-MRI', 'Overlaying|Surgical-VQA', 'Overlaying|Surgical-VQA', 'Segment-MRI|Overlaying', 'Detect-Instrument|Overlaying', 'Segment-MRI|Surgical-VQA', 'Segment-MRI|Detect-Instrument']\n",
            "Exact Match Accuracy: 10.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa",
      "metadata": {
        "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6017dd69-1978-417f-93fe-0393b4cc3f74",
      "metadata": {
        "id": "6017dd69-1978-417f-93fe-0393b4cc3f74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25",
      "metadata": {
        "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c",
      "metadata": {
        "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b18f108956364693b8c2d58417387510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f837eb2b34274dfe81d3e151e429cc03",
              "IPY_MODEL_e26cf7b811c8408f8a953276ced891fa",
              "IPY_MODEL_97553894905f46c0af5ae0989a3a41ed"
            ],
            "layout": "IPY_MODEL_a392dce0f3d549faad1d39d26533ecfc"
          }
        },
        "f837eb2b34274dfe81d3e151e429cc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e6b0925bb184991829fa53eff7c3918",
            "placeholder": "​",
            "style": "IPY_MODEL_27ab62b72b4c4af591591cc84aae1094",
            "value": "Downloading builder script: 100%"
          }
        },
        "e26cf7b811c8408f8a953276ced891fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1240a9f75e9b42dc9669c13981e78bf3",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08ff7798ef01438f93a4ac8e4f1b56ca",
            "value": 6270
          }
        },
        "97553894905f46c0af5ae0989a3a41ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66ced64d47d4a208e6f06f1f636271d",
            "placeholder": "​",
            "style": "IPY_MODEL_5d3f2408d79040d98670d508f0b2dfdb",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 398kB/s]"
          }
        },
        "a392dce0f3d549faad1d39d26533ecfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6b0925bb184991829fa53eff7c3918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ab62b72b4c4af591591cc84aae1094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1240a9f75e9b42dc9669c13981e78bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ff7798ef01438f93a4ac8e4f1b56ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f66ced64d47d4a208e6f06f1f636271d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d3f2408d79040d98670d508f0b2dfdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b756f3277e7e49eb826ca811c298ff91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ee5438c9e6b441d8d810dcde4ed5ecd",
              "IPY_MODEL_04cc4550d49d4800830b1884cb447826",
              "IPY_MODEL_9eb361a8b6884ea997a534f7eb3b4304"
            ],
            "layout": "IPY_MODEL_63e3918c02dd4cf49efb53618b6352a0"
          }
        },
        "7ee5438c9e6b441d8d810dcde4ed5ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_938de35885f04215803f819d6332056d",
            "placeholder": "​",
            "style": "IPY_MODEL_0732d5e286b04e239dfad427d674d39a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "04cc4550d49d4800830b1884cb447826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd5a6082f03b4e7dbc6441664fc4f4d7",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c882c021f862450592bfa9be9fc8c9c9",
            "value": 5
          }
        },
        "9eb361a8b6884ea997a534f7eb3b4304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e3bae476f34c32bddabd6fd4ce51ff",
            "placeholder": "​",
            "style": "IPY_MODEL_c6d16d4025944216b4bb52eaec8038de",
            "value": " 5/5 [01:30&lt;00:00, 15.59s/it]"
          }
        },
        "63e3918c02dd4cf49efb53618b6352a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "938de35885f04215803f819d6332056d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0732d5e286b04e239dfad427d674d39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd5a6082f03b4e7dbc6441664fc4f4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c882c021f862450592bfa9be9fc8c9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62e3bae476f34c32bddabd6fd4ce51ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6d16d4025944216b4bb52eaec8038de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}