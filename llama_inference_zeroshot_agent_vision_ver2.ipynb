{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterHJY628/MyOwnExample/blob/main/llama_inference_zeroshot_agent_vision_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"torch==2.4.0\" tensorboard pillow torchvision accelerate huggingface_hub\n",
        "!pip -q install  --upgrade \\\n",
        "  \"transformers==4.45.1\" \\\n",
        "  \"datasets==3.0.1\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.44.0\" \\\n",
        "  \"trl==0.11.1\" \\\n",
        "  \"peft==0.13.0\" \\\n",
        "  \"qwen_vl_utils\""
      ],
      "metadata": {
        "id": "31hYynMQalOW",
        "outputId": "c854f84c-78da-4ce9-b05d-334c5a3baeba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "31hYynMQalOW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6XTxQsAahAV",
        "outputId": "b50892a6-0bf5-408a-99b2-9898e0f2b83f"
      },
      "id": "j6XTxQsAahAV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
      "metadata": {
        "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
        "outputId": "32ae2eab-ec95-4497-8f6a-6987e486cbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/sa5u24/VQA\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['HF_HOME'] = '/home/sa5u24/VQA'\n",
        "hf_home = os.path.expanduser(\n",
        "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
        ")\n",
        "print(hf_home)\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your-hf-token-here' with your actual Hugging Face token\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6",
      "metadata": {
        "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\" an LLM agent which can call functions and corresponding prompt based on surgeons' query.  The agent will be used  in endonasal pituitary surgery to call visual models Segmentation_Video, Segmentation_MRI, Detection,  Overlaying, Surgical-VQA. There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes. The agent will call more than one model sequentially or in parallel based on query.\n",
        "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
        "If you receive the input like: \"Question: Detect instruments in video and overlay diagnostic tumor segmentation?\" The format of the answer should be like this: \"Model: Detection|Segmentation_Video\n",
        "Prompt: Detect instruments in video?|Overlay diagnostic tumor segmentation?\"\n",
        "\"\"\"\n",
        "\n",
        "def format_data(sample):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"Agent\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                # {\n",
        "                #     \"type\": \"image\",\n",
        "                #     \"image\": None,\n",
        "                # },\n",
        "                # {\n",
        "                #     \"type\": \"image\",\n",
        "                #     \"image\": sample[\"image\"],\n",
        "                # },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[0]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[1]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"HuggingFaceM4/ChartQA\"\n",
        "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=['train[:10%]', 'val[:10%]', 'test[:10%]'])\n",
        "\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "train_dataset[200], len(train_dataset), len(eval_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "e776ZyBAnZ5G",
        "outputId": "afdc7b27-8460-4804-ed0c-31de6149b63c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e776ZyBAnZ5G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'role': 'system',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'You are a Vision Language Model specialized in interpreting visual data from chart images.\\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
              "  {'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Is the rightmost value of light brown graph 58?'}]},\n",
              "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'No'}]}],\n",
              " 2830,\n",
              " 192,\n",
              " 250)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch  # Import torch for dataset splitting and other PyTorch functionalities\n",
        "from torch.utils.data import Dataset, DataLoader  # For creating PyTorch Dataset and DataLoader\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access the file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TextQuestionLabelDataset(Dataset):\n",
        "    def __init__(self, input_file):\n",
        "        # Load data from a CSV file\n",
        "        self.data = pd.read_csv(input_file)\n",
        "        print(self.data.head())  # 显示前 5 行数据\n",
        "\n",
        "\n",
        "        # Extract questions and labels from the Input column\n",
        "        #self.questions = self.data['Input'].apply(lambda x: re.search(r'Question: (.+?)\\n', x).group(1)).tolist()\n",
        "        self.questions = self.data['Input'].tolist()\n",
        "        self.labels = self.data['Label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (question, label) given an index\n",
        "        question = self.questions[idx]\n",
        "        label = self.labels[idx]\n",
        "        return question, label\n",
        "\n",
        "input_file = '/content/drive/My Drive/test50.csv'\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = TextQuestionLabelDataset(input_file)\n",
        "\n",
        "# Split the dataset for train, eval, and test (e.g., 10% for each)\n",
        "dataset_length = len(dataset)\n",
        "train_size = int(0.1 * dataset_length)\n",
        "eval_size = int(0.1 * dataset_length)\n",
        "test_size = dataset_length - train_size - eval_size\n",
        "\n",
        "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, eval_size, test_size]\n",
        ")\n",
        "print(train_dataset[0])\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# print(\"Train Dataset Example:\")\n",
        "# for question, label in train_loader:\n",
        "#     print(\"Question:\", question)\n",
        "#     print(\"Label:\", label)\n",
        "#     break  # Print only the first batch for demonstration\n",
        "\n",
        "# print(\"\\nEvaluation Dataset Example:\")\n",
        "# for question, label in eval_loader:\n",
        "#     print(\"Question:\", question)\n",
        "#     print(\"Label:\", label)\n",
        "#     break\n",
        "\n",
        "# print(\"\\nTest Dataset Example:\")\n",
        "# for question, label in test_loader:\n",
        "#     print(\"Question:\", question)\n",
        "#     print(\"Label:\", label)\n",
        "#     break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0idS6tUnWhuz",
        "outputId": "2e31487d-44f1-4c5d-e31b-0c3b023c206f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0idS6tUnWhuz",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Index                          Type  \\\n",
            "0      1  Detection|Segmentation_Video   \n",
            "1      2                  Surgical-VQA   \n",
            "2      3                   Segment-MRI   \n",
            "3      4                    Overlaying   \n",
            "4      5               Detection-Video   \n",
            "\n",
            "                                               Input  \\\n",
            "0  Question: Detect instruments in video and over...   \n",
            "1   Question: Analyze the instruments in this phase.   \n",
            "2  Question: Locate and segment the optic protube...   \n",
            "3  Question: Merge the tumor segmentation images ...   \n",
            "4  Question: Can you identify the instruments pre...   \n",
            "\n",
            "                                               Label  \n",
            "0  Model: Detection|Segmentation_Video\\nPrompt: D...  \n",
            "1  Model: Surgical-VQA\\nPrompt: analyze the instr...  \n",
            "2  Model: Segment-MRI\\nPrompt: Segment optic prot...  \n",
            "3  Model: Overlaying\\nPrompt: Overlay tumor segme...  \n",
            "4  Model: Detection-Video\\nPrompt: can you identi...  \n",
            "('Question: Locate and segment Clival Recess from Video?', 'Model: Segment-Video\\nPrompt: Segment clival recess?')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b309522e-b40b-4289-87e9-d28c27b81678",
      "metadata": {
        "id": "b309522e-b40b-4289-87e9-d28c27b81678"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
        "    # Prepare the text input by applying the chat template\n",
        "    text_input = processor.apply_chat_template(\n",
        "        sample[1:2],  # Use the sample without the system message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Process the visual input from the sample\n",
        "    # image_inputs, _ = process_vision_info(sample)\n",
        "    #image_inputs = sample[1]['content'][0]['image'].convert(\"RGB\")\n",
        "    image_data = sample[1]['content'][0].get('image')\n",
        "\n",
        "    if image_data is not None:\n",
        "        image_inputs = image_data.convert(\"RGB\")\n",
        "    else:\n",
        "        # 提供一个默认值或适当的处理逻辑\n",
        "        image_inputs = None\n",
        "        #print(\"Warning: The 'image' field is None.\")\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "    # Generate text with the model\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode the output text\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_text[0]  # Return the first decoded output text\n",
        "\n",
        "# def generate_text_from_sample(model, processor, question, max_new_tokens=1024, device=\"cuda\"):\n",
        "#     \"\"\"\n",
        "#     Generate text from a sample using the model and processor.\n",
        "\n",
        "#     Args:\n",
        "#         model: The model used for generation.\n",
        "#         processor: The processor for preparing inputs.\n",
        "#         question: The input question as a string.\n",
        "#         max_new_tokens: The maximum number of tokens to generate.\n",
        "#         device: The device to use for computation (\"cuda\" or \"cpu\").\n",
        "\n",
        "#     Returns:\n",
        "#         A string containing the generated output text.\n",
        "#     \"\"\"\n",
        "#     # Prepare the text input directly from the question\n",
        "#     text_input = processor.apply_chat_template(\n",
        "#         [question],  # Directly use the question\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#     )\n",
        "\n",
        "#     # Handle the case where no image is provided\n",
        "#     image_inputs = None  # No visual input in the current dataset\n",
        "\n",
        "#     # Prepare the inputs for the model\n",
        "#     model_inputs = processor(\n",
        "#         text=[text_input],\n",
        "#         images=image_inputs,\n",
        "#         return_tensors=\"pt\",\n",
        "#     ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "#     # Generate text with the model\n",
        "#     generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "#     # Trim the generated ids to remove the input ids\n",
        "#     trimmed_generated_ids = [\n",
        "#         out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "#     ]\n",
        "\n",
        "#     # Decode the output text\n",
        "#     output_text = processor.batch_decode(\n",
        "#         trimmed_generated_ids,\n",
        "#         skip_special_tokens=True,\n",
        "#         clean_up_tokenization_spaces=False\n",
        "#     )\n",
        "\n",
        "#     return output_text[0]  # Return the first decoded output text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
      "metadata": {
        "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
        "outputId": "d9b79391-c3fb-4032-945f-e5413e2826ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU allocated memory: 7.23 GB\n",
            "GPU reserved memory: 7.33 GB\n"
          ]
        }
      ],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
      "metadata": {
        "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
        "outputId": "83959a47-cf0b-4eed-ab9e-26ff47581fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "0d43e45de30249ff973f0cf438a8386f",
            "6f26e698ca7844b09fdc754ce5ce5008",
            "66114edbaa08437b959a1cb3c17f7045",
            "0f5ca80673bb42a1b7e7f929237325f8",
            "4a83ed1a3a37439f9bd7577755641a88",
            "168d25d4bdd5470eb56ddc5289511114",
            "e9dd399fbebc4239b8ffec968c8fc951",
            "3ab68c4fe8a24e61878b52e1b2499142",
            "41e930af5ae241ec8fdb30707b7ea339",
            "045639e092ac4049b4fa34baf236d868",
            "b2ecc7e3300e4edd92d79518403fcb18"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d43e45de30249ff973f0cf438a8386f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from nltk.translate.meteor_score import meteor_score, single_meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import requests\n",
        "from torch import nn\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor, MllamaConfig, AutoModelForCausalLM\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # low_cpu_mem_usage=True,\n",
        "    # bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = MllamaForConditionalGeneration.from_pretrained(\n",
        "            model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True,\n",
        "            quantization_config=quantization_config,\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "all_ans = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sample in test_dataset:\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[2]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n",
        "# all_pred = []\n",
        "# all_ans = []\n",
        "\n",
        "# model.eval()  # 切换到评估模式\n",
        "# with torch.no_grad():  # 禁用梯度计算\n",
        "#     for question, label in test_loader:  # 遍历测试数据加载器\n",
        "#         # 将问题输入模型，生成预测结果\n",
        "#         # 假设 generate_text_from_sample 是用于推理的函数\n",
        "#         output = [generate_text_from_sample(model, processor, q) for q in question]\n",
        "\n",
        "#         # 将预测结果和真实标签保存\n",
        "#         all_pred.extend(output)\n",
        "#         all_ans.extend(label)\n",
        "\n",
        "# # 打印结果，或者使用 all_pred 和 all_ans 计算指标\n",
        "# print(\"Predictions:\", all_pred)\n",
        "# print(\"Answers:\", all_ans)\n"
      ],
      "metadata": {
        "id": "q7PWaz-aZU-B"
      },
      "id": "q7PWaz-aZU-B",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
      "metadata": {
        "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
        "outputId": "8235d7c6-ef81-49cf-95fb-5e4120a55944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: I donassistant\n",
            "\n",
            "The clival recess is an anatomical structure in the human body, specifically in the cranial cavity. It is a recess or a groove in the base of the skull, extending from the foramen magnum (the opening at the base of the skull through which the spinal cord passes) upwards to the foramen magnum, and it is bounded by the clivus, which is a bony structure formed by the fusion of the occipital bone and the sphenoid bone.\n",
            "\n",
            "The clival recess is divided into several segments based on its relationship to the surrounding structures and its anatomical boundaries. Here is a general segmentation of the clival recess:\n",
            "\n",
            "1. **Anterior Segment**: This is the anterior (front) part of the clival recess, bounded by the anterior margin of the foramen magnum and the anterior surface of the clivus. This segment is relatively short and is located in front of the clivus.\n",
            "\n",
            "2. **Middle Segment**: This is the middle part of the clival recess, bounded by the anterior and posterior margins of the foramen magnum and the posterior surface of the clivus. This segment is the longest part of the clival recess and contains several important structures such as the vertebral arteries, the basilar artery, and the abducens nerve.\n",
            "\n",
            "3. **Posterior Segment**: This is the posterior (rear) part of the clival recess, bounded by the posterior margin of the foramen magnum and the posterior surface of the clivus. This segment is relatively short and is located behind the clivus.\n",
            "\n",
            "4. **Upper Segment**: This is the upper part of the clival recess, bounded by the upper surface of the clivus and the margin of the foramen magnum. This segment is relatively short and is located above the clivus.\n",
            "\n",
            "5. **Lower Segment**: This is the lower part of the clival recess, bounded by the lower surface of the clivus and the margin of the foramen magnum. This segment is relatively short and is located below the clivus.\n",
            "\n",
            "It's worth noting that these segments are not strictly defined and can vary slightly from person to person. The clival recess is a complex structure that contains several important structures, including the vertebral arteries, the basilar artery, and the abducens nerve, and its segmentation is based on its anatomical boundaries and relationships to these structures.\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Identify clival recess?\n",
            "pred: I'll provide a general overview of the key differences between a diagnosis MRI and a surgical MRI, as well as the differences between a diagnosis MRI and a surgical MRI.\n",
            "\n",
            "**Diagnosis MRI (Diagnostic MRI):**\n",
            "\n",
            "A diagnosis MRI is used to create detailed images of the body's internal structures, which helps doctors diagnose various medical conditions. The main goals of a diagnostic MRI are:\n",
            "\n",
            "1. **Imaging**: Create detailed images of internal organs, tissues, and structures, such as the brain, spine, joints, and blood vessels.\n",
            "2. **Diagnosis**: Help doctors diagnose medical conditions, such as injuries, infections, tumors, or diseases, by analyzing the images.\n",
            "3. **Monitoring**: Track the progression of a condition over time to assess the effectiveness of treatment.\n",
            "\n",
            "**Surgical MRI (Surgical MRI suite):**\n",
            "\n",
            "A surgical MRI, also known as an intraoperative MRI, is a specialized MRI suite designed to support surgical procedures. The main goals of a surgical MRI are:\n",
            "\n",
            "1. **Intraoperative imaging**: Provide real-time images of the surgical site during a procedure, allowing surgeons to visualize the area and make adjustments as needed.\n",
            "2. **Navigation**: Guide surgical instruments and implants using the MRI images, ensuring accurate placement and minimizing the risk of complications.\n",
            "3. **Monitoring**: Continuously monitor the patient's condition during surgery, enabling the surgical team to respond to any changes or complications.\n",
            "\n",
            "**Key differences:**\n",
            "\n",
            "1. **Purpose**: Diagnostic MRI is used for diagnosis and monitoring, while surgical MRI is used to support surgical procedures.\n",
            "2. **Equipment**: Diagnostic MRI machines are designed for imaging, while surgical MRI machines are designed for real-time imaging and navigation.\n",
            "3. **Environment**: Diagnostic MRI is typically a separate room, while surgical MRI is integrated into the operating room, with the patient positioned on the MRI table within the surgical suite.\n",
            "4. **Imaging protocols**: Diagnostic MRI uses standard imaging protocols, while surgical MRI uses specialized protocols optimized for surgical procedures.\n",
            "5. **Surgical tools and instruments**: Surgical MRI requires specialized surgical instruments and implants designed for use within the MRI environment.\n",
            "\n",
            "**Comparison of diagnostic and surgical MRI:**\n",
            "\n",
            "|  | Diagnostic MRI | Surgical MRI |\n",
            "| --- | --- | --- |\n",
            "| **Purpose** | Diagnosis and monitoring | Intraoperative imaging and navigation |\n",
            "| **Equipment** | Standard imaging machine | Specialized intraoperative imaging machine |\n",
            "| **Environment** | Separate room | Integrated into the operating room |\n",
            "| **Imaging protocols** | Standard imaging protocols | Specialized protocols optimized for surgery |\n",
            "| **Surgical tools and instruments** | Not used | Specialized instruments and implants |\n",
            "\n",
            "Keep in mind that these are general differences, and specific machines and protocols may vary depending on the institution and the type of procedure being performed.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: I don't see a video frame. Can you provide the video frame or a description of it?\n",
            "ans: Model: Detection-Video\n",
            "Prompt: find all the instruments visible in this video frame?\n",
            "pred: To locate and segment the optic protuberances region in MRI, you would typically use specialized software such as 3D Slicer or ITK-SNAP. However, I'll provide a general outline of the steps involved:\n",
            "\n",
            "**Step 1: Load the MRI data**\n",
            "\n",
            "* Load the MRI dataset into the chosen software.\n",
            "* Ensure the image is in a suitable format (e.g., DICOM or NIfTI).\n",
            "\n",
            "**Step 2: Pre-processing**\n",
            "\n",
            "* Apply any necessary corrections to the image, such as:\n",
            "\t+ Motion correction (if the image was not acquired with sufficient motion correction)\n",
            "\t+ Gradient distortion correction (if the image was acquired with a non-uniform magnetic field)\n",
            "\t+ Spatial normalization (if the image needs to be aligned to a standard space)\n",
            "\n",
            "**Step 3: Segmentation**\n",
            "\n",
            "* Use a suitable segmentation technique, such as:\n",
            "\t+ Thresholding (e.g., Otsu thresholding)\n",
            "\t+ Region growing\n",
            "\t+ Level sets\n",
            "\t+ Active contour models\n",
            "* Use a suitable algorithm, such as:\n",
            "\t+ K-means clustering\n",
            "\t+ Expectation-maximization (EM) algorithm\n",
            "\t+ Graph cuts\n",
            "\n",
            "**Step 4: Manual editing**\n",
            "\n",
            "* Use the software's editing tools to refine the segmentation, if necessary:\n",
            "\t+ Remove any incorrect or incomplete segments\n",
            "\t+ Merge or split adjacent segments\n",
            "\n",
            "**Step 5: Optic protuberances segmentation**\n",
            "\n",
            "* Use a specific segmentation protocol or technique, such as:\n",
            "\t+ 3D Slicer's \"Segmentation\" module, with the \"Optic Protuberances\" protocol\n",
            "\t+ ITK-SNAP's \"Segmentation\" module, with the \"Optic Protuberances\" protocol\n",
            "* Use a specific algorithm, such as:\n",
            "\t+ Active contour models\n",
            "\t+ Level sets\n",
            "\t+ Graph cuts\n",
            "\n",
            "**Step 6: Post-processing**\n",
            "\n",
            "* Apply any necessary post-processing steps, such as:\n",
            "\t+ Smoothing\n",
            "\t+ Filtering\n",
            "\t+ Thresholding\n",
            "\n",
            "**Software options**\n",
            "\n",
            "* 3D Slicer (free, open-source)\n",
            "* ITK-SNAP (free, open-source)\n",
            "* FSL (free, open-source)\n",
            "* SPM (commercial)\n",
            "* ANTs (free, open-source)\n",
            "\n",
            "**Tips and considerations**\n",
            "\n",
            "* Use high-resolution MRI images (e.g., 1x1x1 mm) for accurate segmentation.\n",
            "* Use a suitable contrast agent, such as gadolinium, to enhance the contrast between the optic protuberances and surrounding tissue.\n",
            "* Use a suitable segmentation protocol and algorithm, tailored to the specific MRI data and the anatomy being segmented.\n",
            "* Use manual editing to refine the segmentation, if necessary.\n",
            "* Validate the segmentation results by comparing them to known anatomical landmarks or other imaging modalities (e.g., CT, ultrasound).\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment optic protuberances from MRI.\n",
            "pred: Unfortunately, I don't have the ability to see or access a specific video. Could you please provide more context or information about the video, such as the genre, the artist, or a brief description of the instruments you're interested in? That way, I can give you a more accurate answer.\n",
            "ans: Model: Detection-Video\n",
            "Prompt: can you identify the instruments present in the video?\n",
            "pred: To segment tumor margins from MRI images and overlay them on a video, you'll need to follow these steps:\n",
            "\n",
            "1. **MRI Image Segmentation**:\n",
            "   - Use a suitable MRI image segmentation technique such as thresholding, edge detection, or machine learning-based approaches (e.g., U-Net, V-Network) to segment the tumor from the surrounding tissue.\n",
            "   - Choose a suitable image segmentation library or tool, such as ITK-SNAP, 3D Slicer, or OpenCV with Deep Learning libraries like TensorFlow or PyTorch.\n",
            "\n",
            "2. **Video Generation**:\n",
            "   - Use a video generation library such as OpenCV, FFmpeg, or MoviePy to create a video from the MRI images.\n",
            "   - Ensure the video resolution matches the desired output resolution.\n",
            "\n",
            "3. **Overlaying Segmentation on Video**:\n",
            "   - Use a video annotation library or tool, such as OpenCV, to overlay the segmented tumor margins on the video frames.\n",
            "   - Ensure the overlayed image is properly aligned with the video frames.\n",
            "\n",
            "4. **Integration**:\n",
            "   - Combine the video generation and segmentation steps to create a single executable function or script.\n",
            "   - Use a suitable programming language such as Python, MATLAB, or C++ to integrate the steps.\n",
            "\n",
            "Some popular libraries and tools for the task are:\n",
            "\n",
            "- **ITK-SNAP**: A software for 3D image segmentation.\n",
            "- **3D Slicer**: An open-source software for 3D image segmentation and visualization.\n",
            "- **OpenCV**: A computer vision library for image and video processing.\n",
            "- **TensorFlow** and **PyTorch**: Deep learning libraries for image segmentation.\n",
            "- **FFmpeg**: A command-line video processing tool.\n",
            "- **MoviePy**: A Python library for video editing and processing.\n",
            "\n",
            "Example Python code using OpenCV and OpenCV's image segmentation functions:\n",
            "```python\n",
            "import cv2\n",
            "import numpy as np\n",
            "\n",
            "# Load MRI image\n",
            "img = cv2.imread('image.nii')\n",
            "\n",
            "# Perform image segmentation\n",
            "segmented_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
            "\n",
            "# Create video from MRI images\n",
            "video = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'XVID'), 30, (img.shape[1], img.shape[0]))\n",
            "\n",
            "# Overlay segmentation on video frames\n",
            "for i in range(len(video_frames)):\n",
            "    frame = video_frames[i]\n",
            "    segmented_frame = cv2.addWeighted(frame, 1, segmented_img, 0.5, 0)\n",
            "    video.write(segmented_frame)\n",
            "\n",
            "video.release()\n",
            "```\n",
            "This code snippet demonstrates the basic steps and libraries involved in segmenting tumor margins from MRI images and overlaying them on a video. However, the actual implementation will depend on the specific requirements and complexity of the task.\n",
            "ans: Model: Surgical-VQA|Overlaying\n",
            "Prompt: Segment tumor margins from MRI?|Overlay on video?\n",
            "pred: Overlaying preoperative imaging on the surgical video is a technique used in various medical fields, particularly in neurosurgery, orthopedic surgery, and cardiovascular surgery. This process, often referred to as 'image-guided surgery' or 'image-guided navigation,' utilizes advanced technology to superimpose preoperative imaging data, such as MRI, CT, or ultrasound images, onto real-time surgical video.\n",
            "\n",
            "This technique allows surgeons to better visualize the anatomy and its relationship to the surgical site, enhancing their ability to navigate through complex procedures. The primary purpose is to reduce the risk of complications, such as injury to surrounding tissues or organs, and to improve the accuracy of surgical interventions.\n",
            "\n",
            "The process involves several steps:\n",
            "\n",
            "1. **Preoperative imaging**: High-resolution images of the area are taken using imaging modalities such as MRI, CT, or ultrasound. These images are then digitized and processed to create a 3D model of the anatomy.\n",
            "\n",
            "2. **Registration**: The preoperative images are matched with the real-time video feed from the surgical site. This process is called registration. The registration process involves aligning the preoperative images with the live video feed to ensure accurate overlay.\n",
            "\n",
            "3. **Overlaying**: The preoperative images are then overlaid onto the live surgical video, creating a real-time, three-dimensional visualization of the anatomy.\n",
            "\n",
            "4. **Navigation**: The surgeon can then use this visual information to navigate through the procedure, making precise incisions, avoiding critical structures, and achieving optimal surgical outcomes.\n",
            "\n",
            "Several technologies support this process, including:\n",
            "\n",
            "- **Computer-assisted surgery systems**: These systems use a combination of preoperative imaging, registration, and real-time tracking to guide surgical instruments and provide a three-dimensional view of the anatomy.\n",
            "\n",
            "- **Augmented reality**: This technology overlays digital information, such as preoperative images, onto the real-time video feed, enhancing the surgeon's visualization and decision-making.\n",
            "\n",
            "- **Electromagnetic tracking**: This technology uses electromagnetic fields to track the position of surgical instruments and provide real-time feedback to the surgeon.\n",
            "\n",
            "The benefits of overlaying preoperative imaging on the surgical video include:\n",
            "\n",
            "- **Improved accuracy**: Enhanced visualization of the anatomy and its relationship to the surgical site.\n",
            "- **Reduced complications**: Minimized risk of injury to surrounding tissues or organs.\n",
            "- **Shortened surgical time**: Increased efficiency in complex procedures.\n",
            "- **Personalized medicine**: Tailored surgical approaches based on individual anatomy.\n",
            "\n",
            "However, this technology requires extensive training and expertise to ensure accurate registration and navigation.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay preoperative imaging with surgical scene.\n",
            "pred: Merging tumor segmentation images with surgical video in real-time requires advanced computer vision and machine learning techniques. Here's a high-level overview of a possible approach:\n",
            "\n",
            "1. **Tumor Segmentation:**\n",
            "\t* Use a deep learning-based method, such as U-Net or FCN (Fully Convolutional Network), to segment the tumor from the medical images (e.g., MRI or CT scans).\n",
            "\t* Train the model on a large dataset of labeled images to learn the tumor's appearance and contours.\n",
            "\t* Use transfer learning or fine-tuning to adapt the model to the specific imaging modality and tumor type.\n",
            "2. **Surgical Video Processing:**\n",
            "\t* Use a video processing pipeline to pre-process the surgical video, including:\n",
            "\t\t+ Noise reduction and denoising\n",
            "\t\t+ Image stabilization and registration\n",
            "\t\t+ Feature extraction (e.g., edges, corners, or optical flow)\n",
            "3. **Real-time Image Registration:**\n",
            "\t* Use a registration algorithm, such as SIFT (Scale-Invariant Feature Transform) or SURF (Speeded-Up Robust Features), to align the pre-processed surgical video frames with the pre-operative imaging (e.g., MRI or CT scans).\n",
            "\t* Use a robust and efficient registration algorithm, such as the ICP (Iterative Closest Point) algorithm, to align the images.\n",
            "4. **Merging Tumor Segmentation with Surgical Video:**\n",
            "\t* Use a 3D reconstruction algorithm, such as the Marching Cubes algorithm, to create a 3D model of the tumor from the pre-operative imaging.\n",
            "\t* Project the 3D model onto the pre-processed surgical video frames, creating a 2D projection of the tumor.\n",
            "\t* Merge the projected tumor segmentation with the pre-processed surgical video frames using a combination of image processing and computer vision techniques, such as:\n",
            "\t\t+ Image overlay\n",
            "\t\t+ Image registration\n",
            "\t\t+ Feature matching\n",
            "\n",
            "**Real-time Implementation:**\n",
            "\n",
            "To achieve real-time performance, you can use:\n",
            "\n",
            "1. **GPU acceleration**: Utilize the power of Graphics Processing Units (GPUs) to speed up the processing of images and computations.\n",
            "2. **Distributed computing**: Use multiple machines or nodes to process different parts of the pipeline in parallel, reducing the overall processing time.\n",
            "3. **Cloud computing**: Leverage cloud services, such as AWS or Google Cloud, to access scalable computing resources and reduce the processing time.\n",
            "\n",
            "**Example Code:**\n",
            "\n",
            "Here's a simplified example using Python and OpenCV for image processing and computer vision tasks:\n",
            "```python\n",
            "import cv2\n",
            "import numpy as np\n",
            "\n",
            "# Load pre-processed surgical video frames\n",
            "video_frames = cv2.VideoCapture('video.mp4')\n",
            "\n",
            "# Load pre-operative imaging (e.g., MRI or CT scans)\n",
            "image_data = np.load('image_data.npy')\n",
            "\n",
            "# Load tumor segmentation model\n",
            "model = U_Net()\n",
            "model.load_weights('model_weights.h5')\n",
            "\n",
            "# Pre-process surgical video frames\n",
            "frames_preprocessed = []\n",
            "for frame in video_frames:\n",
            "    # Apply noise reduction, denoising, and image stabilization\n",
            "    frame_preprocessed = cv2.pyrDown(frame)\n",
            "    frames_preprocessed.append(frame_preprocessed)\n",
            "\n",
            "# Register pre-processed frames with pre-operative imaging\n",
            "registered_frames = []\n",
            "for frame in frames_preprocessed:\n",
            "    # Use SIFT or SURF to align frames with pre-operative imaging\n",
            "    registered_frame = cv2.register_frame(frame, image_data)\n",
            "    registered_frames.append(registered_frame)\n",
            "\n",
            "# Merge tumor segmentation with surgical video\n",
            "merged_frames = []\n",
            "for frame in registered_frames:\n",
            "    # Project 3D tumor model onto pre-processed frame\n",
            "    tumor_projection = project_3D_model(frame)\n",
            "    # Merge tumor segmentation with frame\n",
            "    merged_frame = cv2.merge_tumor(frame, tumor_projection)\n",
            "    merged_frames.append(merged_frame)\n",
            "\n",
            "# Display or save merged frames\n",
            "cv2.imshow('Merged Frames', merged_frames)\n",
            "cv2.waitKey(0)\n",
            "cv2.destroyAllWindows()\n",
            "```\n",
            "Note: This is a simplified example and may require modifications to suit your specific requirements. Additionally, the code may not run as-is, as it's a simplified example and may require additional dependencies and configurations.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay tumor segmentation with surgical scene.\n",
            "pred: To display an overlay of anatomy segmentation on the surgical field, you would typically use a technology known as augmented reality (AR) or augmented virtual reality (AVR) in a surgical environment. This technology overlays a virtual, three-dimensional (3D) model of the patient's anatomy onto the actual surgical site.\n",
            "\n",
            "There are several ways to achieve this:\n",
            "\n",
            "1. **Head-Mounted Displays (HMDs)**: Wearable displays like Microsoft HoloLens or Magic Leap One can be used to project the virtual model onto the real-world environment, allowing the surgeon to see the anatomy in real-time.\n",
            "2. **Projection-based Systems**: Projection-based systems, such as those used in the da Vinci Surgical System, project the virtual model onto the surgical site, allowing the surgeon to visualize the anatomy in real-time.\n",
            "3. **Smart Glasses**: Smart glasses, such as Google Glass, can be used to display the virtual model, providing the surgeon with a see-through display of the anatomy.\n",
            "4. **Augmented Reality (AR) Software**: AR software, such as those developed by companies like 3D Systems or Zed, can be used to create a virtual model of the patient's anatomy and display it on a monitor or display.\n",
            "\n",
            "The anatomy segmentation is typically performed using imaging modalities such as:\n",
            "\n",
            "1. **Computed Tomography (CT)**: CT scans provide high-resolution images of the patient's anatomy, which can be used to create a 3D model.\n",
            "2. **Magnetic Resonance Imaging (MRI)**: MRI provides detailed images of the patient's anatomy, which can be used to create a 3D model.\n",
            "3. **Ultrasound**: Ultrasound imaging can be used to create a 3D model of the patient's anatomy.\n",
            "\n",
            "The 3D model is then superimposed onto the real-world environment using the technology mentioned above, allowing the surgeon to visualize the anatomy in real-time and make more accurate decisions during surgery.\n",
            "\n",
            "Some examples of anatomy segmentation and overlay technologies include:\n",
            "\n",
            "* **Stryker's Navio**: A surgical navigation system that uses AR to display a virtual model of the patient's anatomy.\n",
            "* **Medtronic's Mazor X**: A robotic-assisted spine surgery system that uses AR to display a virtual model of the patient's anatomy.\n",
            "* **Brainlab's Curve**: A surgical navigation system that uses AR to display a virtual model of the patient's anatomy.\n",
            "\n",
            "These technologies have the potential to improve surgical outcomes by providing surgeons with more accurate and detailed information about the patient's anatomy, allowing for more precise and minimally invasive procedures.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: Combining preoperative imaging and surgical visuals into an overlay can be a powerful tool for surgeons to enhance their understanding of the anatomy and plan their surgical approach more effectively. This is often achieved through the use of advanced visualization technologies, such as:\n",
            "\n",
            "1. **Augmented Reality (AR) and Virtual Reality (VR)**: These technologies use real-time imaging data from CT or MRI scans, along with the patient's anatomical data, to create a 3D model of the surgical site. This model can be superimposed over real-time video of the patient's anatomy, allowing surgeons to visualize the relationship between the anatomy and the preoperative imaging.\n",
            "2. **3D Printing**: Three-dimensional printing of patient-specific models can be used to create a physical model of the surgical site. This model can be used in conjunction with preoperative imaging to help surgeons plan their approach and identify potential challenges.\n",
            "3. **Surgical Navigation Systems**: These systems use real-time imaging data to create a virtual map of the patient's anatomy, which can be overlaid onto a surgical monitor. This allows surgeons to visualize the relationship between the anatomy and the preoperative imaging in real-time.\n",
            "4. **Computer-Assisted Surgery (CAS) Systems**: These systems use a combination of preoperative imaging and real-time imaging data to create a virtual model of the surgical site. This model can be used to guide the surgeon during the procedure, allowing for more precise and accurate surgery.\n",
            "\n",
            "To combine preoperative imaging and surgical visuals into an overlay, the following steps can be taken:\n",
            "\n",
            "1. **Image Acquisition**: Obtain preoperative imaging data, such as CT or MRI scans, of the patient.\n",
            "2. **Image Registration**: Register the preoperative imaging data with the patient's anatomical data, such as skin markers or fiducials.\n",
            "3. **3D Reconstruction**: Reconstruct the preoperative imaging data into a 3D model of the surgical site.\n",
            "4. **Surgical Planning**: Use the 3D model to plan the surgical approach and identify potential challenges.\n",
            "5. **Overlay Creation**: Create an overlay of the preoperative imaging and real-time imaging data, using technologies such as AR, VR, or surgical navigation systems.\n",
            "6. **Intraoperative Use**: Use the overlay to guide the surgeon during the procedure, allowing for more precise and accurate surgery.\n",
            "\n",
            "Some examples of systems that combine preoperative imaging and surgical visuals into an overlay include:\n",
            "\n",
            "* **BrainLab**: A computer-assisted surgery system that uses preoperative imaging data to create a virtual model of the surgical site.\n",
            "* **Stryker's Navio**: A surgical navigation system that uses real-time imaging data to create a virtual map of the patient's anatomy.\n",
            "* **Philips' Azurion**: A hybrid angiography system that uses preoperative imaging data to create a virtual model of the surgical site.\n",
            "\n",
            "These systems can provide surgeons with a more detailed understanding of the anatomy and help them plan their approach more effectively, ultimately leading to better patient outcomes.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay preoperative imaging with surgical scene.\n",
            "pred: I'm not sure which instrument quantities you are referring to, could you please provide more context or clarify which instrument quantities you are asking about?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: which instrument quantities is being used?\n",
            "pred: I can explain the general roles of the different phases in a process, but I need more information about the specific process you are referring to. However, I can provide a general overview of the typical roles of the phases in a process:\n",
            "\n",
            "1. **Preparation Phase**: This phase involves planning, preparation, and setup. It's where the groundwork is laid, and the necessary resources and materials are gathered.\n",
            "\n",
            "2. **Implementation Phase**: This phase involves executing the plan, putting the preparations into action. It's where the actual work is done, and the plan is carried out.\n",
            "\n",
            "3. **Evaluation Phase**: This phase involves reviewing the results, analyzing the outcomes, and assessing the success of the process. It's where lessons are learned, and improvements are identified.\n",
            "\n",
            "4. **Maintenance Phase**: This phase involves maintaining the results, ensuring the process is sustained, and making adjustments as needed. It's where the results are protected and improved upon.\n",
            "\n",
            "5. **Termination Phase**: This phase involves wrapping up the process, closing out any remaining tasks, and bringing the process to a conclusion. It's where the process is finalized, and any remaining resources are released.\n",
            "\n",
            "These phases are not mutually exclusive, and some processes may have additional phases or combine some of these phases. If you could provide more context or information about the specific process you're referring to, I can provide a more detailed explanation of its phases.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: explain the role of the phases in this step.\n",
            "pred: I can provide a general explanation of how to create an overlay of a diagnosis MRI and a surgical frame. However, please note that this process typically requires specialized software and expertise.\n",
            "\n",
            "1. **Software Requirements:** \n",
            "- DICOM viewer software (e.g., OsiriX, 3D Slicer, or ITK-SNAP)\n",
            "- Surgical planning software (e.g., BrainLab, StealthStation, or 3D Slicer)\n",
            "\n",
            "2. **Step-by-Step Process:**\n",
            "\n",
            "   - **Step 1:** Load the MRI image into the DICOM viewer software. This will allow you to visualize the patient's anatomy.\n",
            "   \n",
            "   - **Step 2:** Load the surgical frame into the surgical planning software. This will allow you to create a 3D model of the surgical instrument.\n",
            "   \n",
            "   - **Step 3:** In the surgical planning software, create a new 3D model of the patient's anatomy by importing the MRI image. This will allow you to align the MRI image with the surgical instrument.\n",
            "   \n",
            "   - **Step 4:** Use the software's alignment tools to align the MRI image with the surgical instrument. This can be done by matching specific landmarks or by using registration algorithms.\n",
            "   \n",
            "   - **Step 5:** Once the MRI image and the surgical instrument are aligned, you can create an overlay of the two images. This will allow you to visualize how the surgical instrument will interact with the patient's anatomy.\n",
            "   \n",
            "   - **Step 6:** Adjust the overlay as needed to ensure accurate placement of the surgical instrument.\n",
            "\n",
            "3. **Example Tools:**\n",
            "\n",
            "   - **OsiriX:** A free, open-source DICOM viewer software that allows you to load and visualize MRI images.\n",
            "   \n",
            "   - **3D Slicer:** A free, open-source software that allows you to create 3D models of the patient's anatomy and align them with surgical instruments.\n",
            "   \n",
            "   - **StealthStation:** A commercial surgical planning software that allows you to create 3D models of the patient's anatomy and align them with surgical instruments.\n",
            "\n",
            "4. **Important Considerations:**\n",
            "\n",
            "   - **Accuracy:** The accuracy of the overlay depends on the quality of the MRI image and the alignment process.\n",
            "   \n",
            "   - **Registration:** The registration process can be time-consuming and requires expertise.\n",
            "   \n",
            "   - **Validation:** The accuracy of the overlay should be validated by a qualified professional to ensure safe and effective surgery.\n",
            "\n",
            "Please note that this is a general explanation, and the specific steps may vary depending on the software and the surgical procedure being performed.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay diagnosis MRI with surgical scene.\n",
            "pred: I'm happy to help you analyze and segment the left optic protuberance from a video. However, I need more information about the video and its context to provide a detailed answer.\n",
            "\n",
            "To better assist you, could you please provide more details about the video, such as:\n",
            "\n",
            "1. What type of video is it? (e.g., medical imaging, 3D animation, or a real-world video)\n",
            "2. What is the purpose of the video? (e.g., educational, research, or entertainment)\n",
            "3. What is the resolution and frame rate of the video?\n",
            "4. Are there any specific features or landmarks that need to be segmented from the left optic protuberance?\n",
            "\n",
            "Assuming the video is a medical imaging video, here's a general approach to analyze and segment the left optic protuberance:\n",
            "\n",
            "**Step 1: Pre-processing**\n",
            "\n",
            "* Convert the video into a 3D volumetric data (e.g., MRI or CT scan)\n",
            "* Apply a thresholding technique to enhance the contrast between the left optic protuberance and the surrounding tissue\n",
            "* Remove noise and artifacts using filters (e.g., Gaussian blur, anisotropic diffusion)\n",
            "\n",
            "**Step 2: Segmentation**\n",
            "\n",
            "* Use a segmentation algorithm (e.g., thresholding, edge detection, or machine learning-based methods) to identify the left optic protuberance\n",
            "* Apply morphological operations (e.g., dilation, erosion, and closing) to refine the segmentation result\n",
            "* Use a region-growing algorithm to grow the segmentation mask from the initial seed points\n",
            "\n",
            "**Step 3: Post-processing**\n",
            "\n",
            "* Refine the segmentation result using a level-set method or a 3D active contour model\n",
            "* Remove any remaining noise or artifacts\n",
            "* Visualize the segmented left optic protuberance in 3D space\n",
            "\n",
            "Please provide more information about the video, and I'll be happy to provide a more detailed and tailored approach to analyze and segment the left optic protuberance.\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Analyze left optic protuberance?\n",
            "pred: I'm happy to help, but I need more information. You're asking about surgical instruments that can be detected in a video, but you haven't specified the type of video or what context it is in. Could you provide more details or clarify your question?\n",
            "ans: Model: Detection-Video\n",
            "Prompt: what surgical instruments can be detected in the video?\n",
            "pred: I'm not sure which specific instrument positions you are referring to. Could you please provide more context or clarify which instrument positions you are asking about?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: which instrument positions is being used?\n",
            "pred: To overlay diagnosis MRI images and highlight anatomical structures, you can use various software and techniques. Here are a few options:\n",
            "\n",
            "1. **OsiriX**: A free, open-source DICOM viewer that allows you to overlay MRI images and highlight anatomical structures. It supports a wide range of image formats, including MRI, CT, and PET.\n",
            "2. **3D Slicer**: An open-source software that allows you to visualize and analyze medical images, including MRI. It provides tools for segmenting and highlighting anatomical structures.\n",
            "3. **ITK-SNAP**: A free, open-source software that allows you to segment and highlight anatomical structures in MRI images. It provides a range of tools for image processing and analysis.\n",
            "4. **Fiji**: A Java-based image processing package that includes tools for segmenting and highlighting anatomical structures in MRI images.\n",
            "5. **MITK**: A software platform for image analysis and visualization, including MRI. It provides tools for segmenting and highlighting anatomical structures.\n",
            "\n",
            "Some specific techniques for highlighting anatomical structures include:\n",
            "\n",
            "1. **Segmentation**: Using algorithms to identify and separate different tissues or structures in the MRI image.\n",
            "2. **Thresholding**: Applying a threshold value to highlight specific structures based on their intensity or contrast.\n",
            "3. **Region growing**: Identifying and highlighting structures based on their texture or shape characteristics.\n",
            "4. **Surface rendering**: Creating a 3D surface representation of the anatomical structure to highlight its shape and contours.\n",
            "\n",
            "To perform these techniques, you'll need to have a basic understanding of image processing and analysis concepts, as well as the specific software you're using.\n",
            "\n",
            "In addition to these software options, some commercial products that can overlay diagnosis MRI images and highlight anatomical structures include:\n",
            "\n",
            "1. **GE Healthcare's Advantage Workstation**: A commercial software platform for image analysis and visualization, including MRI.\n",
            "2. **Philips Healthcare's IntelliSpace**: A commercial software platform for image analysis and visualization, including MRI.\n",
            "3. **Siemens Healthineers' syngo.via**: A commercial software platform for image analysis and visualization, including MRI.\n",
            "\n",
            "These commercial products often come with advanced features and tools for highlighting anatomical structures, but may require a subscription or purchase.\n",
            "\n",
            "Keep in mind that the specific software and techniques used will depend on the type of MRI image, the anatomical structure being highlighted, and the desired level of detail.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay diagnosis MRI with surgical scene.\n",
            "pred: Unfortunately, I can't see the video you're referring to. Can you provide more context or describe the video to me?\n",
            "ans: Model: Detection-Video\n",
            "Prompt: what surgical instruments can be detected in the video?\n",
            "pred: I don't have enough information to provide an accurate answer about the current frame of a video. Can you please provide more context or clarify what you mean by \"Segment Right Optic Protuberance in the current frame of Video\"?\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Identify right optic protuberance?\n",
            "pred: Integrating anatomy segmentation with the current surgical view involves combining detailed anatomical information with real-time imaging data from the surgical site. This can be achieved through various techniques, including:\n",
            "\n",
            "1. **3D Reconstruction**: Create a 3D model of the patient's anatomy from pre-operative imaging data, such as CT or MRI scans. This model can then be superimposed onto the live surgical view, providing a more accurate and detailed understanding of the underlying anatomy.\n",
            "2. **Augmented Reality (AR) Visualization**: Use AR technology to superimpose the 3D anatomical model onto the live surgical view, allowing surgeons to visualize the anatomy in real-time. This can be achieved using specialized glasses or a headset with AR capabilities.\n",
            "3. **Electromagnetic Tracking**: Use electromagnetic tracking systems to track the position and orientation of surgical instruments and the camera. This allows for real-time registration of the anatomy segmentation model with the surgical site, ensuring accurate alignment and visualization.\n",
            "4. **Machine Learning-based Segmentation**: Utilize machine learning algorithms to segment the anatomy from the pre-operative imaging data and then register it with the live surgical view. This can be achieved through techniques such as deep learning-based segmentation and registration.\n",
            "5. **Real-time Image Segmentation**: Perform real-time image segmentation using techniques such as U-Net architectures or other deep learning-based methods to segment the anatomy from the live surgical images.\n",
            "\n",
            "Technologies used for integration:\n",
            "\n",
            "1. **Optical Tracking Systems**: Utilize optical tracking systems, such as infrared or stereo cameras, to track the position and orientation of surgical instruments and the camera.\n",
            "2. **Electromagnetic Tracking Systems**: Employ electromagnetic tracking systems, such as Polaris or Aurora, to track the position and orientation of surgical instruments and the camera.\n",
            "3. **Computer Vision**: Apply computer vision techniques, such as object detection and segmentation, to the live surgical images to enhance the visualization of the anatomy.\n",
            "4. **Machine Learning**: Leverage machine learning algorithms, such as convolutional neural networks (CNNs), to analyze the live surgical images and segment the anatomy in real-time.\n",
            "5. **3D Reconstruction Software**: Utilize software, such as 3D Slicer or OsiriX, to create 3D models of the anatomy from pre-operative imaging data.\n",
            "\n",
            "Benefits:\n",
            "\n",
            "1. **Improved Accuracy**: Integrating anatomy segmentation with the current surgical view can improve the accuracy of surgical procedures by providing a more detailed understanding of the underlying anatomy.\n",
            "2. **Enhanced Visualization**: Real-time visualization of the anatomy can enhance the surgeon's understanding of the surgical site, allowing for more precise dissection and better decision-making.\n",
            "3. **Reduced Complications**: By providing a more accurate and detailed understanding of the anatomy, the risk of complications can be reduced, and patient outcomes can be improved.\n",
            "4. **Increased Efficiency**: Integrating anatomy segmentation with the current surgical view can streamline surgical procedures, reducing the time spent on dissection and improving overall efficiency.\n",
            "\n",
            "Challenges:\n",
            "\n",
            "1. **Accuracy**: Ensuring accurate registration of the anatomy segmentation model with the live surgical view can be challenging, requiring precise tracking and registration techniques.\n",
            "2. **Real-time Processing**: Real-time processing of the anatomy segmentation can be computationally intensive, requiring high-performance computing power and efficient algorithms.\n",
            "3. **Clinical Validation**: Clinical validation of the integrated system is essential to ensure its effectiveness and safety in real-world surgical scenarios.\n",
            "4. **User Training**: Surgeons require specialized training to effectively use the integrated system, ensuring they understand how to interpret the anatomy segmentation and integrate it into their surgical workflow.\n",
            "\n",
            "Examples of successful integration:\n",
            "\n",
            "1. **Neurosurgery**: Integration of anatomy segmentation with the current surgical view has been successfully applied in neurosurgery, allowing for more precise dissection and better decision-making during complex procedures.\n",
            "2. **Orthopedic Surgery**: Integration of anatomy segmentation with the current surgical view has been used in orthopedic surgery to enhance the accuracy of joint replacement procedures and reduce complications.\n",
            "3. **Cardiac Surgery**: Integration of anatomy segmentation with the current surgical view has been applied in cardiac surgery to improve the accuracy of cardiac procedures and reduce the risk of complications.\n",
            "\n",
            "In conclusion, integrating anatomy segmentation with the current surgical view can significantly enhance surgical accuracy, efficiency, and patient outcomes. However, challenges related to accuracy, real-time processing, clinical validation, and user training must be addressed to ensure the successful implementation of this technology in clinical practice.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: I'm not sure what you are referring to by \"Video\". However, I'll provide a general explanation of the carotid arteries and their segments.\n",
            "\n",
            "The carotid arteries are major blood vessels that supply oxygenated blood to the brain. There are two carotid arteries: the right carotid artery and the left carotid artery. Each artery has three main segments:\n",
            "\n",
            "1. **External carotid artery**: This is the terminal segment of the carotid arteries, which branches off from the common carotid artery. It is responsible for supplying blood to the face, neck, and scalp.\n",
            "\n",
            "2. **Internal carotid artery**: This is the main segment of the carotid arteries, which supplies blood to the brain. It arises from the common carotid artery and passes through the neck, entering the cranial cavity.\n",
            "\n",
            "   a. **Cervical segment**: This is the initial segment of the internal carotid artery, extending from the aortic arch to the base of the skull.\n",
            "   \n",
            "   b. **Petrous segment**: This is the middle segment of the internal carotid artery, passing through the carotid canal in the petrous part of the temporal bone.\n",
            "   \n",
            "   c. **Cavernous segment**: This is the terminal segment of the internal carotid artery, passing through the cavernous sinus.\n",
            "   \n",
            "   d. **Supraclinoid segment**: This is the final segment of the internal carotid artery, passing through the cranial cavity and eventually branching into the anterior cerebral artery and the posterior cerebral artery.\n",
            "\n",
            "Please note that this is a general explanation, and the specific segments might vary based on the context or the source. If you could provide more information about the \"Video\" you are referring to, I'd be happy to help you better.\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Analyze left carotid?\n",
            "pred: I can provide a general description of how this process works, but I'm a large language model, I don't have the capability to display images. However, I can explain the concept of overlaying preoperative imaging and the surgical frame.\n",
            "\n",
            "In medical imaging, overlaying preoperative imaging (such as CT or MRI scans) with the surgical frame is a technique used to guide surgical procedures. Here's a general overview of the process:\n",
            "\n",
            "1. **Image Acquisition**: The patient undergoes imaging tests, such as a CT or MRI, to create detailed images of the area to be operated on.\n",
            "2. **Image Analysis**: The images are analyzed to identify the location and extent of the target area, such as a tumor, bone fracture, or other anatomical features.\n",
            "3. **Surgical Frame Creation**: A surgical frame is created, which is a 3D model of the patient's anatomy, based on the preoperative imaging data. This frame provides a visual representation of the patient's anatomy, allowing surgeons to plan and visualize the surgical approach.\n",
            "4. **Image Overlay**: The preoperative imaging is overlaid onto the surgical frame, allowing surgeons to see the relationship between the imaging data and the physical anatomy. This overlay helps surgeons to:\n",
            "\t* Identify the exact location of the target area\n",
            "\t* Plan the optimal surgical approach\n",
            "\t* Visualize the potential risks and complications\n",
            "\t* Make adjustments to the surgical plan as needed\n",
            "5. **Surgical Navigation**: During the surgery, the preoperative imaging and the surgical frame are used to guide the surgical instruments and tools. The overlay provides real-time feedback, allowing surgeons to stay on track and make adjustments as needed.\n",
            "\n",
            "There are various technologies used to achieve this overlay, such as:\n",
            "\n",
            "* **Registration**: This involves matching the preoperative imaging data to the patient's physical anatomy, using techniques such as fiducial markers or surface registration.\n",
            "* **3D printing**: The surgical frame can be created using 3D printing technology, allowing for precise and accurate reproduction of the patient's anatomy.\n",
            "* **Computer-aided design (CAD)**: Software is used to create a digital model of the patient's anatomy, which can be overlaid onto the preoperative imaging data.\n",
            "\n",
            "Keep in mind that the specific techniques and technologies used may vary depending on the type of surgery, the imaging modality, and the surgeon's preference.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay preoperative imaging with surgical scene.\n",
            "pred: Unfortunately, you haven't provided enough context or information about the steps you are referring to. Could you please provide more details or clarify what steps you are asking about? Is it a process, a system, a procedure, or something else? The more information you provide, the better I can assist you.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: describe the steps being used.\n",
            "pred: Based on a typical surgical scene, I can identify the instruments that are commonly found:\n",
            "\n",
            "1. **Surgical Scalpels**: Used for making precise cuts and incisions in the skin and tissues.\n",
            "2. **Forceps**: Used for grasping, holding, and manipulating tissues, organs, or other objects within the body.\n",
            "3. **Retractors**: Used to hold back tissues or organs to expose the area of interest, allowing the surgeon to see the surgical site more clearly.\n",
            "4. **Suction Tube**: Used to remove fluids, blood, or other debris from the surgical site.\n",
            "5. **Surgical Needles**: Used for suturing or stitching tissues together.\n",
            "6. **Suture Material**: Used to close wounds or incisions.\n",
            "7. **Irrigation Cannula**: Used to deliver fluids, such as saline solution or antibiotics, to the surgical site.\n",
            "8. **Surgical Hooks**: Used to grasp and manipulate tissues, especially in delicate areas.\n",
            "9. **Electrocautery**: Used to cauterize or coagulate bleeding tissues.\n",
            "10. **Scissors**: Used to cut sutures, ligatures, or other materials.\n",
            "11. **Gauze**: Used to dress wounds, absorb fluids, or pack the surgical site.\n",
            "12. **Surgical Tape**: Used to hold dressings or bandages in place.\n",
            "13. **Gloves**: Used by the surgeon and surgical team to maintain a sterile environment.\n",
            "14. **Surgical Lights**: Used to illuminate the surgical site, providing clear visibility for the surgical team.\n",
            "15. **Anesthesia Equipment**: Used to administer anesthesia to the patient, if required.\n",
            "16. **Surgical Drapes**: Used to create a sterile environment and prevent contamination.\n",
            "17. **Instrument Trays**: Used to organize and store surgical instruments, keeping them clean and within reach.\n",
            "18. **Suction Pump**: Used to create suction for the suction tube.\n",
            "19. **Electrocautery Cord**: Used to power the electrocautery device.\n",
            "20. **Surgical Marking Pencils**: Used to mark the surgical site with temporary markers.\n",
            "\n",
            "These are the common instruments found in a surgical scene. The specific instruments used may vary depending on the type of surgery being performed.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: locate the instruments in the surgical scene.\n",
            "pred: Segmentation of the clival recess using MRI involves identifying and outlining the boundaries of the clival recess, which is a critical structure in the cranial cavity. Here's a step-by-step guide on how to perform this segmentation:\n",
            "\n",
            "**Imaging Requirements:**\n",
            "\n",
            "* High-resolution MRI images of the brain, preferably with a slice thickness of 1-2 mm, and a matrix size of 256 x 256 pixels or higher.\n",
            "* Sequences: T1-weighted, T2-weighted, and possibly gadolinium-enhanced T1-weighted images.\n",
            "\n",
            "**Segmentation Steps:**\n",
            "\n",
            "1. **Import and Preparation:**\n",
            "\t* Load the MRI images into a DICOM viewer or a segmentation software (e.g., 3D Slicer, ITK-SNAP).\n",
            "\t* Ensure the images are properly aligned and oriented.\n",
            "2. **Image Enhancement:**\n",
            "\t* Apply a bone suppression filter to reduce bone artifacts and enhance soft tissue contrast.\n",
            "\t* Use a contrast-enhanced T1-weighted image to improve the visibility of the clival recess.\n",
            "3. **Manual Segmentation:**\n",
            "\t* Use a region-growing or edge-detection algorithm to outline the clival recess.\n",
            "\t* Start by outlining the clival recess in a sagittal plane, using the midline of the brain as a reference.\n",
            "\t* Gradually extend the segmentation to the surrounding structures, such as the petrous bone, the foramen magnum, and the clivus.\n",
            "\t* Use a 3D surface rendering or a 2D slice-by-slice approach to ensure accurate segmentation.\n",
            "4. **Refining the Segmentation:**\n",
            "\t* Refine the segmentation by adjusting the contours to match the anatomical boundaries.\n",
            "\t* Use a 3D editing tool to correct any errors or inconsistencies.\n",
            "5. **Validation:**\n",
            "\t* Compare the segmented clival recess with anatomical references, such as the petrous bone, the foramen magnum, and the clivus.\n",
            "\t* Verify the accuracy of the segmentation by checking for any errors or inconsistencies.\n",
            "\n",
            "**Software Tools:**\n",
            "\n",
            "* 3D Slicer (open-source software)\n",
            "* ITK-SNAP (open-source software)\n",
            "* Brain Extraction Tool (BET) for bone suppression\n",
            "\n",
            "**Tips and Considerations:**\n",
            "\n",
            "* Use a high-resolution MRI with a small matrix size to ensure accurate segmentation.\n",
            "* Apply a bone suppression filter to reduce bone artifacts and enhance soft tissue contrast.\n",
            "* Use a contrast-enhanced T1-weighted image to improve the visibility of the clival recess.\n",
            "* Refine the segmentation by adjusting the contours to match the anatomical boundaries.\n",
            "* Validate the segmentation by comparing it with anatomical references.\n",
            "\n",
            "**Common Challenges:**\n",
            "\n",
            "* Bone artifacts from the petrous bone or the clivus can obscure the clival recess.\n",
            "* Inconsistent segmentation can occur if the software tools are not used correctly.\n",
            "* Refining the segmentation can be challenging, especially in complex cases.\n",
            "\n",
            "**Common Applications:**\n",
            "\n",
            "* Surgical planning and simulation\n",
            "* Radiation therapy planning\n",
            "* Neurosurgical procedures\n",
            "* Cranial trauma or injury diagnosis\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment clival recess from MRI.\n",
            "pred: Overlaying anatomical segmentation on a surgical video involves combining the 3D anatomical models with the live video feed from the surgical procedure. This can be achieved through various techniques, including:\n",
            "\n",
            "1. **Augmented Reality (AR) integration**: Using AR technology to superimpose the 3D anatomical models onto the live video feed, allowing surgeons to visualize the anatomy in real-time during the procedure.\n",
            "2. **3D modeling and rendering**: Creating detailed 3D models of the anatomy and rendering them in real-time, allowing for a more immersive and interactive experience.\n",
            "3. **Image-guided surgery systems**: Using pre-operative imaging data to create a 3D model of the anatomy, which can be overlaid onto the live video feed during the procedure.\n",
            "4. **Computer-assisted surgery systems**: Using computer algorithms to enhance the video feed with 3D anatomical models, allowing for better visualization and more precise surgical planning.\n",
            "\n",
            "To achieve this, you would need:\n",
            "\n",
            "1. **High-quality video capture**: A high-definition camera system to capture the surgical video feed.\n",
            "2. **3D modeling software**: Software capable of creating detailed 3D models of the anatomy, such as CT or MRI scans.\n",
            "3. **Rendering engines**: Software or hardware capable of rendering the 3D models in real-time, allowing for a smooth and interactive experience.\n",
            "4. **Tracking systems**: Systems that track the movement of the camera and the surgical instruments, allowing for accurate registration of the 3D model with the live video feed.\n",
            "5. **Display systems**: High-resolution displays that can show the combined video feed and 3D model, providing an immersive experience for the surgeons.\n",
            "\n",
            "Some examples of systems that can achieve this include:\n",
            "\n",
            "1. **Brainlab's NeuroNavi**: A navigation system that overlays pre-operative imaging data onto the live video feed during neurosurgery.\n",
            "2. **Medtronic's Mazor X**: A robotic-assisted spine surgery system that uses 3D modeling and real-time tracking to enhance the surgical experience.\n",
            "3. **Stryker's Navio**: A navigation system that overlays pre-operative imaging data onto the live video feed during orthopedic and spine surgery.\n",
            "\n",
            "Please note that these systems are highly specialized and require significant expertise, training, and infrastructure to implement.\n",
            "ans: Model: Overlaying\n",
            "Prompt: Overlay anatomy segmentation with surgical scene.\n",
            "pred: You're looking to integrate medical imaging data with real-time surgical video to enhance surgical accuracy and precision. This is known as augmented reality (AR) or augmented reality surgical navigation.\n",
            "\n",
            "There are several ways to achieve this:\n",
            "\n",
            "1. **Registration**: Align the MRI data with the surgical video using registration techniques, such as:\n",
            "\t* Surface-based registration: Aligns the MRI data with the patient's surface anatomy.\n",
            "\t* Volumetric registration: Aligns the MRI data with the patient's internal anatomy.\n",
            "2. **Overlaying MRI data**: Use AR software to overlay the MRI data onto the surgical video in real-time. This can be achieved through:\n",
            "\t* **3D visualization tools**: Utilize 3D visualization software, such as 3D Slicer, to render the MRI data in a 3D format and overlay it onto the surgical video.\n",
            "\t* **AR glasses or head-mounted displays**: Wear AR glasses or a head-mounted display to visualize the MRI data in real-time, overlaid onto the surgical video.\n",
            "3. **Tool tracking**: Track the surgical tools using:\n",
            "\t* **Optical tracking systems**: Utilize optical tracking systems, such as the Polaris Spectra or the 3D Stryker, to track the surgical tools and display their positions in real-time.\n",
            "\t* **Electromagnetic tracking**: Use electromagnetic tracking systems, such as the Aurora or the EndoControl, to track the surgical tools and display their positions in real-time.\n",
            "\n",
            "Some popular systems for overlaying diagnostic MRI data onto surgical video include:\n",
            "\n",
            "1. **Brainlab's iPlan**: An integrated platform for surgical navigation, which allows for real-time registration and overlaying of MRI data onto the surgical video.\n",
            "2. **Medtronic's StealthStation**: A surgical navigation system that integrates MRI data with real-time tracking of surgical tools.\n",
            "3. **Philips' Azurion**: A cardiovascular imaging and navigation system that allows for real-time registration and overlaying of MRI data onto the surgical video.\n",
            "\n",
            "To detect tool positions, you can use:\n",
            "\n",
            "1. **Computer vision algorithms**: Implement computer vision algorithms, such as machine learning-based approaches, to detect the surgical tools and their positions in real-time.\n",
            "2. **Machine learning-based tool tracking**: Utilize machine learning-based approaches, such as convolutional neural networks (CNNs), to track the surgical tools and display their positions in real-time.\n",
            "\n",
            "Please note that the specific system or approach you choose will depend on your specific requirements, the type of surgery, and the available resources. It's essential to consult with medical professionals and experts in the field to determine the most suitable solution for your needs.\n",
            "ans: Model: Surgical-VQA|Overlaying\n",
            "Prompt: Overlay diagnostic MRI on surgical video?|Detect tool positions?\n",
            "pred: I'm assuming you are referring to the Clival recess and its relation to a video or a visual medium. \n",
            "\n",
            "The Clival recess is a part of the human anatomy, specifically a bony structure in the cranium (skull) that houses the clivus, a long, thin, and bony ridge that runs along the midline of the posterior cranial fossa. It is essentially the posterior wall of the cranial cavity.\n",
            "\n",
            "To \"segment\" this from a video would likely mean breaking down or dividing a video into smaller parts or segments, possibly for analysis, editing, or sharing.\n",
            "\n",
            "However, if you are referring to a different context, such as segmenting an image of the Clival recess from a video, I'd be happy to try and assist further. Please provide more context or clarify your question.\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Precise segmentation of clival recess?\n",
            "pred: I don't see any video frame. Can you please provide the video frame or a description of it, so I can better understand what you are referring to?\n",
            "ans: Model: Detection-Video\n",
            "Prompt: find all the instruments visible in this video frame?\n",
            "pred: I'm not sure what you're referring to, but I'll take a guess. It seems like you're asking about a surgical scenario where a surgeon overlays preoperative imaging, such as an MRI or CT scan, with the actual surgical site to identify the location of instruments and the anatomy.\n",
            "\n",
            "In this scenario, the surgeon would typically use a technology called \"augmented reality\" or \"mixed reality\" to overlay the preoperative imaging with the real-time video feed from the surgical site. This technology is often referred to as \"electromagnetic navigation\" or \"electromagnetic tracking.\"\n",
            "\n",
            "The electromagnetic tracking system consists of:\n",
            "\n",
            "1. A preoperative imaging modality (e.g., MRI or CT scan) that provides a detailed 3D image of the patient's anatomy.\n",
            "2. A surgical navigation system that uses electromagnetic sensors to track the location of the instruments and the patient's anatomy in real-time.\n",
            "3. A display system that overlays the preoperative imaging with the real-time video feed from the surgical site, allowing the surgeon to visualize the anatomy and instruments in 3D.\n",
            "\n",
            "The electromagnetic tracking system can be used to:\n",
            "\n",
            "* Identify the location of instruments and the anatomy in real-time\n",
            "* Enhance surgical accuracy and precision\n",
            "* Reduce the risk of complications and errors\n",
            "* Improve surgical outcomes\n",
            "\n",
            "Some examples of electromagnetic tracking systems include:\n",
            "\n",
            "* Medtronic's StealthStation\n",
            "* Stryker's SRS\n",
            "* Brainlab's VectorVision\n",
            "\n",
            "These systems are commonly used in neurosurgery, orthopedic surgery, and other specialties where precision and accuracy are critical.\n",
            "\n",
            "Is this what you were thinking of?\n",
            "ans: Model: Detection|Overlaying\n",
            "Prompt: Overlay preoperative imaging with the surgical scene?|Identify instruments?\n",
            "pred: I don't have the information to precisely segment the Left Optic Protuberance from a video.\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Detect left optic protuberance?\n",
            "pred: I'm happy to help, but I need more information. What video are you referring to? Can you provide more context or details about the video you're asking about?\n",
            "ans: Model: Detection-Video\n",
            "Prompt: what surgical instruments can be detected in the video?\n",
            "pred: To accurately identify and segment anatomy and tumors from MRI (Magnetic Resonance Imaging) scans, here are some general steps and techniques used:\n",
            "\n",
            "1. **Image Acquisition**: The MRI machine captures images of the body part in question, such as the brain, spine, or abdomen. The images are typically acquired in multiple planes, including sagittal, coronal, and axial planes.\n",
            "\n",
            "2. **Image Reconstruction**: The raw data is reconstructed into a series of images, which are then analyzed to identify different anatomical structures, such as organs, bones, and soft tissues.\n",
            "\n",
            "3. **Image Enhancement**: Techniques like contrast enhancement, filtering, and noise reduction are applied to the images to improve the visibility of certain structures and to suppress noise.\n",
            "\n",
            "4. **Segmentation**: This is the process of dividing the image into its constituent parts, such as organs, tumors, and other anatomical structures. This is typically done using specialized software, such as:\n",
            "   * **Thresholding**: involves setting a threshold value to separate the object of interest from the background.\n",
            "   * **Edge detection**: uses algorithms to identify the boundaries between different structures.\n",
            "   * **Region growing**: involves growing a region of interest from a seed point, such as a tumor.\n",
            "\n",
            "5. **Registration**: The segmented images are registered to a standard coordinate system, allowing for accurate measurement and comparison of different structures.\n",
            "\n",
            "6. **Segmentation Techniques**: Some common segmentation techniques include:\n",
            "   * **K-Means Clustering**: a clustering algorithm that groups similar pixels together.\n",
            "   * **Active Contour Model**: uses a deformable contour to segment the object of interest.\n",
            "   * **Level Set Method**: uses a level set function to evolve the contour of the object of interest.\n",
            "\n",
            "7. **Post-processing**: The segmented images are refined and smoothed using techniques like filtering, smoothing, and thresholding.\n",
            "\n",
            "8. **Quantification**: The segmented images are used to quantify the volume, surface area, and other metrics of the structures of interest.\n",
            "\n",
            "Some popular software used for MRI segmentation include:\n",
            "* ITK-SNAP (Insight Segmentation and Registration Toolkit - Simple Neuron Animator and Parser)\n",
            "* 3D Slicer\n",
            "* ITK-SNAP\n",
            "* FSL (FMRIB Software Library)\n",
            "* ITK (Insight Segmentation and Registration Toolkit)\n",
            "\n",
            "Some popular algorithms used for MRI segmentation include:\n",
            "* Convolutional Neural Networks (CNNs)\n",
            "* U-Net\n",
            "* 3D U-Net\n",
            "* Patch-based segmentation\n",
            "* Graph-based segmentation\n",
            "\n",
            "Keep in mind that the specific techniques and software used may vary depending on the application, the type of MRI scan, and the specific anatomy being imaged.\n",
            "ans: Model: Segment-MRI\n",
            "Prompt: Segment clival recess from MRI.\n",
            "pred: I'm happy to help, but I need a bit more context. Could you please provide more information about the steps you're referring to? Are you talking about a specific set of steps, like a recipe, a process, or a procedure? The more context you provide, the better I can assist you in making an observation about the steps.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: what observation can be made about the steps?\n",
            "pred: To locate and segment the sella from a video, you would need to use computer vision techniques, specifically image segmentation and object detection methods. The sella, also known as the sellar region, is a small, bony structure at the base of the brain, and identifying it from a video would require image analysis.\n",
            "\n",
            "Here's a general approach to locate and segment the sella from a video:\n",
            "\n",
            "1. **Image Preprocessing**: Convert the video frames into images and apply image preprocessing techniques such as resizing, noise reduction, and contrast adjustment to enhance the quality of the images.\n",
            "2. **Image Segmentation**: Use image segmentation algorithms to separate the sella region from the rest of the image. Some common segmentation algorithms include:\n",
            "\t* Thresholding: This involves separating the object from the background based on intensity values.\n",
            "\t* Edge detection: This involves identifying the edges of the object and then filling in the region.\n",
            "\t* Region growing: This involves growing a region from a seed point and then stopping when the desired region is reached.\n",
            "\t* Deep learning-based methods: These include Convolutional Neural Networks (CNNs) and U-Net architectures, which can learn to segment the object from the background.\n",
            "3. **Object Detection**: Once the sella region is segmented, use object detection algorithms to locate the sella in the image. Some common object detection algorithms include:\n",
            "\t* YOLO (You Only Look Once)\n",
            "\t* SSD (Single Shot Detector)\n",
            "\t* Faster R-CNN (Region-based Convolutional Neural Networks)\n",
            "\t* Mask R-CNN (Region-based Convolutional Neural Networks with Masking)\n",
            "4. **Tracking**: To track the sella in the video, you can use tracking algorithms such as the Kanade-Lucas-Tomasi (KLT) feature tracker or the Lucas-Kanade tracker.\n",
            "\n",
            "Some popular libraries and tools for image and video analysis include:\n",
            "\n",
            "* OpenCV\n",
            "* TensorFlow\n",
            "* PyTorch\n",
            "* Keras\n",
            "* scikit-image\n",
            "\n",
            "Here's a simple example using OpenCV and Python to segment the sella region from an image:\n",
            "\n",
            "```python\n",
            "import cv2\n",
            "import numpy as np\n",
            "\n",
            "# Load the image\n",
            "img = cv2.imread('image.jpg')\n",
            "\n",
            "# Convert the image to grayscale\n",
            "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
            "\n",
            "# Apply thresholding to segment the sella region\n",
            "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
            "\n",
            "# Find contours of the sella region\n",
            "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
            "\n",
            "# Draw a bounding rectangle around the sella region\n",
            "x, y, w, h = cv2.boundingRect(contours[0])\n",
            "cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
            "\n",
            "# Display the output\n",
            "cv2.imshow('Image', img)\n",
            "cv2.waitKey(0)\n",
            "cv2.destroyAllWindows()\n",
            "```\n",
            "\n",
            "Note: This is a simplified example and may not work for all images. The actual implementation would require more complex algorithms and fine-tuning of parameters.\n",
            "ans: Model: Segment-Video\n",
            "Prompt: Segment sella?\n",
            "pred: I'm happy to help, but I don't see any information about a specific instrument. Could you please provide more context or clarify which instrument you are referring to?\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: which instruments is being used?\n",
            "pred: Detecting instruments in video and overlaying diagnostic tumor segmentation involves a multi-step process that combines computer vision and machine learning techniques. Here's a general overview of the approach:\n",
            "\n",
            "1. **Object Detection**: Use a deep learning-based object detection algorithm, such as YOLO (You Only Look Once), SSD (Single Shot Detector), or Faster R-CNN, to detect instruments in the video frames. These algorithms can detect objects of various sizes and classes, including instruments.\n",
            "2. **Instrument Classification**: Once instruments are detected, classify them into different categories (e.g., scalpels, forceps, needles, etc.) using a separate deep learning-based classifier, such as a convolutional neural network (CNN).\n",
            "3. **Tumor Segmentation**: Use a segmentation algorithm, such as U-Net or FCN (Fully Convolutional Network), to segment the tumor from the rest of the image. This can be done using a separate dataset of annotated images or using transfer learning from pre-trained models.\n",
            "4. **Overlaying Tumor Segmentation with Instrument Detection**: Once the instrument detection and tumor segmentation are complete, overlay the segmented tumor with the detected instruments to visualize their relationship.\n",
            "\n",
            "Some popular libraries and frameworks for this task include:\n",
            "\n",
            "* OpenCV: A computer vision library that provides pre-trained models and tools for object detection, segmentation, and classification.\n",
            "* TensorFlow: A machine learning library that provides tools for building and training deep learning models.\n",
            "* PyTorch: A machine learning library that provides tools for building and training deep learning models.\n",
            "* Keras: A high-level neural networks API that provides tools for building and training deep learning models.\n",
            "\n",
            "Some specific architectures that can be used for this task include:\n",
            "\n",
            "* YOLOv3: A real-time object detection system that can detect instruments in video frames.\n",
            "* U-Net: A segmentation architecture that can be used for tumor segmentation.\n",
            "* ResNet: A deep learning architecture that can be used for instrument classification.\n",
            "\n",
            "Some pre-trained models that can be used for this task include:\n",
            "\n",
            "* Mask R-CNN: A popular object detection and segmentation model that can be used for instrument detection and tumor segmentation.\n",
            "* Deeplabv3+: A segmentation model that can be used for tumor segmentation.\n",
            "\n",
            "Here's a sample code snippet in Python using OpenCV and TensorFlow to get you started:\n",
            "\n",
            "```python\n",
            "import cv2\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import load_model\n",
            "\n",
            "# Load pre-trained object detection model\n",
            "net = cv2.dnn.readNetFromDarknet(\"yolov3.cfg\", \"yolov3.weights\")\n",
            "\n",
            "# Load pre-trained segmentation model\n",
            "segmentation_model = load_model(\"unet.h5\")\n",
            "\n",
            "# Load video capture\n",
            "cap = cv2.VideoCapture(0)\n",
            "\n",
            "while True:\n",
            "    # Read frame from video\n",
            "    ret, frame = cap.read()\n",
            "    \n",
            "    # Detect instruments in frame\n",
            "    outputs = net.forward(frame)\n",
            "    \n",
            "    # Extract bounding boxes and class probabilities\n",
            "    boxes, scores, class_ids = outputs[0], outputs[1], outputs[2]\n",
            "    \n",
            "    # Classify instruments\n",
            "    instruments = []\n",
            "    for i in range(len(boxes)):\n",
            "        if scores[i] > 0.5:\n",
            "            instrument_class = class_ids[i]\n",
            "            instruments.append(instrument_class)\n",
            "    \n",
            "    # Segment tumor\n",
            "    tumor_mask = segmentation_model.predict(frame)\n",
            "    \n",
            "    # Overlay tumor segmentation with instrument detection\n",
            "    # ...\n",
            "    \n",
            "    cv2.imshow(\"Frame\", frame)\n",
            "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
            "        break\n",
            "\n",
            "cap.release()\n",
            "cv2.destroyAllWindows()\n",
            "```\n",
            "\n",
            "Note that this is a simplified example and you will need to modify it to suit your specific requirements. You will also need to train your own models for instrument classification and tumor segmentation.\n",
            "ans: Model: Detection|Segmentation_Video\n",
            "Prompt: Detect instruments in video?|Overlay diagnostic tumor segmentation?\n",
            "pred: To overlay tumor segmentation with the surgical scene and identify tool positions, you can use computer-aided design (CAD) software or computer vision techniques. Here's a general approach:\n",
            "\n",
            "**Method 1: Using CAD Software**\n",
            "\n",
            "1. **3D Reconstruction**: Use a 3D reconstruction software to create a 3D model of the surgical scene, including the tumor and the surgical instruments.\n",
            "2. **Segmentation**: Perform tumor segmentation using image processing techniques, such as thresholding, edge detection, and morphological operations, to extract the tumor region from the 3D model.\n",
            "3. **Overlay**: Overlay the segmented tumor region with the 3D model of the surgical scene.\n",
            "4. **Tool Positioning**: Identify the positions of the surgical instruments using a tracking system, such as an electromagnetic tracking system or a camera-based tracking system.\n",
            "5. **Integration**: Integrate the tool positions with the 3D model and the segmented tumor region to visualize the surgical scene.\n",
            "\n",
            "**Method 2: Using Computer Vision Techniques**\n",
            "\n",
            "1. **Image Acquisition**: Acquire images of the surgical scene using a camera or other imaging modalities, such as CT or MRI.\n",
            "2. **Image Processing**: Perform image processing techniques, such as image filtering, thresholding, and edge detection, to enhance the images and segment the tumor region.\n",
            "3. **Object Detection**: Use object detection algorithms, such as YOLO (You Only Look Once) or SSD (Single Shot Detector), to detect the surgical instruments in the images.\n",
            "4. **Tracking**: Track the positions of the surgical instruments using a tracking system, such as a camera-based tracking system or a marker-based tracking system.\n",
            "5. **Registration**: Register the tracked tool positions with the segmented tumor region and the surgical scene.\n",
            "\n",
            "**Tools and Technologies**\n",
            "\n",
            "Some popular tools and technologies used for tumor segmentation and tool positioning include:\n",
            "\n",
            "* 3D Slicer: A free, open-source software for 3D image analysis and visualization.\n",
            "* ITK-SNAP: A free, open-source software for image segmentation and registration.\n",
            "* OpenCV: A free, open-source computer vision library for image and video processing.\n",
            "* TensorFlow: A free, open-source machine learning library for object detection and tracking.\n",
            "\n",
            "**Example Code**\n",
            "\n",
            "Here's an example code snippet in Python using OpenCV and TensorFlow for object detection and tracking:\n",
            "```python\n",
            "import cv2\n",
            "import tensorflow as tf\n",
            "import numpy as np\n",
            "\n",
            "# Load the image\n",
            "img = cv2.imread('image.jpg')\n",
            "\n",
            "# Perform image processing and segmentation\n",
            "img_proc = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
            "img_seg = cv2.threshold(img_proc, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
            "\n",
            "# Detect objects using YOLO\n",
            "net = tf.keras.models.load_model('yolo_model.h5')\n",
            "boxes, scores, classes = net.predict(img_seg)\n",
            "\n",
            "# Track the positions of the objects\n",
            "trackers = []\n",
            "for box in boxes:\n",
            "    x, y, w, h = box\n",
            "    trackers.append((x, y, w, h))\n",
            "\n",
            "# Register the tracked positions with the segmented tumor region and the surgical scene\n",
            "registered_positions = []\n",
            "for tracker in trackers:\n",
            "    x, y, w, h = tracker\n",
            "    registered_positions.append((x, y, w, h))\n",
            "```\n",
            "Note that this is a simplified example and may require modifications to suit your specific use case.\n",
            "ans: Model: Segmentation_Video|Segmentation_MRI\n",
            "Prompt: Overlay tumor segmentation with the surgical scene?|Identify tool positions?\n",
            "pred: I'd be happy to help analyze the instruments in a phase. However, you haven't specified which phase you're referring to. Could you please provide more context or clarify which phase you're interested in analyzing?\n",
            "\n",
            "If you're referring to a specific scientific phase, such as a phase in a scientific experiment or a phase in a project, please provide more details. If you're referring to a phase in music, please specify the genre or type of music.\n",
            "\n",
            "Once I have more information, I can provide a more detailed analysis of the instruments involved.\n",
            "ans: Model: Surgical-VQA\n",
            "Prompt: analyze the instruments in this phase.\n",
            "pred: I'm assuming you are referring to the current trends and applications in the field of video production, particularly in relation to the use of optical effects, such as lens flares, bokeh, and other creative optical elements.\n",
            "\n",
            "In recent years, there has been a significant growth in the use of optical effects in video production, driven by advancements in technology and the increasing demand for visually stunning and engaging content. Here's an analysis of the current phase and segment of optic protuberances in video:\n",
            "\n",
            "**Current Phase:**\n",
            "\n",
            "The current phase of optic protuberances in video production is characterized by:\n",
            "\n",
            "1. **Increased Use of Optical Effects**: The use of optical effects, such as lens flares, bokeh, and other creative optical elements, has become more prevalent in video production. This is driven by the growing demand for visually stunning and engaging content, particularly in social media, advertising, and film.\n",
            "2. **Advancements in Technology**: Advances in camera technology, such as the development of high-speed cameras and improved lens designs, have enabled the creation of more sophisticated optical effects.\n",
            "3. **Rise of VFX and CGI**: The use of visual effects (VFX) and computer-generated imagery (CGI) has become more widespread, allowing for the creation of complex and realistic optical effects.\n",
            "4. **Influence of Social Media**: Social media platforms have played a significant role in driving the demand for visually stunning content, with platforms like Instagram and TikTok favoring high-quality visuals.\n",
            "\n",
            "**Segments of Optic Protuberances:**\n",
            "\n",
            "The segments of optic protuberances in video production can be categorized into:\n",
            "\n",
            "1. **Lens Flares**: Lens flares are a type of optical effect that creates a bright, glowing effect around a light source. They are commonly used in sci-fi and fantasy films, but are also used in other genres to add visual interest.\n",
            "2. **Bokeh**: Bokeh refers to the out-of-focus area of an image, often used to create a shallow depth of field effect. It is commonly used in portrait and fashion photography, but is also used in video production to create a cinematic look.\n",
            "3. **Diffraction Rings**: Diffraction rings are a type of optical effect that creates a colorful, swirling pattern around a light source. They are commonly used in music videos and live performances.\n",
            "4. **Glitter and Sparkles**: Glitter and sparkles are used to create a festive or celebratory atmosphere, often used in party and event videos.\n",
            "5. **Light Leaks**: Light leaks are a type of optical effect that creates a soft, glowing effect around a light source. They are commonly used in horror and thriller films, but are also used in other genres to create a moody atmosphere.\n",
            "6. **Motion Blur**: Motion blur is a type of optical effect that creates a blurred effect on moving objects, often used to create a sense of speed or motion.\n",
            "7. **Rings and Halos**: Rings and halos are used to create a sense of otherworldliness or to draw attention to a specific element in the scene.\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "The current phase of optic protuberances in video production is characterized by increased use of optical effects, advancements in technology, and the rise of VFX and CGI. The segments of optic protuberances include lens flares, bokeh, diffraction rings, glitter and sparkles, light leaks, motion blur, and rings and halos. These effects are used to create visually stunning and engaging content, particularly in social media, advertising, and film.\n",
            "ans: Model: Surgical-VQA|Overlaying\n",
            "Prompt: Analyze the current phase?|Segment optic protuberances in video?\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(all_pred)):\n",
        "    print(\"pred:\", all_pred[i])\n",
        "    print(\"ans:\", all_ans[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
      "metadata": {
        "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
        "outputId": "5a7cdeea-32e1-49cb-bedc-abb58901e5ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "{'rouge1': 0.09242480950307173, 'rouge2': 0.03483780699551295, 'rougeL': 0.08021274526141164, 'rougeLsum': 0.08691701089670383}\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions= all_pred, references=all_ans)\n",
        "print(rouge_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
      "metadata": {
        "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
        "outputId": "faf377cc-e7de-474c-e5a3-87ab3d1c7150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01353101855763003\n"
          ]
        }
      ],
      "source": [
        "bleu_score = corpus_bleu(all_ans, all_pred, weights=(1.0, 0.0, 0.0, 0.0))\n",
        "print(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
      "metadata": {
        "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
        "outputId": "fd881b44-a2d9-483a-fcf0-d5bc6436b492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.15005161159208935\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "m_score=0\n",
        "for line in zip(all_ans, all_pred):\n",
        "    ref = word_tokenize(line[0])\n",
        "    hypo = word_tokenize(line[1])\n",
        "    m_score += meteor_score([ref], hypo)\n",
        "meteors = m_score/len(all_ans)\n",
        "print(meteors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa",
      "metadata": {
        "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6017dd69-1978-417f-93fe-0393b4cc3f74",
      "metadata": {
        "id": "6017dd69-1978-417f-93fe-0393b4cc3f74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25",
      "metadata": {
        "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c",
      "metadata": {
        "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d43e45de30249ff973f0cf438a8386f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f26e698ca7844b09fdc754ce5ce5008",
              "IPY_MODEL_66114edbaa08437b959a1cb3c17f7045",
              "IPY_MODEL_0f5ca80673bb42a1b7e7f929237325f8"
            ],
            "layout": "IPY_MODEL_4a83ed1a3a37439f9bd7577755641a88"
          }
        },
        "6f26e698ca7844b09fdc754ce5ce5008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_168d25d4bdd5470eb56ddc5289511114",
            "placeholder": "​",
            "style": "IPY_MODEL_e9dd399fbebc4239b8ffec968c8fc951",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "66114edbaa08437b959a1cb3c17f7045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ab68c4fe8a24e61878b52e1b2499142",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e930af5ae241ec8fdb30707b7ea339",
            "value": 5
          }
        },
        "0f5ca80673bb42a1b7e7f929237325f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_045639e092ac4049b4fa34baf236d868",
            "placeholder": "​",
            "style": "IPY_MODEL_b2ecc7e3300e4edd92d79518403fcb18",
            "value": " 5/5 [01:32&lt;00:00, 16.12s/it]"
          }
        },
        "4a83ed1a3a37439f9bd7577755641a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "168d25d4bdd5470eb56ddc5289511114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9dd399fbebc4239b8ffec968c8fc951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ab68c4fe8a24e61878b52e1b2499142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e930af5ae241ec8fdb30707b7ea339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "045639e092ac4049b4fa34baf236d868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ecc7e3300e4edd92d79518403fcb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}