{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterHJY628/MyOwnExample/blob/main/vision_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"torch==2.4.0\" tensorboard pillow torchvision accelerate huggingface_hub\n",
        "!pip -q install  --upgrade \\\n",
        "  \"transformers==4.45.1\" \\\n",
        "  \"datasets==3.0.1\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.44.0\" \\\n",
        "  \"trl==0.11.1\" \\\n",
        "  \"peft==0.13.0\" \\\n",
        "  \"qwen_vl_utils\""
      ],
      "metadata": {
        "id": "31hYynMQalOW",
        "outputId": "5b3ef0e0-46b9-4b22-cef1-f61fda5bf92d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "31hYynMQalOW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6XTxQsAahAV",
        "outputId": "94d87c81-00a6-4081-b235-396fc304251e"
      },
      "id": "j6XTxQsAahAV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
      "metadata": {
        "id": "66767b92-5bef-443d-a8dd-c3cc5add2e32",
        "outputId": "0f8acfb0-d5e3-43b9-b3e3-1fd2128e25e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/sa5u24/VQA\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['HF_HOME'] = '/home/sa5u24/VQA'\n",
        "hf_home = os.path.expanduser(\n",
        "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
        ")\n",
        "print(hf_home)\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your-hf-token-here' with your actual Hugging Face token\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6",
      "metadata": {
        "id": "6046b99b-1f55-499b-bb7f-8fd1ac0edbe6"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\" You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
        "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
        "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
        "The agent may call more than one model sequentially or in parallel based on query.\n",
        "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
        "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
        "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
        "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
        "\"\"\"\n",
        "\n",
        "def format_data(sample):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"Agent\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                # {\n",
        "                #     \"type\": \"image\",\n",
        "                #     \"image\": None,\n",
        "                # },\n",
        "                # {\n",
        "                #     \"type\": \"image\",\n",
        "                #     \"image\": sample[\"image\"],\n",
        "                # },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[0]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[1]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"HuggingFaceM4/ChartQA\"\n",
        "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=['train[:10%]', 'val[:10%]', 'test[:10%]'])\n",
        "\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "train_dataset[200], len(train_dataset), len(eval_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "e776ZyBAnZ5G",
        "outputId": "afdc7b27-8460-4804-ed0c-31de6149b63c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e776ZyBAnZ5G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'role': 'system',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'You are a Vision Language Model specialized in interpreting visual data from chart images.\\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n",
              "  {'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Is the rightmost value of light brown graph 58?'}]},\n",
              "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'No'}]}],\n",
              " 2830,\n",
              " 192,\n",
              " 250)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch  # Import torch for dataset splitting and other PyTorch functionalities\n",
        "from torch.utils.data import Dataset, DataLoader  # For creating PyTorch Dataset and DataLoader\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access the file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TextQuestionLabelDataset(Dataset):\n",
        "    def __init__(self, input_file):\n",
        "        # Load data from a CSV file\n",
        "        self.data = pd.read_csv(input_file)\n",
        "        print(self.data.head())  # 显示前 5 行数据\n",
        "\n",
        "\n",
        "        # Extract questions and labels from the Input column\n",
        "        #self.questions = self.data['Input'].apply(lambda x: re.search(r'Question: (.+?)\\n', x).group(1)).tolist()\n",
        "        self.questions = self.data['Input'].tolist()\n",
        "        self.labels = self.data['Label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sample (question, label) given an index\n",
        "        question = self.questions[idx]\n",
        "        label = self.labels[idx]\n",
        "        return question, label\n",
        "\n",
        "input_file = '/content/drive/My Drive/agent_data.csv'\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = TextQuestionLabelDataset(input_file)\n",
        "\n",
        "# Split the dataset for train, eval, and test (e.g., 10% for each)\n",
        "dataset_length = len(dataset)\n",
        "#train_size = int(0.1 * dataset_length)\n",
        "train_size = 0\n",
        "#eval_size = int(0.1 * dataset_length)\n",
        "eval_size = 0\n",
        "test_size = dataset_length - train_size - eval_size\n",
        "\n",
        "train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, eval_size, test_size]\n",
        ")\n",
        "#print(train_dataset[0])\n",
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "# eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "test_dataset[0]\n",
        "# print(\"Train Dataset Example:\")\n",
        "# for question, label in train_loader:\n",
        "#     print(\"Question:\", question)\n",
        "#     print(\"Label:\", label)\n",
        "#     break  # Print only the first batch for demonstration\n",
        "\n",
        "# print(\"\\nEvaluation Dataset Example:\")\n",
        "# for question, label in eval_loader:\n",
        "#     print(\"Question:\", question)\n",
        "#     print(\"Label:\", label)\n",
        "#     break\n",
        "\n",
        "# print(\"\\nTest Dataset Example:\")\n",
        "# for question, label in test_loader:\n",
        "#     print(\"Question:\", question)\n",
        "#     print(\"Label:\", label)\n",
        "#     break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0idS6tUnWhuz",
        "outputId": "d46a0dda-4b34-466c-ca1e-13d0a95a83b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0idS6tUnWhuz",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   Index                    Type  \\\n",
            "0      1             Segment-MRI   \n",
            "1      2  Segment-MRI|Overlaying   \n",
            "2      3            Surgical-VQA   \n",
            "3      4           Segment-Video   \n",
            "4      5              Overlaying   \n",
            "\n",
            "                                               Input  \\\n",
            "0  Question: Identify and segment the right carot...   \n",
            "1  Question: Can you segment the Left Optic Protu...   \n",
            "2              Question: Can you identify the steps?   \n",
            "3  Question: Precisely segment Right Optic Protub...   \n",
            "4  Question: Project tumor segmentation imaging o...   \n",
            "\n",
            "                                               Label  \n",
            "0  Model: Segment-MRI\\nPrompt: Segment right caro...  \n",
            "1  Model: Segment-MRI|Overlaying\\nPrompt: Segment...  \n",
            "2  Model: Surgical-VQA\\nPrompt: can you identify ...  \n",
            "3  Model: Segment-Video\\nPrompt: Precise segmenta...  \n",
            "4  Model: Overlaying\\nPrompt: Overlay tumor segme...  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'Agent',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': ' You are an LLM agent which can call functions and corresponding prompt based on surgeons\\' query.\\nThe agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video,\\xa0Overlaying, Surgical-VQA.\\nThere are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\\nThe agent may call more than one model sequentially or in parallel based on query.\\nYour task is to analyze the user\\'s query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\\nIf you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\\nPrompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video,\\xa0Overlaying, Surgical-VQA].\\nIf you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\\nPrompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video,\\xa0Overlaying, Surgical-VQA].\\n'}]},\n",
              " {'role': 'user',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': 'Question: Analyze the steps in this phase.'}]},\n",
              " {'role': 'assistant',\n",
              "  'content': [{'type': 'text',\n",
              "    'text': 'Model: Surgical-VQA\\nPrompt: analyze the steps in this phase.'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b309522e-b40b-4289-87e9-d28c27b81678",
      "metadata": {
        "id": "b309522e-b40b-4289-87e9-d28c27b81678"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
        "    # Prepare the text input by applying the chat template\n",
        "    text_input = processor.apply_chat_template(\n",
        "        sample[:2],  # Use the sample without the system message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    print(\"Text Input:\", text_input)\n",
        "\n",
        "\n",
        "    # Process the visual input from the sample\n",
        "    # image_inputs, _ = process_vision_info(sample)\n",
        "    #image_inputs = sample[1]['content'][0]['image'].convert(\"RGB\")\n",
        "    image_data = sample[1]['content'][0].get('image')\n",
        "\n",
        "    if image_data is not None:\n",
        "        image_inputs = image_data.convert(\"RGB\")\n",
        "    else:\n",
        "        # 提供一个默认值或适当的处理逻辑\n",
        "        image_inputs = None\n",
        "        #print(\"Warning: The 'image' field is None.\")\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "    # Generate text with the model\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode the output text\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_text[0]  # Return the first decoded output text\n",
        "\n",
        "# def generate_text_from_sample(model, processor, question, max_new_tokens=1024, device=\"cuda\"):\n",
        "#     \"\"\"\n",
        "#     Generate text from a sample using the model and processor.\n",
        "\n",
        "#     Args:\n",
        "#         model: The model used for generation.\n",
        "#         processor: The processor for preparing inputs.\n",
        "#         question: The input question as a string.\n",
        "#         max_new_tokens: The maximum number of tokens to generate.\n",
        "#         device: The device to use for computation (\"cuda\" or \"cpu\").\n",
        "\n",
        "#     Returns:\n",
        "#         A string containing the generated output text.\n",
        "#     \"\"\"\n",
        "#     # Prepare the text input directly from the question\n",
        "#     text_input = processor.apply_chat_template(\n",
        "#         [question],  # Directly use the question\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True\n",
        "#     )\n",
        "\n",
        "#     # Handle the case where no image is provided\n",
        "#     image_inputs = None  # No visual input in the current dataset\n",
        "\n",
        "#     # Prepare the inputs for the model\n",
        "#     model_inputs = processor(\n",
        "#         text=[text_input],\n",
        "#         images=image_inputs,\n",
        "#         return_tensors=\"pt\",\n",
        "#     ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "#     # Generate text with the model\n",
        "#     generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "#     # Trim the generated ids to remove the input ids\n",
        "#     trimmed_generated_ids = [\n",
        "#         out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "#     ]\n",
        "\n",
        "#     # Decode the output text\n",
        "#     output_text = processor.batch_decode(\n",
        "#         trimmed_generated_ids,\n",
        "#         skip_special_tokens=True,\n",
        "#         clean_up_tokenization_spaces=False\n",
        "#     )\n",
        "\n",
        "#     return output_text[0]  # Return the first decoded output text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
      "metadata": {
        "id": "e0e56e15-fd77-4664-a9c9-dfa610d0ddee",
        "outputId": "bf0f8c14-0df1-4910-b7bd-fe4e15aff94f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU allocated memory: 7.16 GB\n",
            "GPU reserved memory: 7.27 GB\n"
          ]
        }
      ],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
      "metadata": {
        "id": "a45230ca-ca62-439d-8984-398cb07db7e4",
        "outputId": "2f0ced20-01a0-401c-ab49-6cadabdfb80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623,
          "referenced_widgets": [
            "a15ddd610949447394edfe7fac3b0c53",
            "c922f983cb2a401f89129d2bbe488504",
            "12ef631a071648219718eff5f36ca082",
            "fc38c445a49b4827b7ceb711a4b3a0b8",
            "938f3eb70926434c888e60f320911439",
            "c0c63722a0b943b283219ef4c891f228",
            "0614bd15bb3c47b888cd095af6fbde3d",
            "91c25d12037e4cce9249ed1fa01cd373",
            "c88640b9f74a4c94ae8b7f75f9c096d6",
            "0e1a80ac8e2149dcac2aa03ed30baf1c",
            "977ca32a5bc147659ab3750c223eac37",
            "fdf7f3386c044daab2b6db2817f9a843",
            "7533ff2b01a746ada24db0615951527c",
            "6f690cdd6edf42828afc957b9178bb37",
            "dae2158227a240c1aed3ef43630225d4",
            "5d77c3cb35004fa7bf0f86bd4d88bb12",
            "da2cb0f727a64d83a2a60ba9f9f049c4",
            "bcc58986d4114ea7b7e1e98a01c54d35",
            "4b269ab43c7c4b0eaf76aafc90319633",
            "26841d7765744acd93466ef5ff1e627a",
            "c7c52a8fb9c844aea3f894668224e292",
            "0b8d4f7dc8b740029d496b8e96659768",
            "32cbd42afab142ee958fa2dd4802cbb1",
            "785f061d8f55429f8e0c1263788074ac",
            "02daad6c59404b34b9892c21bc8ee536",
            "c8ec9cc3664a49569848c0a00450c05c",
            "33d09228c0864eb38aece8edbb4f391b",
            "47d614dd15ba4eacbee29fd7412fae9c",
            "08e61cae439747339b2d4b28a143f251",
            "1d5f1144e04a4cbfa70d7fd0a04e3bd4",
            "bbd12088eb7a438194bced5f971aab80",
            "3450ab4e42bc4a7f9bb7ab3495422c89",
            "8e78774c22df4678b8082bfb9487ac7f",
            "a22b0e360b244953ba1575b777849d86",
            "f0eb35f575b54a44b86010e11f0ef8e7",
            "7e7b0f1435cf482d846c157acb4ac327",
            "e62746836f544c86b9c22abe8248d7eb",
            "8f12f5324e6f4e5c86f9633a1bc16e98",
            "9c547561d90842d1bf808403b2039c0f",
            "09cea1c01f4944d7a4cefd5bcaead6bc",
            "954594f971034e1da177ed3cd4020c64",
            "564eeafd21d341fb8622b137d1216f78",
            "e9b0a16bb6744f0ba4c019862ac154c4",
            "ec7bf01cd83945c6ad02c84701612fd5",
            "5df4d9d662dc41b1a51592fb7c7a5b9d",
            "1dcc390988f54b5493ff496b9cdedcd4",
            "93439be504da439b85b08f4449d6143a",
            "a996947075ad43e9be092c1cca69d371",
            "008cc8f2fc1847a6b0f4618d041f4a7e",
            "b00775cdb5e04932badaea8c8ca5bb14",
            "6781efb7b9864244aed291995e9a35b4",
            "af36d1a7c9354376acd87424a8c177a8",
            "ccbe6e64cffa461aa6b42d9e265b4b58",
            "af0e0b0945a7476e842acd3009cec098",
            "bb7ccae90c354f1e8252227325f32510",
            "10134c11d53540cb8d1f8e1c990e2707",
            "6e5a6cd374db4647b13025054da1aaed",
            "6d9b380b214b488bb072d86f89c6fc2f",
            "bb78dab9dc8648438f514195089a0159",
            "6a737105c5b341238a44c3cfc422f9c7",
            "4a6a5783e2e0484181c6f9980e3e7bcc",
            "a1e622d1487c45d3b9d53822d7885616",
            "b15182f0fa9e4c64835f5c36e66fe940",
            "aec176f1958f410f8427789551297df2",
            "f87719337cf646b28b6ba631d1c65fba",
            "d359643a4f9c40439d3ad54f394b3b20",
            "e28c013966dc4378a6232599cd7d48b3",
            "7e75c3ee506a483f8ace85002174b61b",
            "53a8d9c20b2e42b3a0621602fdf07446",
            "13642d95e21c47c09496cf2392336c38",
            "fa868c50583f405cae527c3a66c933ba",
            "5f561d5d5393403fae6e5fac3514b257",
            "9f82246d40e24dc4806a19a32e81d6a2",
            "f24fde7717b94f6ebbba8f11a00a341d",
            "8b850978bff24891bc203c81f63d7ab3",
            "ccb0a0eea8d74d43b56a5621d0c08bf2",
            "93da11b0625f4444a7b248663636eee8",
            "8bd0434be91d46f38c736dc591592130",
            "9c1bcb16a8594176b5b9a2562d886321",
            "7f9f602df22040d18ed970ed44ac362c",
            "358841d6483f4179bc91d6480626e8f7",
            "55e1459f01df4439ae6c3ad303484726",
            "326a1b5c84d74c00ab446840ef33dad8",
            "6b641dcf98b9482582053adbfc4b0d2d",
            "c75333dee0884e9d807f339c7171c4af",
            "7f2cf8796b6f46819f6ac4a314103222",
            "45efd54e12f14661a72c5b41cf34aaaa",
            "90f0d6b3f76e4f99ab8c0c768f8bc281",
            "d0747f049735444e94c866db1ad78fe1",
            "e3e56d61636d44eba932f958d0d00764",
            "b54ff756e2994ea6bc0753543edd32a1",
            "55e1569f2d40430fbdf48875e0757da8",
            "e0ceb7976450466f96f222a974c5be77",
            "353a799096354129a004138f55429343",
            "25f2fc32426e4674a4f884dd63daaef7",
            "f098ae5c40834a8e9d169f30b63a4c1a",
            "adfb322e996e41798b60ebdf48463ee7",
            "7cfb0f37b3394960a0844dc97cc76df8",
            "705f538eb3ad432cac3ee92b286ecafa",
            "2d0c5cb690c74cac9392d600a8227678",
            "65e9f4784dd34f65be91bedd68e6b71a",
            "67aafe22253f47b3a0e320f2bcb3f63a",
            "6ca0596dd4434826b67e751cbe9c0fba",
            "e5863aa02b25489d88777bf46de46f4d",
            "9da386e64bc446149e13c4f47aed3671",
            "550c2ae6278c4719af39b30c5c740542",
            "a94889a1be1741ff9ca40f939e5263ca",
            "816990ab2cfb4b94ba21962069f702b6",
            "f1fb48b3b4b54fef9ba6ae427d8d43f8",
            "2d840c7e57fe41ebaa8b19769ca04766",
            "1bab37234f1049fea519c53855a678f3",
            "5ac44f376a1f4e06b3ba6195102b6044",
            "e76a91ee385d461fb5af661f0fc98c3f",
            "45e282323b6d4072bef9adebeffdb99c",
            "a2c0e55b85884ef1b38fc9074d58c67f",
            "eee05d15c0bc42a3a5cfac2ead756818",
            "c617b89f5004457098b24111a9c6c7d1",
            "fe8f8abf1ece425891819cb743cf161a",
            "55282459f83d4cc8959c1931628852e1",
            "8004fdd545c74f07b153c3ef40dc7a05",
            "38e0e38a10bb4edc80c46878101d3c63",
            "49b9b590603e40cd80b0e4fcfa7c2459",
            "055d5ae163a045c59487d0741bb803f0",
            "3675d7626ca543ddb0daa6b7f57811ce",
            "01a94322f40641b0bb96b9b8a4ae70dd",
            "8a1e35c2756040d4ac31b19b20940e13",
            "9ac489ad31ec4b609f143f842d4d3b98",
            "8d0491c9048a42cc8e400e195b845026",
            "8caea6c5d1c34c68b121ab553d0fd1f2",
            "d374067d8456490aaedf122b98dac507",
            "f46944ae53ac4ae48ef6b4b18ed2e902",
            "9498482a49e14f1480eda130115d02b9",
            "539d1bd3ba8446dfbc42c460270e99c8",
            "3cb16b74a07f491b98064e86bc6766af",
            "ec5c14b1971f4fb7aa4a23a1639e91a5",
            "5ec8dc9715454e75bb26974a8fa6701e",
            "96ad9355f02d4a16b98deeb3ded3b741",
            "239a2443d0564a9a89eab5bf2d8f7552",
            "90b03b1e0b8240348e48049c8a391c2f",
            "1dd1df8c6786404ea864a417e144d826",
            "0398e44c3a8e4ff09fe5ec740e15e1fb",
            "7d3270e1a3dd4fa39d0fe1b5cd78ddee",
            "cdcc6ae4ba274cb5bb8761aef47ec210",
            "f5516b6259ad470ba765dd611c9ccd6d",
            "3274c30dd43b4d1093d5dbef49b6e10f",
            "835b1eb88dee4ad4a93b2dc488bfa76d",
            "15fed7d47ae4425da8c7198b08805729",
            "9156d40dfbae487e96baf3768c79601e",
            "17319d694dea40bcb2b88fae5a8cb1c2",
            "e8e1508ada8b4f1eb8c328533cb1efcf",
            "427c56e33eb44f16a4ad404ef2180759",
            "c26c5491f1264ac4b9e0bdc65b0ed822",
            "90a4c67792304fc4982e56bf36f43316",
            "e5d2b36524604248b2b2ffa860cff1b8",
            "9e9c2defe743459bb671f32429ed6b8c",
            "8f4b4c00cc564875818c67ca0e50d964",
            "cfa594c088174bcba270a923a6bf7164",
            "416df2cd0b9044eca9d104c63ff21bc4",
            "004f670ad9cf4fcfba01adb07a2f9c11",
            "f0707a97f23a42fbb496f6652ce37b01",
            "b4b8eb75b5404686bb27bd8e3223d612",
            "527cd8224c764053890cb66297823544",
            "09946bdc287e41a5945adf81e01446ef",
            "2cca0c77598849f68c4f42dfee16a51e",
            "7d98b9bb56fe46b9a3b4a6ecab91aa37"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a15ddd610949447394edfe7fac3b0c53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdf7f3386c044daab2b6db2817f9a843"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/5.07k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32cbd42afab142ee958fa2dd4802cbb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a22b0e360b244953ba1575b777849d86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5df4d9d662dc41b1a51592fb7c7a5b9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.json:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10134c11d53540cb8d1f8e1c990e2707"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e28c013966dc4378a6232599cd7d48b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bd0434be91d46f38c736dc591592130"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0747f049735444e94c866db1ad78fe1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d0c5cb690c74cac9392d600a8227678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bab37234f1049fea519c53855a678f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49b9b590603e40cd80b0e4fcfa7c2459"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "539d1bd3ba8446dfbc42c460270e99c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5516b6259ad470ba765dd611c9ccd6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e9c2defe743459bb671f32429ed6b8c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "import torch\n",
        "from nltk.translate.meteor_score import meteor_score, single_meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import requests\n",
        "from torch import nn\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor, MllamaConfig, AutoModelForCausalLM\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    # low_cpu_mem_usage=True,\n",
        "    # bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = MllamaForConditionalGeneration.from_pretrained(\n",
        "            model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True,\n",
        "            quantization_config=quantization_config,\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "all_ans = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for sample in test_dataset:\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[2]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n",
        "# all_pred = []\n",
        "# all_ans = []\n",
        "\n",
        "# model.eval()  # 切换到评估模式\n",
        "# with torch.no_grad():  # 禁用梯度计算\n",
        "#     for question, label in test_loader:  # 遍历测试数据加载器\n",
        "#         # 将问题输入模型，生成预测结果\n",
        "#         # 假设 generate_text_from_sample 是用于推理的函数\n",
        "#         output = [generate_text_from_sample(model, processor, q) for q in question]\n",
        "\n",
        "#         # 将预测结果和真实标签保存\n",
        "#         all_pred.extend(output)\n",
        "#         all_ans.extend(label)\n",
        "\n",
        "# # 打印结果，或者使用 all_pred 和 all_ans 计算指标\n",
        "# print(\"Predictions:\", all_pred)\n",
        "# print(\"Answers:\", all_ans)\n"
      ],
      "metadata": {
        "id": "q7PWaz-aZU-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40696820-83f4-4794-f5e5-ed409964732a"
      },
      "id": "q7PWaz-aZU-B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of Clival Recess from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Extract the pituitary region information from MRI scans?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the region of Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Right Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Clival Recess from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What operation notes is present in the frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Display an overlay of anatomy segmentation on the surgical field?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Integrate diagnosis MRI with the current surgical view?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the detailed left carotid structure in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What steps is present in the frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Sella and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the anatomical structure of anatomical structures using MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Precisely detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the tools being used in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate the instrument quantities in the surgical scene.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the tumor margins from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Left Optic Protuberance and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay diagnosis MRI and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Right Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Clival Recess accurately from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Show an overlay of diagnosis MRI and the surgical frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze the instruments in this phase.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze and segment Left Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay tumor segmentation and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you overlay the tumor segmentation on the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze the video and identify all instruments present?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Extract the sella information from MRI scans?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Merge the tumor segmentation images with surgical video in real-time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate the phases in the surgical scene.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you overlay the diagnosis MRI on the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Provide an overlay of diagnosis MRI and surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of Right Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and detect the instruments in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What surgical instruments can be detected in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze the instruments in this phase.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What instruments are visible in the current frame of the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Sella from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Left Optic Protuberance and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Merge the anatomy segmentation images with surgical video in real-time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Integrate diagnosis MRI with the current surgical view?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze and segment Clival Recess from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Right Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Show an overlay of tumor segmentation and the surgical frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Combine diagnosis MRI and surgical visuals into an overlay?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Sella from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Sella and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Integrate anatomy segmentation with the current surgical view?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate the instruments in the surgical scene.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Precisely detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Merge the anatomy segmentation images with surgical video in real-time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Display an overlay of preoperative imaging on the surgical field?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and detect the instruments in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Precisely detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Integrate diagnosis MRI with the current surgical view?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment the optic protuberances region in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Extract the clival recess information from MRI scans?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What steps is present in the frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay diagnosis MRI and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Sella and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Which instruments is being used?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Clival Recess accurately from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay anatomy segmentation and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What surgical instruments can be detected in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What phases is present in the frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Precisely detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment Left Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What is the position of the phases?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the anatomical structure of Left Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you overlay the preoperative imaging on the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the region of Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Sella and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Right Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Left Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the region of Sella from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and detect the instruments in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Describe the instrument positions being used.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Left Carotid accurately from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay the anatomy segmentation data onto the current surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the position of instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Explain the role of the operation notes in this step.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What observation can be made about the operation notes?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the region of Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Explain the role of the phases in this step.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the tumor margins from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay diagnosis MRI and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Explain the role of the instruments in this step.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment Clival Recess from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the steps?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay preoperative imaging and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What is the position of the phases?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What instruments are visible in the current frame of the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Describe the instrument quantities being used.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What surgical instruments can be detected in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Right Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What is the position of the instruments?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Combine tumor segmentation and surgical visuals into an overlay?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the right carotid from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify and segment the sella in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Sella and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Sella and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Provide an overlay of diagnosis MRI and surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay diagnosis MRI and highlight anatomical structures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Display an overlay of preoperative imaging on the surgical field?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What is the current instrument quantities?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Display an overlay of preoperative imaging on the surgical field?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Project diagnosis MRI imaging onto the live surgical feed?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of clival recess using MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the optic protuberances from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Sella in the current frame of Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the instrument positions?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of optic protuberances using MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of left carotid using MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Clival Recess in the current frame of Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the tools being used in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Combine diagnosis MRI and surgical visuals into an overlay?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay the preoperative imaging data onto the current surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you overlay the anatomy segmentation on the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Precisely segment Left Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay the anatomy segmentation data onto the current surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Right Carotid accurately from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and detect the instruments in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Right Carotid accurately from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the instruments present in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the instruments present in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What is the position of the operation notes?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the anatomical structure of Left Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze and segment Clival Recess from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the instruments present in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate the instrument quantities in the surgical scene.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Left Optic Protuberance and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Left Optic Protuberance and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Left Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment the left carotid region in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment the optic protuberances region in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the instruments present in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and detect the instruments in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Explain the role of the phases in this step.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment the clival recess region in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Right Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the tools being used in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment Clival Recess in the current frame of Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect instruments near the Right Carotid and segment the anatomy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Describe the instrument positions being used.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Integrate tumor segmentation with the current surgical view?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze the instrument quantities in this phase.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and segment Sella from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze the pituitary region and segment it from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Precisely detect the instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze and segment Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Provide an overlay of diagnosis MRI and surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Integrate preoperative imaging with the current surgical view?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify and segment the tumor in MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you identify the operation notes?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the position of instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you overlay the diagnosis MRI on the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the left carotid from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the position of instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the anatomical structure of Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Overlay the preoperative imaging data onto the current surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the anatomical structure of Sella from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze and segment Right Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Detect the position of instruments in the video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate the instruments in the surgical scene.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Merge the anatomy segmentation images with surgical video in real-time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze the video and identify all instruments present?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Show an overlay of tumor segmentation and the surgical frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Perform segmentation of optic protuberances using MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment Right Optic Protuberance from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Provide an overlay of diagnosis MRI and surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Left Optic Protuberance and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the left carotid from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Can you segment the Left Optic Protuberance and overlay it onto the surgical scene?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Combine preoperative imaging and surgical visuals into an overlay?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What observation can be made about the phases?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Left Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: What is the current instrument quantities?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate and detect the instruments in the surgical video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment anatomy and tumor from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Segment the anatomical structure of Right Carotid from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Find all the instruments visible in this video frame?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Locate the instrument positions in the surgical scene.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Merge the preoperative imaging images with surgical video in real-time?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Identify the surgical phase and overlay the Right Carotid location from MRI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Text Input: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 09 Jan 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>Agent<|end_header_id|>\n",
            "\n",
            " You are an LLM agent which can call functions and corresponding prompt based on surgeons' query.\n",
            "The agent will be used in endonasal pituitary surgery to call visual models Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA.\n",
            "There are 59 classes overall, including 4 phases, 15 steps, 18 instruments, 3 variations of instruments present in a frame, 5 positions of the instruments, and 14 operation notes in the annotation classes.\n",
            "The agent may call more than one model sequentially or in parallel based on query.\n",
            "Your task is to analyze the user's query, determine the intent, and return the name of the AI model to be invoked along with the corresponding prompt.\n",
            "If you receive the input like: \"Question: Segment tumor from MRI and overlay on video?\" The format of the answer should be like this: Model: Segmen-MRI|Overlaying\n",
            "Prompt: Segment tumor from MRI?|Overlay on video?\" This is the format for the answers may need more than one model. The model options are Model Options: [Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "If you receive the input like: \"Question: Segment Sella from Video?\" The format of the answer should be like this: Model: Segment-Video\n",
            "Prompt: Segment Sella from video?\" This is the format for the answers need only one model. The model options are Model Options:[Segment-Video, Segment-MRI, Detect-Video, Overlaying, Surgical-VQA].\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Question: Analyze and segment Sella from Video?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 设置保存路径\n",
        "save_path = '/content/drive/My Drive/test_progress.pkl'\n",
        "\n",
        "# 尝试加载之前的进度\n",
        "if os.path.exists(save_path):\n",
        "    with open(save_path, 'rb') as f:\n",
        "        saved_data = pickle.load(f)\n",
        "        start_index = saved_data['start_index']\n",
        "        all_pred = saved_data['all_pred']\n",
        "        all_ans = saved_data['all_ans']\n",
        "        print(f\"Resuming from index {start_index}\")\n",
        "else:\n",
        "    start_index = 0\n",
        "    all_pred = []\n",
        "    all_ans = []\n",
        "    print(\"Starting fresh\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, sample in enumerate(test_dataset):\n",
        "        # 跳过已处理的样本\n",
        "        if i < start_index:\n",
        "            continue\n",
        "\n",
        "        output = generate_text_from_sample(model, processor, sample)\n",
        "        ans = sample[2]['content'][0]['text']\n",
        "        all_pred.append(output)\n",
        "        all_ans.append(ans)\n",
        "\n",
        "        # 更新进度\n",
        "        start_index = i + 1\n",
        "        saved_data = {\n",
        "            'start_index': start_index,\n",
        "            'all_pred': all_pred,\n",
        "            'all_ans': all_ans\n",
        "        }\n",
        "\n",
        "        # 保存到Google Drive\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(saved_data, f)\n",
        "\n",
        "        print(f\"Processed sample {i+1}/{len(test_dataset)}\")\n",
        "\n",
        "print(\"All samples processed!\")\n"
      ],
      "metadata": {
        "id": "TUO0A8QbFjF3"
      },
      "id": "TUO0A8QbFjF3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
      "metadata": {
        "id": "b2591d13-d5de-4107-99d6-95aa50682dbd",
        "outputId": "8a992b56-d03c-4609-9697-c417bddb96e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_pred' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-017ffff57831>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ans:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_pred' is not defined"
          ]
        }
      ],
      "source": [
        "for i in range(len(all_pred)):\n",
        "    print(\"pred:\", all_pred[i])\n",
        "    print(\"ans:\", all_ans[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
      "metadata": {
        "id": "cbed0fa6-53f8-448d-891a-1931c5605d6e",
        "outputId": "172ab424-162e-4c40-9e5b-752e4c322eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "ca82631144c94f8d9359af5ae9974929",
            "8826c971196d47f6904114bb6951cbd0",
            "ae22db2bfc8349fa92d4874d4bbe5e25",
            "4ddcc8d262d24fd6b3072a2bc7b02557",
            "7a2cae6602d6424bb7b53cc33b14cccd",
            "6325e0fd68b74c78a943d27c2ec0e6c2",
            "602892a1015a467ea9c308eda7894c7f",
            "b39d7deb2e594dbeb74a4894c904cd47",
            "baf050162c1347ab899ff2a38005223b",
            "062d84cc30bc49618b9a382c176f11eb",
            "d4e45da432954b688ac67d703c7f65fe"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=9236672979bbc344ac59f4c84d836942b7cc641496ec7d56f6c28b036b11b689\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca82631144c94f8d9359af5ae9974929"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.7465812230783828, 'rouge2': 0.48334332063518365, 'rougeL': 0.7225223873358213, 'rougeLsum': 0.7413116110396836}\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(predictions= all_pred, references=all_ans)\n",
        "print(rouge_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
      "metadata": {
        "id": "a890dd44-b069-4624-869a-9799c8ddc9aa",
        "outputId": "46e41c08-bffa-478c-b11e-5538087ddde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2917373394717713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "bleu_score = corpus_bleu(all_ans, all_pred, weights=(1.0, 0.0, 0.0, 0.0))\n",
        "print(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
      "metadata": {
        "id": "81cfd734-fbec-4aac-8f89-50f567a96c48",
        "outputId": "413bf5b2-6856-42d3-fa26-c5cfe546c1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.757290762185491\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "m_score=0\n",
        "for line in zip(all_ans, all_pred):\n",
        "    ref = word_tokenize(line[0])\n",
        "    hypo = word_tokenize(line[1])\n",
        "    m_score += meteor_score([ref], hypo)\n",
        "meteors = m_score/len(all_ans)\n",
        "print(meteors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract the \"Model\" part from a given text\n",
        "def extract_model(text):\n",
        "    match = re.search(r\"Model:\\s*(.*?)\\n\", text)\n",
        "    return match.group(1) if match else \"\"\n",
        "\n",
        "# Function to calculate exact match accuracy\n",
        "def calculate_exact_match_accuracy(predictions, answers):\n",
        "    pred_models = [extract_model(pred) for pred in predictions]\n",
        "    true_models = [extract_model(ans) for ans in answers]\n",
        "    correct_matches = sum(p == t for p, t in zip(pred_models, true_models))\n",
        "    total = len(true_models)\n",
        "    return correct_matches / total if total > 0 else 0\n",
        "\n",
        "# Example data\n",
        "all_pred = [\n",
        "    \"Model: Segment-MRI|Overlaying\\nPrompt: Segment Left Optic Protuberance from MRI?|Overlay Left Optic Protuberance onto surgical scene?\",\n",
        "    \"Model: Detect-Video\\nPrompt: Detect surgical instruments in the video.\"\n",
        "]\n",
        "\n",
        "all_ans = [\n",
        "    \"Model: Segment-MRI|Overlaying\\nPrompt: Segment Left Optic Protuberance from MRI?|Overlay Left Optic Protuberance onto surgical scene?\",\n",
        "    \"Model: Detect-Video\\nPrompt: Detect surgical instruments in the video.\"\n",
        "]\n",
        "\n",
        "# Calculate exact match accuracy\n",
        "accuracy = calculate_exact_match_accuracy(all_pred, all_ans)\n",
        "print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "0RmPYe-zHvfP"
      },
      "id": "0RmPYe-zHvfP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa",
      "metadata": {
        "id": "0b31786b-2e00-4a3c-a85b-809ce48cfcaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6017dd69-1978-417f-93fe-0393b4cc3f74",
      "metadata": {
        "id": "6017dd69-1978-417f-93fe-0393b4cc3f74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25",
      "metadata": {
        "id": "c66908c8-03b7-4e61-ab7c-a4fc51cfac25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c",
      "metadata": {
        "id": "aeb57e80-8aeb-4dc2-bde4-999d999f1e7c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca82631144c94f8d9359af5ae9974929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8826c971196d47f6904114bb6951cbd0",
              "IPY_MODEL_ae22db2bfc8349fa92d4874d4bbe5e25",
              "IPY_MODEL_4ddcc8d262d24fd6b3072a2bc7b02557"
            ],
            "layout": "IPY_MODEL_7a2cae6602d6424bb7b53cc33b14cccd"
          }
        },
        "8826c971196d47f6904114bb6951cbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6325e0fd68b74c78a943d27c2ec0e6c2",
            "placeholder": "​",
            "style": "IPY_MODEL_602892a1015a467ea9c308eda7894c7f",
            "value": "Downloading builder script: 100%"
          }
        },
        "ae22db2bfc8349fa92d4874d4bbe5e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b39d7deb2e594dbeb74a4894c904cd47",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_baf050162c1347ab899ff2a38005223b",
            "value": 6270
          }
        },
        "4ddcc8d262d24fd6b3072a2bc7b02557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_062d84cc30bc49618b9a382c176f11eb",
            "placeholder": "​",
            "style": "IPY_MODEL_d4e45da432954b688ac67d703c7f65fe",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 331kB/s]"
          }
        },
        "7a2cae6602d6424bb7b53cc33b14cccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6325e0fd68b74c78a943d27c2ec0e6c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "602892a1015a467ea9c308eda7894c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b39d7deb2e594dbeb74a4894c904cd47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf050162c1347ab899ff2a38005223b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "062d84cc30bc49618b9a382c176f11eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e45da432954b688ac67d703c7f65fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a15ddd610949447394edfe7fac3b0c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c922f983cb2a401f89129d2bbe488504",
              "IPY_MODEL_12ef631a071648219718eff5f36ca082",
              "IPY_MODEL_fc38c445a49b4827b7ceb711a4b3a0b8"
            ],
            "layout": "IPY_MODEL_938f3eb70926434c888e60f320911439"
          }
        },
        "c922f983cb2a401f89129d2bbe488504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c63722a0b943b283219ef4c891f228",
            "placeholder": "​",
            "style": "IPY_MODEL_0614bd15bb3c47b888cd095af6fbde3d",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "12ef631a071648219718eff5f36ca082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c25d12037e4cce9249ed1fa01cd373",
            "max": 437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c88640b9f74a4c94ae8b7f75f9c096d6",
            "value": 437
          }
        },
        "fc38c445a49b4827b7ceb711a4b3a0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e1a80ac8e2149dcac2aa03ed30baf1c",
            "placeholder": "​",
            "style": "IPY_MODEL_977ca32a5bc147659ab3750c223eac37",
            "value": " 437/437 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "938f3eb70926434c888e60f320911439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c63722a0b943b283219ef4c891f228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0614bd15bb3c47b888cd095af6fbde3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91c25d12037e4cce9249ed1fa01cd373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c88640b9f74a4c94ae8b7f75f9c096d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e1a80ac8e2149dcac2aa03ed30baf1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "977ca32a5bc147659ab3750c223eac37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdf7f3386c044daab2b6db2817f9a843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7533ff2b01a746ada24db0615951527c",
              "IPY_MODEL_6f690cdd6edf42828afc957b9178bb37",
              "IPY_MODEL_dae2158227a240c1aed3ef43630225d4"
            ],
            "layout": "IPY_MODEL_5d77c3cb35004fa7bf0f86bd4d88bb12"
          }
        },
        "7533ff2b01a746ada24db0615951527c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2cb0f727a64d83a2a60ba9f9f049c4",
            "placeholder": "​",
            "style": "IPY_MODEL_bcc58986d4114ea7b7e1e98a01c54d35",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6f690cdd6edf42828afc957b9178bb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b269ab43c7c4b0eaf76aafc90319633",
            "max": 55782,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26841d7765744acd93466ef5ff1e627a",
            "value": 55782
          }
        },
        "dae2158227a240c1aed3ef43630225d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c52a8fb9c844aea3f894668224e292",
            "placeholder": "​",
            "style": "IPY_MODEL_0b8d4f7dc8b740029d496b8e96659768",
            "value": " 55.8k/55.8k [00:00&lt;00:00, 2.24MB/s]"
          }
        },
        "5d77c3cb35004fa7bf0f86bd4d88bb12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2cb0f727a64d83a2a60ba9f9f049c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc58986d4114ea7b7e1e98a01c54d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b269ab43c7c4b0eaf76aafc90319633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26841d7765744acd93466ef5ff1e627a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7c52a8fb9c844aea3f894668224e292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8d4f7dc8b740029d496b8e96659768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32cbd42afab142ee958fa2dd4802cbb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_785f061d8f55429f8e0c1263788074ac",
              "IPY_MODEL_02daad6c59404b34b9892c21bc8ee536",
              "IPY_MODEL_c8ec9cc3664a49569848c0a00450c05c"
            ],
            "layout": "IPY_MODEL_33d09228c0864eb38aece8edbb4f391b"
          }
        },
        "785f061d8f55429f8e0c1263788074ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d614dd15ba4eacbee29fd7412fae9c",
            "placeholder": "​",
            "style": "IPY_MODEL_08e61cae439747339b2d4b28a143f251",
            "value": "config.json: 100%"
          }
        },
        "02daad6c59404b34b9892c21bc8ee536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d5f1144e04a4cbfa70d7fd0a04e3bd4",
            "max": 5068,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbd12088eb7a438194bced5f971aab80",
            "value": 5068
          }
        },
        "c8ec9cc3664a49569848c0a00450c05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3450ab4e42bc4a7f9bb7ab3495422c89",
            "placeholder": "​",
            "style": "IPY_MODEL_8e78774c22df4678b8082bfb9487ac7f",
            "value": " 5.07k/5.07k [00:00&lt;00:00, 317kB/s]"
          }
        },
        "33d09228c0864eb38aece8edbb4f391b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d614dd15ba4eacbee29fd7412fae9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e61cae439747339b2d4b28a143f251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d5f1144e04a4cbfa70d7fd0a04e3bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd12088eb7a438194bced5f971aab80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3450ab4e42bc4a7f9bb7ab3495422c89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e78774c22df4678b8082bfb9487ac7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a22b0e360b244953ba1575b777849d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0eb35f575b54a44b86010e11f0ef8e7",
              "IPY_MODEL_7e7b0f1435cf482d846c157acb4ac327",
              "IPY_MODEL_e62746836f544c86b9c22abe8248d7eb"
            ],
            "layout": "IPY_MODEL_8f12f5324e6f4e5c86f9633a1bc16e98"
          }
        },
        "f0eb35f575b54a44b86010e11f0ef8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c547561d90842d1bf808403b2039c0f",
            "placeholder": "​",
            "style": "IPY_MODEL_09cea1c01f4944d7a4cefd5bcaead6bc",
            "value": "tokenizer.json: 100%"
          }
        },
        "7e7b0f1435cf482d846c157acb4ac327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954594f971034e1da177ed3cd4020c64",
            "max": 9085825,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_564eeafd21d341fb8622b137d1216f78",
            "value": 9085825
          }
        },
        "e62746836f544c86b9c22abe8248d7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b0a16bb6744f0ba4c019862ac154c4",
            "placeholder": "​",
            "style": "IPY_MODEL_ec7bf01cd83945c6ad02c84701612fd5",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 22.4MB/s]"
          }
        },
        "8f12f5324e6f4e5c86f9633a1bc16e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c547561d90842d1bf808403b2039c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09cea1c01f4944d7a4cefd5bcaead6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954594f971034e1da177ed3cd4020c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564eeafd21d341fb8622b137d1216f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9b0a16bb6744f0ba4c019862ac154c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec7bf01cd83945c6ad02c84701612fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5df4d9d662dc41b1a51592fb7c7a5b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dcc390988f54b5493ff496b9cdedcd4",
              "IPY_MODEL_93439be504da439b85b08f4449d6143a",
              "IPY_MODEL_a996947075ad43e9be092c1cca69d371"
            ],
            "layout": "IPY_MODEL_008cc8f2fc1847a6b0f4618d041f4a7e"
          }
        },
        "1dcc390988f54b5493ff496b9cdedcd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b00775cdb5e04932badaea8c8ca5bb14",
            "placeholder": "​",
            "style": "IPY_MODEL_6781efb7b9864244aed291995e9a35b4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "93439be504da439b85b08f4449d6143a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af36d1a7c9354376acd87424a8c177a8",
            "max": 454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccbe6e64cffa461aa6b42d9e265b4b58",
            "value": 454
          }
        },
        "a996947075ad43e9be092c1cca69d371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af0e0b0945a7476e842acd3009cec098",
            "placeholder": "​",
            "style": "IPY_MODEL_bb7ccae90c354f1e8252227325f32510",
            "value": " 454/454 [00:00&lt;00:00, 27.7kB/s]"
          }
        },
        "008cc8f2fc1847a6b0f4618d041f4a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00775cdb5e04932badaea8c8ca5bb14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6781efb7b9864244aed291995e9a35b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af36d1a7c9354376acd87424a8c177a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbe6e64cffa461aa6b42d9e265b4b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af0e0b0945a7476e842acd3009cec098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7ccae90c354f1e8252227325f32510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10134c11d53540cb8d1f8e1c990e2707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e5a6cd374db4647b13025054da1aaed",
              "IPY_MODEL_6d9b380b214b488bb072d86f89c6fc2f",
              "IPY_MODEL_bb78dab9dc8648438f514195089a0159"
            ],
            "layout": "IPY_MODEL_6a737105c5b341238a44c3cfc422f9c7"
          }
        },
        "6e5a6cd374db4647b13025054da1aaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a6a5783e2e0484181c6f9980e3e7bcc",
            "placeholder": "​",
            "style": "IPY_MODEL_a1e622d1487c45d3b9d53822d7885616",
            "value": "chat_template.json: 100%"
          }
        },
        "6d9b380b214b488bb072d86f89c6fc2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b15182f0fa9e4c64835f5c36e66fe940",
            "max": 5089,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aec176f1958f410f8427789551297df2",
            "value": 5089
          }
        },
        "bb78dab9dc8648438f514195089a0159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87719337cf646b28b6ba631d1c65fba",
            "placeholder": "​",
            "style": "IPY_MODEL_d359643a4f9c40439d3ad54f394b3b20",
            "value": " 5.09k/5.09k [00:00&lt;00:00, 189kB/s]"
          }
        },
        "6a737105c5b341238a44c3cfc422f9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6a5783e2e0484181c6f9980e3e7bcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1e622d1487c45d3b9d53822d7885616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b15182f0fa9e4c64835f5c36e66fe940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aec176f1958f410f8427789551297df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f87719337cf646b28b6ba631d1c65fba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d359643a4f9c40439d3ad54f394b3b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e28c013966dc4378a6232599cd7d48b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e75c3ee506a483f8ace85002174b61b",
              "IPY_MODEL_53a8d9c20b2e42b3a0621602fdf07446",
              "IPY_MODEL_13642d95e21c47c09496cf2392336c38"
            ],
            "layout": "IPY_MODEL_fa868c50583f405cae527c3a66c933ba"
          }
        },
        "7e75c3ee506a483f8ace85002174b61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f561d5d5393403fae6e5fac3514b257",
            "placeholder": "​",
            "style": "IPY_MODEL_9f82246d40e24dc4806a19a32e81d6a2",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "53a8d9c20b2e42b3a0621602fdf07446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f24fde7717b94f6ebbba8f11a00a341d",
            "max": 89446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b850978bff24891bc203c81f63d7ab3",
            "value": 89446
          }
        },
        "13642d95e21c47c09496cf2392336c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb0a0eea8d74d43b56a5621d0c08bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_93da11b0625f4444a7b248663636eee8",
            "value": " 89.4k/89.4k [00:00&lt;00:00, 1.39MB/s]"
          }
        },
        "fa868c50583f405cae527c3a66c933ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f561d5d5393403fae6e5fac3514b257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f82246d40e24dc4806a19a32e81d6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f24fde7717b94f6ebbba8f11a00a341d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b850978bff24891bc203c81f63d7ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccb0a0eea8d74d43b56a5621d0c08bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93da11b0625f4444a7b248663636eee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bd0434be91d46f38c736dc591592130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c1bcb16a8594176b5b9a2562d886321",
              "IPY_MODEL_7f9f602df22040d18ed970ed44ac362c",
              "IPY_MODEL_358841d6483f4179bc91d6480626e8f7"
            ],
            "layout": "IPY_MODEL_55e1459f01df4439ae6c3ad303484726"
          }
        },
        "9c1bcb16a8594176b5b9a2562d886321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_326a1b5c84d74c00ab446840ef33dad8",
            "placeholder": "​",
            "style": "IPY_MODEL_6b641dcf98b9482582053adbfc4b0d2d",
            "value": "Downloading shards: 100%"
          }
        },
        "7f9f602df22040d18ed970ed44ac362c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c75333dee0884e9d807f339c7171c4af",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f2cf8796b6f46819f6ac4a314103222",
            "value": 5
          }
        },
        "358841d6483f4179bc91d6480626e8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45efd54e12f14661a72c5b41cf34aaaa",
            "placeholder": "​",
            "style": "IPY_MODEL_90f0d6b3f76e4f99ab8c0c768f8bc281",
            "value": " 5/5 [08:43&lt;00:00, 91.11s/it]"
          }
        },
        "55e1459f01df4439ae6c3ad303484726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "326a1b5c84d74c00ab446840ef33dad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b641dcf98b9482582053adbfc4b0d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c75333dee0884e9d807f339c7171c4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f2cf8796b6f46819f6ac4a314103222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45efd54e12f14661a72c5b41cf34aaaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f0d6b3f76e4f99ab8c0c768f8bc281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0747f049735444e94c866db1ad78fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3e56d61636d44eba932f958d0d00764",
              "IPY_MODEL_b54ff756e2994ea6bc0753543edd32a1",
              "IPY_MODEL_55e1569f2d40430fbdf48875e0757da8"
            ],
            "layout": "IPY_MODEL_e0ceb7976450466f96f222a974c5be77"
          }
        },
        "e3e56d61636d44eba932f958d0d00764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353a799096354129a004138f55429343",
            "placeholder": "​",
            "style": "IPY_MODEL_25f2fc32426e4674a4f884dd63daaef7",
            "value": "model-00001-of-00005.safetensors: 100%"
          }
        },
        "b54ff756e2994ea6bc0753543edd32a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f098ae5c40834a8e9d169f30b63a4c1a",
            "max": 4992622346,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adfb322e996e41798b60ebdf48463ee7",
            "value": 4992622346
          }
        },
        "55e1569f2d40430fbdf48875e0757da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cfb0f37b3394960a0844dc97cc76df8",
            "placeholder": "​",
            "style": "IPY_MODEL_705f538eb3ad432cac3ee92b286ecafa",
            "value": " 4.99G/4.99G [02:00&lt;00:00, 42.0MB/s]"
          }
        },
        "e0ceb7976450466f96f222a974c5be77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353a799096354129a004138f55429343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f2fc32426e4674a4f884dd63daaef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f098ae5c40834a8e9d169f30b63a4c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adfb322e996e41798b60ebdf48463ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cfb0f37b3394960a0844dc97cc76df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "705f538eb3ad432cac3ee92b286ecafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d0c5cb690c74cac9392d600a8227678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65e9f4784dd34f65be91bedd68e6b71a",
              "IPY_MODEL_67aafe22253f47b3a0e320f2bcb3f63a",
              "IPY_MODEL_6ca0596dd4434826b67e751cbe9c0fba"
            ],
            "layout": "IPY_MODEL_e5863aa02b25489d88777bf46de46f4d"
          }
        },
        "65e9f4784dd34f65be91bedd68e6b71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da386e64bc446149e13c4f47aed3671",
            "placeholder": "​",
            "style": "IPY_MODEL_550c2ae6278c4719af39b30c5c740542",
            "value": "model-00002-of-00005.safetensors: 100%"
          }
        },
        "67aafe22253f47b3a0e320f2bcb3f63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a94889a1be1741ff9ca40f939e5263ca",
            "max": 4966251712,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_816990ab2cfb4b94ba21962069f702b6",
            "value": 4966251712
          }
        },
        "6ca0596dd4434826b67e751cbe9c0fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1fb48b3b4b54fef9ba6ae427d8d43f8",
            "placeholder": "​",
            "style": "IPY_MODEL_2d840c7e57fe41ebaa8b19769ca04766",
            "value": " 4.97G/4.97G [01:59&lt;00:00, 42.2MB/s]"
          }
        },
        "e5863aa02b25489d88777bf46de46f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da386e64bc446149e13c4f47aed3671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550c2ae6278c4719af39b30c5c740542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a94889a1be1741ff9ca40f939e5263ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816990ab2cfb4b94ba21962069f702b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1fb48b3b4b54fef9ba6ae427d8d43f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d840c7e57fe41ebaa8b19769ca04766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bab37234f1049fea519c53855a678f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ac44f376a1f4e06b3ba6195102b6044",
              "IPY_MODEL_e76a91ee385d461fb5af661f0fc98c3f",
              "IPY_MODEL_45e282323b6d4072bef9adebeffdb99c"
            ],
            "layout": "IPY_MODEL_a2c0e55b85884ef1b38fc9074d58c67f"
          }
        },
        "5ac44f376a1f4e06b3ba6195102b6044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eee05d15c0bc42a3a5cfac2ead756818",
            "placeholder": "​",
            "style": "IPY_MODEL_c617b89f5004457098b24111a9c6c7d1",
            "value": "model-00003-of-00005.safetensors: 100%"
          }
        },
        "e76a91ee385d461fb5af661f0fc98c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe8f8abf1ece425891819cb743cf161a",
            "max": 4915919704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55282459f83d4cc8959c1931628852e1",
            "value": 4915919704
          }
        },
        "45e282323b6d4072bef9adebeffdb99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8004fdd545c74f07b153c3ef40dc7a05",
            "placeholder": "​",
            "style": "IPY_MODEL_38e0e38a10bb4edc80c46878101d3c63",
            "value": " 4.92G/4.92G [02:05&lt;00:00, 40.2MB/s]"
          }
        },
        "a2c0e55b85884ef1b38fc9074d58c67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eee05d15c0bc42a3a5cfac2ead756818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c617b89f5004457098b24111a9c6c7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe8f8abf1ece425891819cb743cf161a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55282459f83d4cc8959c1931628852e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8004fdd545c74f07b153c3ef40dc7a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e0e38a10bb4edc80c46878101d3c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49b9b590603e40cd80b0e4fcfa7c2459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_055d5ae163a045c59487d0741bb803f0",
              "IPY_MODEL_3675d7626ca543ddb0daa6b7f57811ce",
              "IPY_MODEL_01a94322f40641b0bb96b9b8a4ae70dd"
            ],
            "layout": "IPY_MODEL_8a1e35c2756040d4ac31b19b20940e13"
          }
        },
        "055d5ae163a045c59487d0741bb803f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac489ad31ec4b609f143f842d4d3b98",
            "placeholder": "​",
            "style": "IPY_MODEL_8d0491c9048a42cc8e400e195b845026",
            "value": "model-00004-of-00005.safetensors: 100%"
          }
        },
        "3675d7626ca543ddb0daa6b7f57811ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8caea6c5d1c34c68b121ab553d0fd1f2",
            "max": 4999823980,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d374067d8456490aaedf122b98dac507",
            "value": 4999823980
          }
        },
        "01a94322f40641b0bb96b9b8a4ae70dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f46944ae53ac4ae48ef6b4b18ed2e902",
            "placeholder": "​",
            "style": "IPY_MODEL_9498482a49e14f1480eda130115d02b9",
            "value": " 5.00G/5.00G [02:01&lt;00:00, 40.8MB/s]"
          }
        },
        "8a1e35c2756040d4ac31b19b20940e13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac489ad31ec4b609f143f842d4d3b98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d0491c9048a42cc8e400e195b845026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8caea6c5d1c34c68b121ab553d0fd1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d374067d8456490aaedf122b98dac507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f46944ae53ac4ae48ef6b4b18ed2e902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9498482a49e14f1480eda130115d02b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "539d1bd3ba8446dfbc42c460270e99c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cb16b74a07f491b98064e86bc6766af",
              "IPY_MODEL_ec5c14b1971f4fb7aa4a23a1639e91a5",
              "IPY_MODEL_5ec8dc9715454e75bb26974a8fa6701e"
            ],
            "layout": "IPY_MODEL_96ad9355f02d4a16b98deeb3ded3b741"
          }
        },
        "3cb16b74a07f491b98064e86bc6766af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239a2443d0564a9a89eab5bf2d8f7552",
            "placeholder": "​",
            "style": "IPY_MODEL_90b03b1e0b8240348e48049c8a391c2f",
            "value": "model-00005-of-00005.safetensors: 100%"
          }
        },
        "ec5c14b1971f4fb7aa4a23a1639e91a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dd1df8c6786404ea864a417e144d826",
            "max": 1465943128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0398e44c3a8e4ff09fe5ec740e15e1fb",
            "value": 1465943128
          }
        },
        "5ec8dc9715454e75bb26974a8fa6701e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3270e1a3dd4fa39d0fe1b5cd78ddee",
            "placeholder": "​",
            "style": "IPY_MODEL_cdcc6ae4ba274cb5bb8761aef47ec210",
            "value": " 1.47G/1.47G [00:35&lt;00:00, 40.9MB/s]"
          }
        },
        "96ad9355f02d4a16b98deeb3ded3b741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "239a2443d0564a9a89eab5bf2d8f7552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b03b1e0b8240348e48049c8a391c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dd1df8c6786404ea864a417e144d826": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0398e44c3a8e4ff09fe5ec740e15e1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d3270e1a3dd4fa39d0fe1b5cd78ddee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdcc6ae4ba274cb5bb8761aef47ec210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5516b6259ad470ba765dd611c9ccd6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3274c30dd43b4d1093d5dbef49b6e10f",
              "IPY_MODEL_835b1eb88dee4ad4a93b2dc488bfa76d",
              "IPY_MODEL_15fed7d47ae4425da8c7198b08805729"
            ],
            "layout": "IPY_MODEL_9156d40dfbae487e96baf3768c79601e"
          }
        },
        "3274c30dd43b4d1093d5dbef49b6e10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17319d694dea40bcb2b88fae5a8cb1c2",
            "placeholder": "​",
            "style": "IPY_MODEL_e8e1508ada8b4f1eb8c328533cb1efcf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "835b1eb88dee4ad4a93b2dc488bfa76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_427c56e33eb44f16a4ad404ef2180759",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c26c5491f1264ac4b9e0bdc65b0ed822",
            "value": 5
          }
        },
        "15fed7d47ae4425da8c7198b08805729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a4c67792304fc4982e56bf36f43316",
            "placeholder": "​",
            "style": "IPY_MODEL_e5d2b36524604248b2b2ffa860cff1b8",
            "value": " 5/5 [01:29&lt;00:00, 15.51s/it]"
          }
        },
        "9156d40dfbae487e96baf3768c79601e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17319d694dea40bcb2b88fae5a8cb1c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e1508ada8b4f1eb8c328533cb1efcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "427c56e33eb44f16a4ad404ef2180759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26c5491f1264ac4b9e0bdc65b0ed822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90a4c67792304fc4982e56bf36f43316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d2b36524604248b2b2ffa860cff1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e9c2defe743459bb671f32429ed6b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f4b4c00cc564875818c67ca0e50d964",
              "IPY_MODEL_cfa594c088174bcba270a923a6bf7164",
              "IPY_MODEL_416df2cd0b9044eca9d104c63ff21bc4"
            ],
            "layout": "IPY_MODEL_004f670ad9cf4fcfba01adb07a2f9c11"
          }
        },
        "8f4b4c00cc564875818c67ca0e50d964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0707a97f23a42fbb496f6652ce37b01",
            "placeholder": "​",
            "style": "IPY_MODEL_b4b8eb75b5404686bb27bd8e3223d612",
            "value": "generation_config.json: 100%"
          }
        },
        "cfa594c088174bcba270a923a6bf7164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_527cd8224c764053890cb66297823544",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09946bdc287e41a5945adf81e01446ef",
            "value": 215
          }
        },
        "416df2cd0b9044eca9d104c63ff21bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cca0c77598849f68c4f42dfee16a51e",
            "placeholder": "​",
            "style": "IPY_MODEL_7d98b9bb56fe46b9a3b4a6ecab91aa37",
            "value": " 215/215 [00:00&lt;00:00, 14.9kB/s]"
          }
        },
        "004f670ad9cf4fcfba01adb07a2f9c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0707a97f23a42fbb496f6652ce37b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b8eb75b5404686bb27bd8e3223d612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "527cd8224c764053890cb66297823544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09946bdc287e41a5945adf81e01446ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cca0c77598849f68c4f42dfee16a51e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d98b9bb56fe46b9a3b4a6ecab91aa37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}